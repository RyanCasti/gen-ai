{
 "cells": [
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "9hZNSDOBJcBm"
 },
 "source": "# LLM Pretraining & Fine-Tuning for Sentiment Classification\n\nThis notebook demonstrates pretraining and fine-tuning techniques for large language models, focusing on sentiment classification.\n\n**Skills Demonstrated:**\n- Continued pretraining of GPT-2 on domain-specific data (movie plots)\n- Measuring perplexity changes across domains\n- Fine-tuning for sequence classification tasks\n- Sentiment analysis with custom classifiers\n- Training loop implementation and evaluation\n\n**Technologies:** PyTorch, Hugging Face Transformers, GPT-2"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "peRi8BBqFtrF"
 },
 "source": "## 0. Setup\n\nLet us first install a few required packages. (You may want to comment this out in case you use a local environment that already has the suitable packages installed.)"
 },
 {
 "cell_type": "code",
 "execution_count": 1,
 "metadata": {
 "id": "GuN074G-JK_T",
 "executionInfo": {
 "status": "ok",
 "timestamp": 1759709795635,
 "user_tz": 360,
 "elapsed": 32465,
 "user": {
 "displayName": "Ryan C",
 "userId": "13217471319505539255"
 }
 }
 },
 "outputs": [],
 "source": "%%capture\n\n#!pip install torch # not required for colabs. Uncomment if needed otherwise\n#!pip install transformers # not required for colabs. Uncomment if needed otherwise\n#!pip install numpy # not required for colabs. Uncomment if needed otherwise\n!pip install portalocker\n!pip install -U datasets fsspec huggingface_hub # Hugging Face's dataset library\n#!pip install pandas # not required for colabs. Uncomment if needed otherwise"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "ymhMnEdJFtrF"
 },
 "source": "Next, we will import required libraries"
 },
 {
 "cell_type": "code",
 "execution_count": 2,
 "metadata": {
 "id": "NySGJMIlFtrF",
 "executionInfo": {
 "status": "ok",
 "timestamp": 1759709832628,
 "user_tz": 360,
 "elapsed": 36997,
 "user": {
 "displayName": "Ryan C",
 "userId": "13217471319505539255"
 }
 }
 },
 "outputs": [],
 "source": "import copy\nimport random\n\nimport torch\nimport numpy as np\nimport pandas as pd\n\nfrom datasets import load_dataset\n\n#from torchtext import data as torchtext_data\nfrom torch import nn\n\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import AutoTokenizer, GPT2Model, GPT2ForSequenceClassification, GPT2LMHeadModel"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "LZmkkCXxuf0o"
 },
 "source": "Let's make sure we will later put data and models on the proper device."
 },
 {
 "cell_type": "code",
 "execution_count": 3,
 "metadata": {
 "colab": {
 "base_uri": "https://localhost:8080/"
 },
 "executionInfo": {
 "elapsed": 15,
 "status": "ok",
 "timestamp": 1759709832646,
 "user": {
 "displayName": "Ryan C",
 "userId": "13217471319505539255"
 },
 "user_tz": 360
 },
 "id": "9piY_5XnuVa9",
 "outputId": "8d9bcb77-daf3-4947-bdc2-31cce698c27c"
 },
 "outputs": [
 {
 "output_type": "execute_result",
 "data": {
 "text/plain": [
 "device(type='cuda')"
 ]
 },
 "metadata": {},
 "execution_count": 3
 }
 ],
 "source": "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n#device = torch.device(\"mps\") # in case you run on a local Mac with metal performance shaders (setup/support is up to you)\ndevice"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "t91i97P4FtrG"
 },
 "source": "Now we will define various functions that we will use in this nodebook. You can jump over this part... but you don't have to.\n\nWe'll define:\n\n1. perplexity(text, model) - a calculation giving us the perplexity for a given text and model. 'How certain is the model in picking the actual nect token?'\n2. ClassificationData class - a class that created the Dataset for our IMDB classification with GPT-2. It has a number of options that we'll use to augment the text to make the classification easier for the model.\n3. BERT ClassificationData class - same for a BERT model.\n4. create_temp_set(base_data, split) - a function used to massage the IMDB dataset, just as we did in PyTorch Intro I.\n5. random_huggingface_blog_text - A list of text of random Hugging Face blog snippets for some validation tests.\n6. fake_data_loader - a function that converts an array of text (fixed batch size for now) into a format that the perplexity calculation can use."
 },
 {
 "cell_type": "code",
 "execution_count": 25,
 "metadata": {
 "id": "vbplEALAFtrG",
 "executionInfo": {
 "status": "ok",
 "timestamp": 1759710968814,
 "user_tz": 360,
 "elapsed": 11,
 "user": {
 "displayName": "Ryan C",
 "userId": "13217471319505539255"
 }
 }
 },
 "outputs": [],
 "source": "#@title Some Definitions\n\ndef perplexity(text_data, model):\n\n loss = []\n for step, batch_data in enumerate(text_data):\n\n batch_input = batch_data['input']\n batch_labels = batch_data['labels']\n\n if step % 100 == 0:\n print('Current batch: ', step)\n\n with torch.no_grad():\n try:\n batch_output = model(batch_input)\n batch_output_reshaped = batch_output.reshape((batch_size * (max_len - 1), -1))\n\n batch_labels_reshaped = batch_labels.reshape((batch_size * (max_len - 1),))\n batch_loss = loss_fn(batch_output_reshaped, batch_labels_reshaped)\n\n loss.append(batch_loss)\n except:\n continue\n\n avg_cost = np.mean([x.cpu().detach() for x in loss])\n avg_perplexity = np.exp(avg_cost)\n\n return avg_perplexity\n\nclass ClassificationData(Dataset):\n def __init__(self,\n base_data,\n tokenizer,\n max_len,\n use_prompt=False,\n prompt_pre_text='',\n prompt_post_text='',\n classification_tokenset={1: 'good', 0: 'bad'},\n num_examples=-1):\n\n self.max_len = max_len\n self.tokenizer = tokenizer # assume that padding token has already been added to tokenizer\n self.data = []\n\n # really not ideal having to iterate through the whole set. But ok for this small data volume\n\n prompt_pre_text = prompt_pre_text.strip()\n prompt_post_text = prompt_post_text.strip()\n\n for num_example, example in enumerate(base_data):\n\n if num_examples != -1 and num_example >= num_examples:\n break\n\n if num_example == 0:\n print(example)\n\n stripped_example = example['text'].strip()\n\n token_encoder = self.tokenizer(' ' + stripped_example)['input_ids'] # simulating that the text will not be at the beginning\n\n if len(token_encoder) <= self.max_len:\n continue # avoids complications with short sentences. No padding is needed then.\n\n truncated_encoding = token_encoder[:self.max_len]\n truncated_example = tokenizer.decode(truncated_encoding) # reconstruct shortened review\n\n # LLMs do next-word predictions. You may want to add a prompt that the model can work with!\n\n if use_prompt:\n\n additional_token_length = len(self.tokenizer(prompt_pre_text)['input_ids']) + len(self.tokenizer(' ' + prompt_post_text)['input_ids']) # simulating that the prompt_post_text will not be at the beginning\n\n cutoff = self.max_len + additional_token_length\n\n prompted_text_line = prompt_pre_text + truncated_example + ' ' + prompt_post_text\n\n else:\n cutoff = self.max_len\n prompted_text_line = truncated_example\n\n if len(self.tokenizer(prompted_text_line)['input_ids']) != cutoff:\n continue\n\n tokenized_example = self.tokenizer(prompted_text_line,\n return_tensors=\"pt\",\n max_length=cutoff,\n truncation=True,\n padding='max_length').to(device)\n\n if example['label'] == 1:\n token = classification_tokenset[1]\n else:\n token = classification_tokenset[0]\n\n token_id = self.tokenizer.encode(' ' + token)[0]\n label = torch.tensor(token_id, dtype=torch.int64, device=device)\n\n self.data.append({'label': label,\n 'input_ids': torch.squeeze(tokenized_example['input_ids']).to(device)\n })\n\n def __len__(self):\n return len(self.data)\n\n def __getitem__(self, index):\n\n return {\n 'input_ids': self.data[index]['input_ids'],\n 'label': self.data[index]['label']\n }\n\nclass BERTClassificationData(Dataset):\n def __init__(self,\n base_data,\n tokenizer,\n max_len,\n num_examples=-1):\n\n self.max_len = max_len\n self.tokenizer = tokenizer # assume that padding token has already been added to tokenizer\n self.data = []\n\n # really not ideal having to iterate through the whole set. But ok for this small data volume\n\n for num_example, example in enumerate(base_data):\n\n if num_examples != -1 and num_example >= num_examples:\n break\n\n token_encoder = self.tokenizer(example['text'])['input_ids']\n\n if len(token_encoder) <= self.max_len:\n continue # avoids complications with short sentences. No padding is needed then.\n\n truncated_encoding = token_encoder[1:self.max_len + 1]\n truncated_example = tokenizer.decode(truncated_encoding) # reconstruct shortened review\n\n cutoff = self.max_len\n prompted_text_line = truncated_example\n\n tokenized_example = self.tokenizer(prompted_text_line,\n return_tensors=\"pt\",\n max_length=cutoff,\n truncation=True,\n padding='max_length').to(device)\n\n label_val = example['label']\n\n label = torch.tensor(label_val, dtype=torch.int64, device=device)\n\n self.data.append({'label': label,\n 'input_ids': torch.squeeze(tokenized_example['input_ids']).to(device)\n })\n\n def __len__(self):\n return len(self.data)\n\n def __getitem__(self, index):\n\n return {\n 'input_ids': self.data[index]['input_ids'],\n 'label': self.data[index]['label']\n }\n\ndef create_temp_set(base_data, num_examples=1000000000):\n num_positive = 0\n num_negative = 0\n num_other = 0\n\n temp_data = []\n out_data = []\n\n for example_num, example in enumerate(base_data):\n\n temp_data.append(example)\n\n random.shuffle(temp_data)\n\n for example_num, example in enumerate(temp_data):\n\n if num_examples != -1 and example_num > num_examples:\n break\n\n if example['label'] == 0:\n num_negative += 1\n elif example['label'] == 1:\n num_positive += 1\n else:\n num_other += 1\n\n out_data.append(example)\n\n print('positive: ', num_positive)\n print('negative: ', num_negative)\n print('other: ', num_other)\n\n return out_data\n\nrandom_huggingface_blog_text = [\"\"\"1. Aspect candidate extraction\n\nIn this work we assume that aspects, which are usually features of products and services, are mostly nouns or noun compounds (strings of consecutive nouns). We use spaCy to tokenize and extract nouns/noun compounds from the sentences in the (few-shot) training set. Since not all extracted nouns/noun compounds are aspects, we refer to them as aspect candidates.\n\n2. Aspect/Non-aspect classification\n\nNow that we have aspect candidates, we need to train a model to be able to distinguish between nouns that are aspects and nouns that are non-aspects. For this purpose, we need training samples with aspect/no-aspect labels. This is done by considering aspects in the training set as True aspects, while other non-overlapping candidate aspects are considered non-aspects and therefore labeled as False:\n\nTraining sentence: \"Waiters aren't friendly but the cream pasta is out of this world.\"\nTokenized: [Waiters, are, n't, friendly, but, the, cream, pasta, is, out, of, this, world, .]\nExtracted aspect candidates: [Waiters, are, n't, friendly, but, the, cream, pasta, is, out, of, this, world, .]\nGold labels from training set, in BIO format: [B-ASP, O, O, O, O, O, B-ASP, I-ASP, O, O, O, O, O, .]\nGenerated aspect/non-aspect Labels: [Waiters, are, n't, friendly, but, the, cream, pasta, is, out, of, this, world, .]\nNow that we have all the aspect candidates labeled, how do we use it to train the candidate aspect classification model? In other words, how do we use SetFit, a sentence classification framework, to classify individual tokens? Well, this is the trick: each aspect candidate is concatenated with the entire training sentence to create a training instance using the following template:\"\"\",\n \"\"\"Normalization interrogations\nDuring our first deeper dive in these surprising behavior, we observed that the normalization step was possibly not working as intended: in some cases, this normalization ignored the correct numerical answers when they were directly followed by a whitespace character other than a space (a line return, for example). Let's look at an example, with the generation being 10\\n\\nPassage: The 2011 census recorded a population of 1,001,360, and the gold answer being 10.\n\nNormalization happens in several steps, both for generation and gold:\n\nSplit on separators |, -, or The beginning sequence of the generation 10\\n\\nPassage: contain no such separator, and is therefore considered a single entity after this step.\nPunctuation removal The first token then becomes 10\\n\\nPassage (: is removed)\nHomogenization of numbers Every string that can be cast to float is considered a number and cast to float, then re-converted to string. 10\\n\\nPassage stays the same, as it cannot be cast to float, whereas the gold 10 becomes 10.0.\nOther steps A lot of other normalization steps ensue (removing articles, removing other whitespaces, etc.) and our original example becomes 10 passage 2011.0 census recorded population of 1001360.0.\nHowever, the overall score is not computed on the string, but on the bag of words (BOW) extracted from the string, here {'recorded', 'population', 'passage', 'census', '2011.0', '1001360.0', '10'}, which is compared with the BOW of the gold, also normalized in the above manner, {10.0}. As you can see, they don\u2019t intersect, even though the model predicted the correct output!\n\nIn summary, if a number is followed by any kind of whitespace other than a simple space, it will not pass through the number normalization, hence never match the gold if it is also a number! This first issue was likely to mess up the scores quite a bit, but clearly it was not the only factor causing DROP scores to be so low. We decided to investigate a bit more.\n\nDiving into the results\nExtending our investigations, our friends at Zeno joined us and undertook a much more thorough exploration of the results, looking at 5 models which were representative of the problems we noticed in DROP scores: falcon-180B and mistral-7B were underperforming compared to what we were expecting, Yi-34B and tigerbot-70B had a very good performance on DROP correlated with their average scores, and facebook/xglm-7.5B fell in the middle.\n\nYou can give analyzing the results a try in the Zeno project here if you want to!\n\nThe Zeno team found two even more concerning features:\n\nNot a single model got a correct result on floating point answers\nHigh quality models which generate long answers actually have a lower f1-score\nAt this point, we believed that both failure cases were actually caused by the same root factor: using . as a stopword token (to end the generations):\n\nFloating point answers are systematically interrupted before their generation is complete\nHigher quality models, which try to match the few-shot prompt format, will generate Answer\\n\\nPlausible prompt for the next question., and only stop during the plausible prompt continuation after the actual answer on the first ., therefore generating too many words and getting a bad f1 score.\nWe hypothesized that both these problems could be fixed by using \\n instead of . as an end of generation stop word.\"\"\",\n \"\"\"Text generation is a rich topic, and there exist several generation strategies for different purposes. We recommend this excellent overview on the subject. Many generation algorithms are supported by the text generation endpoints, and they can be configured using the following parameters:\n\ndo_sample: If set to False (the default), the generation method will be greedy search, which selects the most probable continuation sequence after the prompt you provide. Greedy search is deterministic, so the same results will always be returned from the same input. When do_sample is True, tokens will be sampled from a probability distribution and will therefore vary across invocations.\ntemperature: Controls the amount of variation we desire from the generation. A temperature of 0 is equivalent to greedy search. If we set a value for temperature, then do_sample will automatically be enabled. The same thing happens for top_k and top_p. When doing code-related tasks, we want less variability and hence recommend a low temperature. For other tasks, such as open-ended text generation, we recommend a higher one.\"\"\",\n \"\"\"Recently, we released our Object Detection Leaderboard, ranking object detection models available in the Hub according to some metrics. In this blog, we will demonstrate how the models were evaluated and demystify the popular metrics used in Object Detection, from Intersection over Union (IoU) to Average Precision (AP) and Average Recall (AR). More importantly, we will spotlight the inherent divergences and pitfalls that can occur during evaluation, ensuring that you're equipped with the knowledge not just to understand but to assess model performance critically.\n\nEvery developer and researcher aims for a model that can accurately detect and delineate objects. Our Object Detection Leaderboard is the right place to find an open-source model that best fits their application needs. But what does \"accurate\" truly mean in this context? Which metrics should one trust? How are they computed? And, perhaps more crucially, why some models may present divergent results in different reports? All these questions will be answered in this blog.\n\nSo, let's embark on this exploration together and unlock the secrets of the Object Detection Leaderboard! If you prefer to skip the introduction and learn how object detection metrics are computed, go to the Metrics section. If you wish to find how to pick the best models based on the Object Detection Leaderboard, you may check the Object Detection Leaderboard section.\"\"\"]\n\ndef fake_data_loader(text, tokenizer, max_len):\n return [{'input': tokenizer(text, return_tensors='pt', truncation=True, max_length=max_len)['input_ids'][:, :max_len-1].to(device),\n 'labels': tokenizer(text, return_tensors='pt', truncation=True, max_length=max_len)['input_ids'][:, 1:max_len].to(device)}]"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "QEh7So11FtrG"
 },
 "source": "This should say 'cpu' if using a CPU, or 'cuda', if a GPU is used (or 'mps' per the comment about macs)."
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "CiXfjjjGFtrG"
 },
 "source": "Now we are ready to move to Language Models.\n\n## 1. Continued Pretraining of GPT2 with a Movie Plots dataset\n\nWe are now downloading GPT-2 from Hugging Face, i.e. the tokenizer and the model. We will i) make sure that it is on the proper device, and ii) copy the model to a second one that will see additional pre-training before being used for a classification task."
 },
 {
 "cell_type": "code",
 "execution_count": 5,
 "metadata": {
 "id": "rTut1Px2FtrG",
 "executionInfo": {
 "status": "ok",
 "timestamp": 1759709855804,
 "user_tz": 360,
 "elapsed": 23145,
 "user": {
 "displayName": "Ryan C",
 "userId": "13217471319505539255"
 }
 }
 },
 "outputs": [],
 "source": "%%capture\n\ntorch.manual_seed(10)\nrandom.seed(10)\nnp.random.seed(10)\n\ngpt_2_tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\ngpt_2_tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n\nbase_gpt2_model = GPT2LMHeadModel.from_pretrained(\"gpt2\").to(device)\nadditinal_pretrain_gpt2_model = copy.deepcopy(base_gpt2_model)"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "Gav_H2C8FtrH"
 },
 "source": "We will now continue to pretrain the model *additinal_pretrain_gpt2_model* on a dataset of a specific domain - movies. We use the *CMU Movie Summary Corpus* (https://www.cs.cmu.edu/~ark/personas/, license: http://creativecommons.org/licenses/by-sa/3.0/us/legalcode). It contains 40k+ unlabeled movie plot summaries. As such, they represent domain-specific text which is available at many companies and institutions using their internal documents.\n\nGet the dataset by uncommenting the first line below (we have it commented here because you may need to rerun the notebook multiple times when you already have the dataset. When you do that... make sure you comment out this line again):"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "-YSDM6Z9FtrH"
 },
 "source": "Next, we will create a list of 15k plots and convince ourselves that the data looks roughly as expected:"
 },
 {
 "cell_type": "code",
 "execution_count": 8,
 "metadata": {
 "colab": {
 "base_uri": "https://localhost:8080/",
 "height": 53
 },
 "executionInfo": {
 "elapsed": 1682,
 "status": "ok",
 "timestamp": 1759710153013,
 "user": {
 "displayName": "Ryan C",
 "userId": "13217471319505539255"
 },
 "user_tz": 360
 },
 "id": "e9Z8Ivc0FqiH",
 "outputId": "d4c8695a-0b70-4437-a71c-dcaa4a6077cc"
 },
 "outputs": [
 {
 "output_type": "execute_result",
 "data": {
 "text/plain": [
 "'The nation of Panem consists of a wealthy Capitol and twelve poorer districts. As punishment for a past rebellion, each district must provide a boy and girl between the ages of 12 and 18 selected by lottery for the annual Hunger Games. The tributes must fight to the death in an arena; the sole survivor is rewarded with fame and wealth. In her first Reaping, 12-year-old Primrose Everdeen is chose'"
 ],
 "application/vnd.google.colaboratory.intrinsic+json": {
 "type": "string"
 }
 },
 "metadata": {},
 "execution_count": 8
 }
 ],
 "source": "plots = pd.read_csv('MovieSummaries/plot_summaries.txt', delimiter='\\t')\nplots.columns = ['id', 'plot']\nplot_summary_list = [x for x in plots['plot']][:15000]\nplot_summary_list[0][:400]"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "SAR7ej4nlHdb"
 },
 "source": "Next, we will create a Dataset class that takes text data and returns input token ids and labels for the next word predictions (simply the input token ids shifted one to the left). For simplicity, we will throw out any examples that are shorter than our desired length and truncate all other examples to this length:"
 },
 {
 "cell_type": "code",
 "execution_count": 9,
 "metadata": {
 "id": "N9xh-xTXRzYD",
 "executionInfo": {
 "status": "ok",
 "timestamp": 1759710153021,
 "user_tz": 360,
 "elapsed": 5,
 "user": {
 "displayName": "Ryan C",
 "userId": "13217471319505539255"
 }
 }
 },
 "outputs": [],
 "source": "#@title Class for Creation of Continued Pretraining Dataset (Movie Plots)\n\nclass ContinuedPretrainData(Dataset):\n def __init__(self, base_data, tokenizer, max_len, device):\n self.max_len = max_len\n self.tokenizer = tokenizer # assume that padding token has already been added to tokenizer\n self.data = []\n\n tokenized_examples = tokenizer(base_data,\n max_length=max_len,\n truncation=True, padding='max_length',\n return_tensors=\"pt\")\n\n tokens = tokenized_examples['input_ids'][tokenized_examples['attention_mask'][:, max_len - 1] > 0]\n\n self.data = tokens.to(device)\n\n def __len__(self):\n return self.data.shape[0]\n\n def __getitem__(self, index):\n\n return {'input': self.data[index][:self.max_len - 1],\n 'labels': self.data[index][1:]\n }"
 },
 {
 "cell_type": "markdown",
 "source": "Now, please build a simple neural net that serves for continued pre-training:",
 "metadata": {
 "id": "gY308rAT5hi3"
 }
 },
 {
 "cell_type": "code",
 "execution_count": 10,
 "metadata": {
 "id": "OKG3AXjSRzpZ",
 "executionInfo": {
 "status": "ok",
 "timestamp": 1759710179604,
 "user_tz": 360,
 "elapsed": 2,
 "user": {
 "displayName": "Ryan C",
 "userId": "13217471319505539255"
 }
 }
 },
 "outputs": [],
 "source": "%%capture\n\nclass ContinuedTrainingNetwork(torch.nn.Module):\n \"\"\"\n Build a simple PyTorch network that takes a batch (from a Dataloader)\n as input and returns the logits of each next word prediction.\n When instantiated, you need to pass in a pretrained base model.\n You need to define both, the __init__ and the forward methods.\n \"\"\"\n def __init__(self, pretrainModel):\n super().__init__()\n self.lm = pretrainModel\n\n def forward(self, x): # x stands for the input that the network will use/act on later\n # get the logits for all tokens in all examples and call it logits.\n\n outputs = self.lm(input_ids=x)\n logits = outputs.logits\n\n return logits\n\npretrain_network = ContinuedTrainingNetwork(pretrainModel=additinal_pretrain_gpt2_model)\n\npretrain_network.to(device)"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "tv4d8UFCFtrH"
 },
 "source": "Then we create the training sets:"
 },
 {
 "cell_type": "code",
 "execution_count": 11,
 "metadata": {
 "id": "JpoJG71tRzg-",
 "executionInfo": {
 "status": "ok",
 "timestamp": 1759710196691,
 "user_tz": 360,
 "elapsed": 15002,
 "user": {
 "displayName": "Ryan C",
 "userId": "13217471319505539255"
 }
 }
 },
 "outputs": [],
 "source": "max_len=100\n\ntrain_data = ContinuedPretrainData(plot_summary_list[:10000], tokenizer=gpt_2_tokenizer, max_len=max_len, device=device)\ntest_data = ContinuedPretrainData(plot_summary_list[10000:], tokenizer=gpt_2_tokenizer, max_len=max_len, device=device)"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "C0Z4ZV6GgTAH"
 },
 "source": "Here is the data loader:"
 },
 {
 "cell_type": "code",
 "execution_count": 12,
 "metadata": {
 "id": "yEFJx0qiRzmJ",
 "executionInfo": {
 "status": "ok",
 "timestamp": 1759710258511,
 "user_tz": 360,
 "elapsed": 10,
 "user": {
 "displayName": "Ryan C",
 "userId": "13217471319505539255"
 }
 }
 },
 "outputs": [],
 "source": "torch.manual_seed(10)\nbatch_size = 4\ntrain_texts = DataLoader(train_data, batch_size=batch_size, shuffle=True)\ntest_texts = DataLoader(test_data, batch_size=batch_size, shuffle=True)"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "s50t7hGkFtrH"
 },
 "source": "Next, we construct a network that takes the input from the loaders and returns the logits of each next word prediction:"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "5i-Z0llqFtrH"
 },
 "source": "Test the shape of the output. Is it correct? We first need to grab an example and then look at the shape of the model output:"
 },
 {
 "cell_type": "code",
 "execution_count": 13,
 "metadata": {
 "colab": {
 "base_uri": "https://localhost:8080/"
 },
 "executionInfo": {
 "elapsed": 375,
 "status": "ok",
 "timestamp": 1759710096,
 "user": {
 "displayName": "Ryan C",
 "userId": "13217471319505539255"
 },
 "user_tz": 360
 },
 "id": "V6yUS2tQRztk",
 "outputId": "f9a8af4e-0ea3-422d-adde-143be5c5fd9d"
 },
 "outputs": [
 {
 "output_type": "execute_result",
 "data": {
 "text/plain": [
 "torch.Size([4, 99, 50257])"
 ]
 },
 "metadata": {},
 "execution_count": 13
 }
 ],
 "source": "example_data = next(iter(test_texts))\n\n# Please call your model output pretrain_model_output\n\npretrain_model_output = pretrain_network(example_data['input'])\n\npretrain_model_output.shape"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "zik6v4lIFtrH"
 },
 "source": "**1.a. What do the numbers above refer to**\n"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "ygyfBr_2FtrH"
 },
 "source": "Next, we will calculate the initial perplexity for the test set of the movie plot summaries. We need the loss function for this. Please use the cross entropy to define the loss function *loss_fn*."
 },
 {
 "cell_type": "code",
 "execution_count": 15,
 "metadata": {
 "colab": {
 "base_uri": "https://localhost:8080/"
 },
 "executionInfo": {
 "elapsed": 22,
 "status": "ok",
 "timestamp": 1759710329078,
 "user": {
 "displayName": "Ryan C",
 "userId": "13217471319505539255"
 },
 "user_tz": 360
 },
 "id": "uSQ_n6vDRzyx",
 "outputId": "cc7bada6-4fdd-4d11-aad4-2920c51b4889"
 },
 "outputs": [
 {
 "output_type": "execute_result",
 "data": {
 "text/plain": [
 "tensor(3.3036)"
 ]
 },
 "metadata": {},
 "execution_count": 15
 }
 ],
 "source": "\"\"\"\nDefine the loss function loss_fn as the cross entropy and validate/report on the\ncalculation for the average loss for the two examples\n\nexample 1:\n label: 0\n logits: [-3.1, -2.4]\nexample 2:\n label: 1\n logits: [2.4, -3.1]\n\n\"\"\"\n\n# Define logits as a tensor\ntest_input = torch.tensor([[-3.1, -2.4],\n [ 2.4, -3.1]], dtype=torch.float32)\n\n# Define labels as tensor\ntest_target = torch.tensor([0, 1], dtype=torch.long)\n\ndef loss_fn(logits, label):\n return torch.nn.functional.cross_entropy(logits,label)\n\nloss_fn(test_input,test_target)"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "a0_QsbCVDGcU"
 },
 "source": "**1.b. Examining the average loss for these two examples**\n"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "evqpJ8gIgMpY"
 },
 "source": "**1.c. (Ideally, by just looking at the labels and logits), which example contributes the higher loss? Choose from 'first' or 'second'.**\n"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "OxR6qSGOFtrI"
 },
 "source": "Also, consider https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html and investigate the dimensions of the input! We will need to reshape the output and the labels, because CrossEntropy expects a tensor of individual decisions, not a tensor of decision sequences. Similar for the labels. Recall how to reshape from the previous notebook.\n\nFor the example data we calculate the loss. Then you need to calculate the perplexity."
 },
 {
 "cell_type": "code",
 "execution_count": 20,
 "metadata": {
 "colab": {
 "base_uri": "https://localhost:8080/"
 },
 "executionInfo": {
 "elapsed": 50,
 "status": "ok",
 "timestamp": 1759710547304,
 "user": {
 "displayName": "Ryan C",
 "userId": "13217471319505539255"
 },
 "user_tz": 360
 },
 "id": "XGgN9KjUMkWQ",
 "outputId": "e210a1c1-789a-414b-8886-dcf3c555cfd2"
 },
 "outputs": [
 {
 "output_type": "stream",
 "name": "stdout",
 "text": [
 "Shape of reshaped outputs: torch.Size([396, 50257])\n",
 "Shape of reshaped labels: torch.Size([396])\n"
 ]
 }
 ],
 "source": "# Reshape pretrain_model_output and example_data['labels']. Name them reshaped_pretrain_model_output and reshaped_pretrain_model_labels\n\nreshaped_pretrain_model_output = pretrain_model_output.reshape(396,50257)\nreshaped_pretrain_model_labels = example_data['labels'].reshape(396,)\n\nprint('Shape of reshaped outputs: ', reshaped_pretrain_model_output.shape)\nprint('Shape of reshaped labels: ', reshaped_pretrain_model_labels.shape)"
 },
 {
 "cell_type": "code",
 "execution_count": 21,
 "metadata": {
 "colab": {
 "base_uri": "https://localhost:8080/"
 },
 "executionInfo": {
 "elapsed": 428,
 "status": "ok",
 "timestamp": 1759710878964,
 "user": {
 "displayName": "Ryan C",
 "userId": "13217471319505539255"
 },
 "user_tz": 360
 },
 "id": "u_AaKxpYFtrI",
 "outputId": "771ea58b-15fd-414d-8bf3-e6d864f930bd"
 },
 "outputs": [
 {
 "output_type": "stream",
 "name": "stdout",
 "text": [
 "Initial batch loss: tensor(3.8234, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
 "Initial batch perplexity: tensor(45.7585, device='cuda:0', grad_fn=<ExpBackward0>)\n"
 ]
 }
 ],
 "source": "# Now use the loss function loss_fn to first calculate - for this batch - the loss and then the perplexity.\n\ninitial_batch_loss = loss_fn(reshaped_pretrain_model_output, reshaped_pretrain_model_labels)\ninitial_batch_perplexity = torch.exp(initial_batch_loss)\n\nprint('Initial batch loss: ', initial_batch_loss)\nprint('Initial batch perplexity: ', initial_batch_perplexity)"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "bQpSH7TUFtrI"
 },
 "source": "**1.d. Examining the perplexity of this batch before the training**\n"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "RLSOPMqLFtrI"
 },
 "source": "Next, we will calculate the perplexity of the whole test set. For that we will use the perplexity function defined at the outset:"
 },
 {
 "cell_type": "code",
 "execution_count": 26,
 "metadata": {
 "colab": {
 "base_uri": "https://localhost:8080/"
 },
 "executionInfo": {
 "elapsed": 36298,
 "status": "ok",
 "timestamp": 1759711022239,
 "user": {
 "displayName": "Ryan C",
 "userId": "13217471319505539255"
 },
 "user_tz": 360
 },
 "id": "imll7TlSrRmY",
 "outputId": "545a57ae-5eac-4d86-b73b-08e5fb3cf23e"
 },
 "outputs": [
 {
 "output_type": "stream",
 "name": "stdout",
 "text": [
 "Current batch: 0\n",
 "Current batch: 100\n",
 "Current batch: 200\n",
 "Current batch: 300\n",
 "Current batch: 400\n",
 "Current batch: 500\n",
 "Current batch: 600\n",
 "Current batch: 700\n",
 "Current batch: 800\n",
 "Current batch: 900\n",
 "CPU times: user 23.9 s, sys: 12.1 s, total: 36.1 s\n",
 "Wall time: 36.3 s\n"
 ]
 },
 {
 "output_type": "execute_result",
 "data": {
 "text/plain": [
 "np.float32(49.22481)"
 ]
 },
 "metadata": {},
 "execution_count": 26
 }
 ],
 "source": "%%time\n\ntest_movie_plot_perplexity_before = perplexity(test_texts, pretrain_network)\ntest_movie_plot_perplexity_before"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "QTBvxzNwksC8"
 },
 "source": "**Demonstration:**\n:\n\n1.e. (1 pt) What is the perplexity of the test set before further training?"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "ABTv8wdBQfhz"
 },
 "source": "Ok. What about random text not from this domain? Let us look at the random snippets from Hugging Face blogs defined above in random_huggingface_blog_text:"
 },
 {
 "cell_type": "code",
 "execution_count": 28,
 "metadata": {
 "colab": {
 "base_uri": "https://localhost:8080/"
 },
 "executionInfo": {
 "elapsed": 75,
 "status": "ok",
 "timestamp": 1759711048162,
 "user": {
 "displayName": "Ryan C",
 "userId": "13217471319505539255"
 },
 "user_tz": 360
 },
 "id": "0Sq1xBa3JMBv",
 "outputId": "c68fc148-d8a2-4cbc-ae1f-69e5c3cf3a87"
 },
 "outputs": [
 {
 "output_type": "stream",
 "name": "stdout",
 "text": [
 "Current batch: 0\n"
 ]
 },
 {
 "output_type": "execute_result",
 "data": {
 "text/plain": [
 "np.float32(48.41251)"
 ]
 },
 "metadata": {},
 "execution_count": 28
 }
 ],
 "source": "test_hf_perplexity_before = perplexity(fake_data_loader(random_huggingface_blog_text, tokenizer=gpt_2_tokenizer, max_len=100), pretrain_network)\ntest_hf_perplexity_before"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "bLLMsPPtFtrI"
 },
 "source": "Good, about the same. (As hoped/expected. The model should not have any particular better understanding for either type of text.)\n\nNext, we need to create the optimizer and generate a training loop. Nothing to do here for you, but take a look if interested."
 },
 {
 "cell_type": "code",
 "source": "# example_data = next(iter(test_texts))\n\n# pretrain_model_output = pretrain_network(example_data['input'])\n# pretrain_model_output.shape",
 "metadata": {
 "id": "Z2EOOS9qAwrJ",
 "executionInfo": {
 "status": "ok",
 "timestamp": 1759711075478,
 "user_tz": 360,
 "elapsed": 19,
 "user": {
 "displayName": "Ryan C",
 "userId": "13217471319505539255"
 }
 }
 },
 "execution_count": 31,
 "outputs": []
 },
 {
 "cell_type": "code",
 "execution_count": 35,
 "metadata": {
 "id": "HQQdjKecFtrI",
 "executionInfo": {
 "status": "ok",
 "timestamp": 1759711413175,
 "user_tz": 360,
 "elapsed": 5,
 "user": {
 "displayName": "Ryan C",
 "userId": "13217471319505539255"
 }
 }
 },
 "outputs": [],
 "source": "pretrain_optimizer = torch.optim.AdamW(pretrain_network.parameters(), lr=0.00001)\n\ndef continued_train_loop(dataloader,\n model,\n loss_fn,\n optimizer,\n reporting_interval=100,\n max_len=100,\n steps=None):\n\n \"\"\"\n Write the training loop for continued pre-training. In particular, you need to:\n - initialize the epoch_loss to 0 and set the model into training mode\n - iterate over the batches:\n - break if you are at 'steps' number of batches\n - get the inputs X and labels y (which in this case will be the actual next token)\n - get the model outputs\n - reshape y and model outputs in proper format for cross entropy calculation\n - zero out the gradient\n - calculate loss\n - propagate loss (loss.backward) and apply optimizer step\n - add the loss to the epoch_loss\n Reporting:\n - report the current average loss and perplexity every 'reporting_interval' batches\n - report the average loss and perplexity at the end of the epoch (done for you)\n\n \"\"\"\n\n # Set the model to training mode - important for batch normalization and dropout layers\n # Unnecessary in this situation but added for best practices\n\n # initialize the epoch_loss to 0 and set the model into training mode\n epoch_loss = 0\n model.train()\n\n # iterate over the batches\n for batch, example in enumerate(dataloader):\n\n # get the inputs X and labels y (which in this case will be the actual next token)\n X = example['input']\n y = example['labels']\n\n # break if you are at 'steps' number of batches\n if steps is not None:\n if batch >= steps:\n break\n\n # Compute prediction and loss\n\n pred = model(X)\n\n # reshape y and model outputs in proper format for cross entropy calculation\n pred_reshaped = pred.reshape(396, 50257)\n y_reshaped = y.reshape(396,)\n\n # zero out the gradient\n optimizer.zero_grad() # the gradients need to be zeroed out after the gradients are applied by the optimizer\n\n # calculate loss\n loss = loss_fn(pred_reshaped, y_reshaped)\n\n # Backpropagation\n # propagate loss (loss.backward) and apply optimizer step\n loss.backward()\n optimizer.step()\n\n epoch_loss += loss.item()\n\n if int(batch+1) % reporting_interval == 0:\n print(f'\\tBatch {batch+1}:')\n print(f'\\tAvg loss: {epoch_loss/ (batch+1):.4f}')\n print(f'\\tAvg perplexity: {torch.exp(torch.tensor(epoch_loss/(batch+1))):.4f}')\n\n print(batch)\n print(f\"Training Results: \\n Avg train loss: {epoch_loss/batch:>8f} \\n Avg train perplexity: {np.exp(epoch_loss/batch):>8f} \")"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "FBuMQv3-FtrI"
 },
 "source": "Now we do the training:"
 },
 {
 "cell_type": "code",
 "execution_count": 36,
 "metadata": {
 "colab": {
 "base_uri": "https://localhost:8080/"
 },
 "executionInfo": {
 "elapsed": 149455,
 "status": "ok",
 "timestamp": 1759711906679,
 "user": {
 "displayName": "Ryan C",
 "userId": "13217471319505539255"
 },
 "user_tz": 360
 },
 "id": "9DahA3McFtrI",
 "outputId": "56e69163-3b6e-4fdf-8499-609dc6ec6ab1"
 },
 "outputs": [
 {
 "output_type": "stream",
 "name": "stdout",
 "text": [
 "Epoch 1\n",
 "-------------------------------\n",
 "\tBatch 100:\n",
 "\tAvg loss: 3.7316\n",
 "\tAvg perplexity: 41.7470\n",
 "\tBatch 200:\n",
 "\tAvg loss: 3.7295\n",
 "\tAvg perplexity: 41.6591\n",
 "\tBatch 300:\n",
 "\tAvg loss: 3.7184\n",
 "\tAvg perplexity: 41.1980\n",
 "\tBatch 400:\n",
 "\tAvg loss: 3.7167\n",
 "\tAvg perplexity: 41.1294\n",
 "\tBatch 500:\n",
 "\tAvg loss: 3.7143\n",
 "\tAvg perplexity: 41.0297\n",
 "\tBatch 600:\n",
 "\tAvg loss: 3.7057\n",
 "\tAvg perplexity: 40.6799\n",
 "\tBatch 700:\n",
 "\tAvg loss: 3.7041\n",
 "\tAvg perplexity: 40.6152\n",
 "\tBatch 800:\n",
 "\tAvg loss: 3.7049\n",
 "\tAvg perplexity: 40.6447\n",
 "\tBatch 900:\n",
 "\tAvg loss: 3.7021\n",
 "\tAvg perplexity: 40.5330\n",
 "\tBatch 1000:\n",
 "\tAvg loss: 3.6995\n",
 "\tAvg perplexity: 40.4286\n",
 "1000\n",
 "Training Results: \n",
 " Avg train loss: 3.699538 \n",
 " Avg train perplexity: 40.428625 \n",
 "Done!\n"
 ]
 }
 ],
 "source": "epochs = 1\nfor t in range(epochs):\n # we just train for 1000 batches\n print(f\"Epoch {t+1}\\n-------------------------------\")\n continued_train_loop(train_texts, pretrain_network, loss_fn, pretrain_optimizer, steps=1000)\n\nprint(\"Done!\")"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "Vq71uMOeFtrI"
 },
 "source": "How did the perplexity of the test set change after the additional pre-training?"
 },
 {
 "cell_type": "code",
 "execution_count": 37,
 "metadata": {
 "id": "1YAz5ORhFtrJ",
 "colab": {
 "base_uri": "https://localhost:8080/"
 },
 "executionInfo": {
 "status": "ok",
 "timestamp": 1759711942946,
 "user_tz": 360,
 "elapsed": 36261,
 "user": {
 "displayName": "Ryan C",
 "userId": "13217471319505539255"
 }
 },
 "outputId": "a30375ef-ed8c-4804-fa46-ca15496fd141"
 },
 "outputs": [
 {
 "output_type": "stream",
 "name": "stdout",
 "text": [
 "Current batch: 0\n",
 "Current batch: 100\n",
 "Current batch: 200\n",
 "Current batch: 300\n",
 "Current batch: 400\n",
 "Current batch: 500\n",
 "Current batch: 600\n",
 "Current batch: 700\n",
 "Current batch: 800\n",
 "Current batch: 900\n",
 "CPU times: user 24.4 s, sys: 11.8 s, total: 36.1 s\n",
 "Wall time: 36.3 s\n"
 ]
 },
 {
 "output_type": "execute_result",
 "data": {
 "text/plain": [
 "np.float32(41.839035)"
 ]
 },
 "metadata": {},
 "execution_count": 37
 }
 ],
 "source": "%%time\n\ntest_movie_plot_perplexity_after = perplexity(test_texts, pretrain_network)\ntest_movie_plot_perplexity_after"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "zY0Ri85JRWht"
 },
 "source": "What about the Hugging Face blog snippets that were not in the movie domain:"
 },
 {
 "cell_type": "code",
 "execution_count": 40,
 "metadata": {
 "id": "wQfYKFVgRW1w",
 "colab": {
 "base_uri": "https://localhost:8080/"
 },
 "executionInfo": {
 "status": "ok",
 "timestamp": 1759711943007,
 "user_tz": 360,
 "elapsed": 19,
 "user": {
 "displayName": "Ryan C",
 "userId": "13217471319505539255"
 }
 },
 "outputId": "81dcb856-ce06-4a4d-a98e-1bd68f2915a9"
 },
 "outputs": [
 {
 "output_type": "stream",
 "name": "stdout",
 "text": [
 "Current batch: 0\n"
 ]
 },
 {
 "output_type": "execute_result",
 "data": {
 "text/plain": [
 "np.float32(80.69183)"
 ]
 },
 "metadata": {},
 "execution_count": 40
 }
 ],
 "source": "test_hf_perplexity_after = perplexity(fake_data_loader(random_huggingface_blog_text, tokenizer=gpt_2_tokenizer, max_len=100), pretrain_network)\ntest_hf_perplexity_after"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "0SgmygRcuVa5"
 },
 "source": "**1.f. Examining your observation about the perplexity change for the test movie plot set texts after the additional pre-training? About the same ('within 2'), higher, or lower**\n"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "Z5dAROz3g34c"
 },
 "source": "**1.g. Examining your observation about the perplexity change for the Hugging Face texts after the additional pre-training? About the same ('within 2'), higher, or lower**\n"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "JvkaprBcg4Nm"
 },
 "source": "**1.h. (Free form) What would these observations imply in terms of where/how this model could be used**\n"
 },
 {
 "cell_type": "code",
 "execution_count": 87,
 "metadata": {
 "id": "Ls5mzAmPg4WG",
 "colab": {
 "base_uri": "https://localhost:8080/",
 "height": 70
 },
 "executionInfo": {
 "status": "ok",
 "timestamp": 1759717302659,
 "user_tz": 360,
 "elapsed": 34,
 "user": {
 "displayName": "Ryan C",
 "userId": "13217471319505539255"
 }
 },
 "outputId": "42abff13-87f4-4bd3-ffeb-5b697677747b"
 },
 "outputs": [
 {
 "output_type": "execute_result",
 "data": {
 "text/plain": [
 "'The perlexity of the movie plot set texts is lower, while the perlexity of the hugging face texts is higher.\\nThe model became specialized in movie review language and lost some general language capabilities.\\n\\nThe model could be used as a better starting point for:\\n- Fine-tuning on movie review sentiment analysis (would converge faster)\\n- Generating realistic movie review text\\n- Other movie review-related NLP tasks\\n\\nHowever, it cannot directly classify sentiment without additional fine-tuning.\\n'"
 ],
 "application/vnd.google.colaboratory.intrinsic+json": {
 "type": "string"
 }
 },
 "metadata": {},
 "execution_count": 87
 }
 ],
 "source": "\"\"\"The perlexity of the movie plot set texts is lower, while the perlexity of the hugging face texts is higher.\nThe model became specialized in movie review language and lost some general language capabilities.\n\nThe model could be used as a better starting point for:\n- Fine-tuning on movie review sentiment analysis (would converge faster)\n- Generating realistic movie review text\n- Other movie review-related NLP tasks\n\nHowever, it cannot directly classify sentiment without additional fine-tuning.\n\"\"\""
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "l9VdYiNgw_jm"
 },
 "source": "## 2. Sentiment Classification of the IMDB Movie dataset using GPT2 and Prompts\n\nWe will now get the IMDB dataset, just like we did in the PyTorch Intro II notebook. Refer to it for details."
 },
 {
 "cell_type": "code",
 "execution_count": 45,
 "metadata": {
 "id": "yU4fUhe6BEJZ",
 "colab": {
 "base_uri": "https://localhost:8080/",
 "height": 241,
 "referenced_widgets": [
 "bc84adf3ec66498b882c8e34de94e1b0",
 "a014098616a745b88fe25a028c17ae64",
 "eb4783a56c734ed39c8dcebc4c8b1392",
 "c3661fde103544eaaba881176fc62844",
 "2e966e3c71964548871627efc7807b17",
 "d08af4be29854fad95e00685f86eb370",
 "5a6544cdb9994ce48b6f0087076bd905",
 "c7efff7ea52b41d28b39400ca25a2f47",
 "d9b1c26cf91645febb90c9e9c9eaf4fa",
 "d4c07c3b76ca4aecafaa7e53f6d03904",
 "be8d317b7da745e9907d572f9d674372",
 "ec38cb69815144e1b83fb7746b51c1d0",
 "6666b239894542f0a986a28ea371f86c",
 "d51d7493c196439eb584a1ec3e8ca72e",
 "39ee523c293a4664a138071ba4005444",
 "9ad85ba54c864cf7b7b577617df3a812",
 "c1f4a04c1b334efdb1ea738c015c60cd",
 "b602f5a61e1742d9b55e58edcbc79bb7",
 "d9dbeaef230946b9ba0df7a01ca5b5b6",
 "dd4038bfb4b2488a8cb1cfa829c4bc45",
 "7fb759783a7948d48b08043d149aca7e",
 "7dd1be48587d4059ab5416a29854ab40",
 "d1f553c47c454a23b634800ec608fddd",
 "6a215073fad4492cabec3369ae8e0291",
 "796f1284bc7341a199c4784538a64d88",
 "b81d9444396e42e7a59e053a04716050",
 "b4b73048f8824c5c92201afa0ef06f4a",
 "15441cd0b4b2471cb62135f439753f01",
 "d9d46e3caf0c481dabc2a000ee34caae",
 "9eb2817f1a854d09a2f8218c41618ea3",
 "436128b1dd0c4d8288f83c5e529db39d",
 "8b67c74ec0b14b7983252b99077ab30c",
 "5bf705b036b94bf998203597bb2511f8",
 "2b12fb4cf1f44803b830e33338706362",
 "f1cab43b91ce481ba21b6902628d7854",
 "06b57c00b8f2453593c4dcd4d31a8bda",
 "cca75623f72a4a76805b235f7f0a0438",
 "5693c3d25dc544eeb96e832a3e60f60e",
 "6454fe3a5c384371b09189aa8680e5e7",
 "a886a9c89b874fae96ec631e38593e9c",
 "37a48a3396464787b2bfc0bd3ce743ac",
 "97478926957e484981350124714b6c0c",
 "b0214ab6545649ff8bd7b2cd65f394ff",
 "0b570f8445a54c0c86b1d098d7d0e6d4",
 "92e53c2ed1614e249aa9c18cb104122a",
 "54a515a77da241269ff3dba57f4cd0ad",
 "e69c7822fda04b8b9741f8985a604d30",
 "78f50c0ee2fc43f7b54f9556088affc1",
 "944da8b38150448aa9536b5ff15cacfb",
 "048649e44e5a465bb916e967b8ca6",
 "544de90c919749b3aae6f646af8f330d",
 "f6af2055001c426f80ba0b4fe2e999f8",
 "3144484222384495acec2fa709dfa0ad",
 "0b45b764f3234680854414866c7550bb",
 "0e2751ca97a648d1bf2233185e977c33",
 "7f6a7babb5ed49299add9d92372e6946",
 "2d273f81e9f84cb19c864e7c530ba529",
 "151ae2c00f554724bbe30e529a9ac06e",
 "d6dc3d0683c540e3b5685c211496e3dd",
 "dba0504e3b43437caa26cc0f24a675d8",
 "97f1a1d4539e4607be17b6481c69aa25",
 "2156400a2d12420da365704c9cb1c668",
 "6d759590aa1946e9a51349e38d894c4a",
 "13782fc248b648049a6644270270ca48",
 "3cf7ec49da6546d5a566c092551bd5db",
 "5817efaf444229ad4de149038ca0e",
 "31faf891378745e1a6ef7dfda80efcd3",
 "f0bbec25c2c24b6a848a1c9f028438d4",
 "16506adebb4847c9b88590d8e0beccc1",
 "98099a10026b41d2b19fb39af907776a",
 "b920cea588304cfb9376497dd8197131",
 "00ceb88d890d4e0c97764a76c69265d3",
 "13313d5809ee405c9238f4967d0afd87",
 "b86fa4e9261544e39c98a72e616301a1",
 "d524b2751f0340e2b7093087b593c651",
 "5e4c30a7d4a94c93a1e6950c8e0d3d8a",
 "659bbd2593d44017b113b31f0d0bb145"
 ]
 },
 "executionInfo": {
 "status": "ok",
 "timestamp": 1759712157384,
 "user_tz": 360,
 "elapsed": 6605,
 "user": {
 "displayName": "Ryan C",
 "userId": "13217471319505539255"
 }
 },
 "outputId": "f2992da0-ea5d-402d-c169-1338fde53e45"
 },
 "outputs": [
 {
 "output_type": "display_data",
 "data": {
 "text/plain": [
 "README.md: 0.00B [00:00, ?B/s]"
 ],
 "application/vnd.jupyter.widget-view+json": {
 "version_major": 2,
 "version_minor": 0,
 "model_id": "bc84adf3ec66498b882c8e34de94e1b0"
 }
 },
 "metadata": {}
 },
 {
 "output_type": "display_data",
 "data": {
 "text/plain": [
 "plain_text/train-00000-of-00001.parquet: 0%| | 0.00/21.0M [00:00<?, ?B/s]"
 ],
 "application/vnd.jupyter.widget-view+json": {
 "version_major": 2,
 "version_minor": 0,
 "model_id": "ec38cb69815144e1b83fb7746b51c1d0"
 }
 },
 "metadata": {}
 },
 {
 "output_type": "display_data",
 "data": {
 "text/plain": [
 "plain_text/test-00000-of-00001.parquet: 0%| | 0.00/20.5M [00:00<?, ?B/s]"
 ],
 "application/vnd.jupyter.widget-view+json": {
 "version_major": 2,
 "version_minor": 0,
 "model_id": "d1f553c47c454a23b634800ec608fddd"
 }
 },
 "metadata": {}
 },
 {
 "output_type": "display_data",
 "data": {
 "text/plain": [
 "plain_text/unsupervised-00000-of-00001.p(\u2026): 0%| | 0.00/42.0M [00:00<?, ?B/s]"
 ],
 "application/vnd.jupyter.widget-view+json": {
 "version_major": 2,
 "version_minor": 0,
 "model_id": "2b12fb4cf1f44803b830e33338706362"
 }
 },
 "metadata": {}
 },
 {
 "output_type": "display_data",
 "data": {
 "text/plain": [
 "Generating train split: 0%| | 0/25000 [00:00<?, ? examples/s]"
 ],
 "application/vnd.jupyter.widget-view+json": {
 "version_major": 2,
 "version_minor": 0,
 "model_id": "92e53c2ed1614e249aa9c18cb104122a"
 }
 },
 "metadata": {}
 },
 {
 "output_type": "display_data",
 "data": {
 "text/plain": [
 "Generating test split: 0%| | 0/25000 [00:00<?, ? examples/s]"
 ],
 "application/vnd.jupyter.widget-view+json": {
 "version_major": 2,
 "version_minor": 0,
 "model_id": "7f6a7babb5ed49299add9d92372e6946"
 }
 },
 "metadata": {}
 },
 {
 "output_type": "display_data",
 "data": {
 "text/plain": [
 "Generating unsupervised split: 0%| | 0/50000 [00:00<?, ? examples/s]"
 ],
 "application/vnd.jupyter.widget-view+json": {
 "version_major": 2,
 "version_minor": 0,
 "model_id": "31faf891378745e1a6ef7dfda80efcd3"
 }
 },
 "metadata": {}
 }
 ],
 "source": "imdb_dataset = load_dataset(\"IMDB\")"
 },
 {
 "cell_type": "code",
 "execution_count": 47,
 "metadata": {
 "id": "S5UaatHFEjEK",
 "colab": {
 "base_uri": "https://localhost:8080/"
 },
 "executionInfo": {
 "status": "ok",
 "timestamp": 1759712158598,
 "user_tz": 360,
 "elapsed": 1146,
 "user": {
 "displayName": "Ryan C",
 "userId": "13217471319505539255"
 }
 },
 "outputId": "390c8cf9-7930-49e3-b24b-ca97ca99d855"
 },
 "outputs": [
 {
 "output_type": "stream",
 "name": "stdout",
 "text": [
 "positive: 12500\n",
 "negative: 12500\n",
 "other: 0\n",
 "positive: 12500\n",
 "negative: 12500\n",
 "other: 0\n"
 ]
 }
 ],
 "source": "imdb_train_set = create_temp_set(imdb_dataset['train'])\nimdb_test_set = create_temp_set(imdb_dataset['test'])"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "XOzsNC6aFtrJ"
 },
 "source": "Usually we would first get the Dataset and the Dataloader, however for reasons that hopefully become apparent, in this case we first want to build the network. You should think of the network as taking input_ids $x$ from a batch of suitably tokenized sentences, and the **output should be the logits of the last token for each example in the batch.**\n\nPlease fill in the missing line:"
 },
 {
 "cell_type": "code",
 "execution_count": 48,
 "metadata": {
 "id": "ZGSGslw1BJtk",
 "executionInfo": {
 "status": "ok",
 "timestamp": 1759712163035,
 "user_tz": 360,
 "elapsed": 11,
 "user": {
 "displayName": "Ryan C",
 "userId": "13217471319505539255"
 }
 }
 },
 "outputs": [],
 "source": "%%capture\n\nclass TokenPredictionNetworkClass(torch.nn.Module):\n \"\"\"\n Define a simple PyTorch network that takes a batch (from a Dataloader)\n as input and returns the logits for the last next-token prediction.\n When instantiated, you need to pass in a pretrained base language model\n (the 'logit_model').\n You need to define both, the __init__ and the forward methods.\n \"\"\"\n\n def __init__(self, logit_model):\n super().__init__()\n self.lm = logit_model\n\n def forward(self, x):\n # get the logits for the last position of each each example. (Call them last_token_logits.). This will be just one line.\n # Use self.logit_model(x) to get the model output\n outputs = self.lm(input_ids=x)\n logits = outputs.logits\n last_token_logits = logits[:, -1, :]\n\n return last_token_logits\n\nloss_fn = torch.nn.CrossEntropyLoss()\n\ntoken_prediction_network_base_model = TokenPredictionNetworkClass(logit_model=base_gpt2_model)\ntoken_prediction_network_addnl_pretrain_model = TokenPredictionNetworkClass(logit_model=additinal_pretrain_gpt2_model)\n\ntoken_prediction_network_base_model.to(device)\ntoken_prediction_network_addnl_pretrain_model.to(device)"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "Dad6GNLWBdvb"
 },
 "source": "We now construct our training and test sets for the Sentiment Classification. We want to follow a different approach than we have in the PyTorch intro II notebook. We want to leverage the language model and what it is good at - predicting the next tokens - to the maximum. So why not put a 'wrapper' around the review in a way that the proper sentiment would be naturally the next word?\n\nAs a simple example, rather than trying to use the last output vector and add a classification layer let's try to reframe the problem like this (as an illustrative example):\n\n \"This is a review: <truncated review text>... The reviewer classifies reviews as good or bad. In this case they thought the movie was\"\n\n or\n\n \"This is a review: <truncated review text>... The reviewer has positive or negative sentiments about movies. In this case the sentiment was\"\n\n ...\n\n One would think that the LM should already do a decent job getting the proper sentiment simply using the next word prediction task it is trained on!\n\n How could we test this? We could simply consider the cross entropy loss for the next token relative to the actual sentiment, i.e. the next word we would expect for a positive or a negative review.\n\n So we can experient with:\n\n * The pre-fix before the review\n * The text after the review\n * The words we would expect for pos/neg reviews\n\nTry a few combinations and see which ones give you the lowest loss.\n\n**NOTE:** Usually we also would do a good chunk of text pre-processing (take out html, etc.), but for simplicity we will ignore this."
 },
 {
 "cell_type": "code",
 "execution_count": 49,
 "metadata": {
 "id": "o0yNQL1WFuJu",
 "executionInfo": {
 "status": "ok",
 "timestamp": 1759712185988,
 "user_tz": 360,
 "elapsed": 13,
 "user": {
 "displayName": "Ryan C",
 "userId": "13217471319505539255"
 }
 }
 },
 "outputs": [],
 "source": "class ClassificationData(Dataset):\n def __init__(self,\n base_data,\n tokenizer,\n max_len,\n use_prompt=False,\n prompt_pre_text='',\n prompt_post_text='',\n classification_tokenset={1: 'good', 0: 'bad'},\n num_examples=-1):\n\n self.max_len = max_len\n self.tokenizer = tokenizer # assume that padding token has already been added to tokenizer\n self.data = []\n\n # really not ideal having to iterate through the whole set. But ok for this small data volume\n\n for num_example, example in enumerate(base_data):\n\n if num_examples != -1 and num_example >= num_examples:\n break\n\n if num_example == 0:\n print(example)\n\n token_encoder = self.tokenizer(example['text'])['input_ids']\n\n if len(token_encoder) <= self.max_len:\n continue # avoids complications with short sentences. No padding is needed then.\n\n truncated_encoding = token_encoder[:self.max_len]\n truncated_example = tokenizer.decode(truncated_encoding) # reconstruct shortened review\n\n # LLMs do next-word predictions. You may want to add a prompt that the model can work with!\n\n if use_prompt:\n\n additional_token_length = len(self.tokenizer(prompt_pre_text)['input_ids']) + len(self.tokenizer(prompt_post_text)['input_ids'])\n cutoff = self.max_len + additional_token_length - 1\n\n prompted_text_line = prompt_pre_text + truncated_example + prompt_post_text\n\n else:\n cutoff = self.max_len\n prompted_text_line = truncated_example\n\n if len(self.tokenizer(prompted_text_line)['input_ids']) != cutoff:\n continue\n\n tokenized_example = self.tokenizer(prompted_text_line,\n return_tensors=\"pt\",\n max_length=cutoff,\n truncation=True,\n padding='max_length').to(device)\n\n #if num_example == 0:\n # print(self.tokenizer.decode(tokenized_example['input_ids'][0]))\n\n if example['label'] == 1:\n token = classification_tokenset[1]\n else:\n token = classification_tokenset[0]\n\n token_id = self.tokenizer.encode(' ' + token)[0]\n label = torch.tensor(token_id, dtype=torch.int64, device=device)\n\n self.data.append({'label': label,\n 'input_ids': torch.squeeze(tokenized_example['input_ids']).to(device)\n })\n\n def __len__(self):\n return len(self.data)\n\n def __getitem__(self, index):\n\n return {\n 'input_ids': self.data[index]['input_ids'],\n 'label': self.data[index]['label']\n }"
 },
 {
 "cell_type": "code",
 "execution_count": 50,
 "metadata": {
 "id": "lIavTztxFtrJ",
 "colab": {
 "base_uri": "https://localhost:8080/"
 },
 "executionInfo": {
 "status": "ok",
 "timestamp": 1759712234778,
 "user_tz": 360,
 "elapsed": 637,
 "user": {
 "displayName": "Ryan C",
 "userId": "13217471319505539255"
 }
 },
 "outputId": "3647eaa2-40e4-4728-c1aa-3a3fa737840a"
 },
 "outputs": [
 {
 "output_type": "stream",
 "name": "stdout",
 "text": [
 "{'text': \"Normally I would never rent a movie like this, because you know it's going to be bad just by looking at the box. I rented seven movies at the same time, including Nightmare on Elm Street 5, 6 and Wes Craven's New Nightmare. Unfortunately, when I got home I found out the videostore-guy gave me the wrong tape. In the box of Wes Craven's New Nightmare I found this lame movie.<br /><br />This movie is incredibly boring, the acting is bad and the plot doesn't make any sense. It's hard to write a good review, because I have no idea what the movie was really about. At the end of the movie you have more questions then answers.<br /><br />On 'Max Power's Scale of 1 to 10' I rate this movie: 1<br /><br />PS I would like to correct Corinthian's review (right below mine). He says Robert Englund is ripping off lingerie, riding horses naked, etc. The guy that did those things was Mahmoud, played by Juliano Mer, not by Robert Englund.\", 'label': 0}\n",
 "Average loss: tensor(1.0649, device='cuda:0')\n",
 "Predicted tokens vs labels: [('good', 'good'), ('good', 'bad'), ('good', 'good'), ('good', 'good'), ('good', 'good'), ('good', 'bad'), ('good', 'good'), ('good', 'good'), ('good', 'good'), ('good', 'bad'), ('good', 'bad'), ('good', 'good'), ('good', 'good'), ('good', 'good'), ('good', 'bad'), ('good', 'good'), ('good', 'bad'), ('good', 'bad'), ('good', 'good'), ('good', 'bad')]\n"
 ]
 }
 ],
 "source": "#Suggested, but try a bunch!\n# prompt_pre_text = 'Here is a movie review: '\n#prompt_post_text = ' ... The reviewer classifies reviews as good or bad. In this case they thought the movie was'\n#classification_tokenset = {1: 'good', 0: 'bad'}\n\n# make a modification to the prompt_pre_text, prompt_post_text, and classification_tokenset\n# that gets the loss below 1.7\n\nprompt_pre_text = \"\"\"Here is a movie review: \"Great film!\" ... The reviewer classifies reviews as good or bad. In this case they thought the movie was good\nHere is a movie review: \"Terrible movie.\" ... The reviewer classifies reviews as good or bad. In this case they thought the movie was bad\nHere is a movie review: \"Loved the acting!\" ... The reviewer classifies reviews as good or bad. In this case they thought the movie was good\nHere is a movie review: \"Boring plot.\" ... The reviewer classifies reviews as good or bad. In this case they thought the movie was bad\nHere is a movie review: \"\"\"\n\nprompt_post_text = \" ... The reviewer classifies reviews as good or bad. In this case they thought the movie was\"\nclassification_tokenset = {1: 'good', 0: 'bad'}\n\nplay_data = ClassificationData(imdb_train_set,\n tokenizer=gpt_2_tokenizer,\n max_len=100,\n use_prompt=True,\n prompt_pre_text = prompt_pre_text,\n prompt_post_text = prompt_post_text,\n classification_tokenset=classification_tokenset,\n num_examples=20\n )\n\nbatch_size = 4\ntoy_texts = DataLoader(play_data, batch_size=batch_size, shuffle=True)\n\nloss = 0\npredicted_tokens = []\nlabels = []\n\nfor batch_num, toy_text_batch in enumerate(toy_texts):\n sample_output = token_prediction_network_base_model(toy_text_batch['input_ids']).to(device)\n sample_labels = toy_text_batch['label']\n loss += loss_fn(sample_output, sample_labels).detach()\n\n predicted_tokens += gpt_2_tokenizer.decode(torch.argmax(sample_output, dim=-1)).split()\n labels += gpt_2_tokenizer.decode(sample_labels).split()\n\nloss /= (batch_num + 1)\n\nprint('Average loss: ', loss)\nprint('Predicted tokens vs labels: ', [(x, y) for x,y in zip(predicted_tokens, labels)])"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "vB_NC-2yFtrJ"
 },
 "source": "Ok, the accuracy using the old GPT2 is not exactly amazing (newer and larger models would be MUCH better out of the box). However, even for GPT2 at least a token of the right type is predicted. Fine-tuning should make this much better!\n\n**2.a. Write down two different prompt_pre_text/prompt_post_text combinations and their respective average loss. Pick one that sounds reasonable but is quite a bit worse (say, average loss > 3), and another that gets the loss below 1.7. (Note, for the latter you probably have to counteract a bit the model's tendency to be positve. You may also want to be more clear where the review starts and ends.)**\n"
 },
 {
 "cell_type": "code",
 "execution_count": 78,
 "metadata": {
 "id": "7CD1XwvWh-vj",
 "colab": {
 "base_uri": "https://localhost:8080/",
 "height": 105
 },
 "executionInfo": {
 "status": "ok",
 "timestamp": 1759716291989,
 "user_tz": 360,
 "elapsed": 11,
 "user": {
 "displayName": "Ryan C",
 "userId": "13217471319505539255"
 }
 },
 "outputId": "700aa509-f690-47fe-d486-7e197f92c632"
 },
 "outputs": [
 {
 "output_type": "execute_result",
 "data": {
 "text/plain": [
 "'\\n\\nPre-text and post-text that performed poorly:\\nprompt_pre_text = \\'You are an expert movie review sentiment detector. Below is a movie review.\\'\\n\\n[review]\\n\\nprompt_post_text = \\'Given the above sentiment, use the information to determine if the review was good (1) or bad (0).\\'\\n\\nPre-text and post-text that performed well:\\nprompt_pre_text = \"Here is a movie review: \"Great film!\" ... The reviewer classifies reviews as good or bad. In this case they thought the movie was good\\nHere is a movie review: \"Terrible movie.\" ... The reviewer classifies reviews as good or bad. In this case they thought the movie was bad\\nHere is a movie review: \"Loved the acting!\" ... The reviewer classifies reviews as good or bad. In this case they thought the movie was good\\nHere is a movie review: \"Boring plot.\" ... The reviewer classifies reviews as good or bad. In this case they thought the movie was bad\\nHere is a movie review: \"\\n\\n[review]\\n\\nprompt_post_text = \" ... The reviewer classifies reviews as good or bad. In this case they thought the movie was\"\\n\\n\\n\\n'"
 ],
 "application/vnd.google.colaboratory.intrinsic+json": {
 "type": "string"
 }
 },
 "metadata": {},
 "execution_count": 78
 }
 ],
 "source": "\"\"\"\n\nPre-text and post-text that performed poorly:\nprompt_pre_text = 'You are an expert movie review sentiment detector. Below is a movie review.'\n\n[review]\n\nprompt_post_text = 'Given the above sentiment, use the information to determine if the review was good (1) or bad (0).'\n\nPre-text and post-text that performed well:\nprompt_pre_text = \"Here is a movie review: \"Great film!\" ... The reviewer classifies reviews as good or bad. In this case they thought the movie was good\nHere is a movie review: \"Terrible movie.\" ... The reviewer classifies reviews as good or bad. In this case they thought the movie was bad\nHere is a movie review: \"Loved the acting!\" ... The reviewer classifies reviews as good or bad. In this case they thought the movie was good\nHere is a movie review: \"Boring plot.\" ... The reviewer classifies reviews as good or bad. In this case they thought the movie was bad\nHere is a movie review: \"\n\n[review]\n\nprompt_post_text = \" ... The reviewer classifies reviews as good or bad. In this case they thought the movie was\"\n\n\"\"\""
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "MEYWYxPnh6-x"
 },
 "source": "Now let's do the fine-tuning that is supposed to help! Start by getting the full dataset and dataloaders:"
 },
 {
 "cell_type": "code",
 "execution_count": 52,
 "metadata": {
 "id": "6seAp7dmFtrJ",
 "colab": {
 "base_uri": "https://localhost:8080/"
 },
 "executionInfo": {
 "status": "ok",
 "timestamp": 1759712508609,
 "user_tz": 360,
 "elapsed": 184580,
 "user": {
 "displayName": "Ryan C",
 "userId": "13217471319505539255"
 }
 },
 "outputId": "1fd40135-99a3-4d6e-8bc2-0d64a949425b"
 },
 "outputs": [
 {
 "output_type": "stream",
 "name": "stdout",
 "text": [
 "{'text': \"Normally I would never rent a movie like this, because you know it's going to be bad just by looking at the box. I rented seven movies at the same time, including Nightmare on Elm Street 5, 6 and Wes Craven's New Nightmare. Unfortunately, when I got home I found out the videostore-guy gave me the wrong tape. In the box of Wes Craven's New Nightmare I found this lame movie.<br /><br />This movie is incredibly boring, the acting is bad and the plot doesn't make any sense. It's hard to write a good review, because I have no idea what the movie was really about. At the end of the movie you have more questions then answers.<br /><br />On 'Max Power's Scale of 1 to 10' I rate this movie: 1<br /><br />PS I would like to correct Corinthian's review (right below mine). He says Robert Englund is ripping off lingerie, riding horses naked, etc. The guy that did those things was Mahmoud, played by Juliano Mer, not by Robert Englund.\", 'label': 0}\n"
 ]
 },
 {
 "output_type": "stream",
 "name": "stderr",
 "text": [
 "Token indices sequence length is longer than the specified maximum sequence length for this model (1038 > 1024). Running this sequence through the model will result in indexing errors\n"
 ]
 },
 {
 "output_type": "stream",
 "name": "stdout",
 "text": [
 "{'text': \"I know most of the other reviews say that this movie was great, but I have to disagree.<br /><br />Sure, it's a good book! It was actually one of my favorites when I was verrry little. But it's just not meant for theaters. Maybe for a little half-hour short, but I don't see how they can turn a short kiddie book into a whole feature film.<br /><br />It is a cute movie, but I would only recommend it for really little kids. Older kids will have no interest it. Adults may have a little more interest if they watch it with their young ones. But anyone ages 7-Adult will have a snore-fest.<br /><br />Sorry if you disagree with me, but this is my opinion. :)\", 'label': 0}\n"
 ]
 }
 ],
 "source": "imdb_train_data = ClassificationData(imdb_train_set,\n tokenizer=gpt_2_tokenizer,\n max_len=100,\n use_prompt=True,\n prompt_pre_text = prompt_pre_text,\n prompt_post_text = prompt_post_text,\n classification_tokenset=classification_tokenset,\n num_examples=-1\n )\n\nimdb_test_data = ClassificationData(imdb_test_set,\n tokenizer=gpt_2_tokenizer,\n max_len=100,\n use_prompt=True,\n prompt_pre_text = prompt_pre_text,\n prompt_post_text = prompt_post_text,\n classification_tokenset=classification_tokenset,\n num_examples=-1\n )\n\nbatch_size = 4\nimdb_train_loader = DataLoader(imdb_train_data, batch_size=batch_size, shuffle=True)\n\nimdb_test_loader = DataLoader(imdb_test_data, batch_size=batch_size, shuffle=True)"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "6lMTx6IUMFyv"
 },
 "source": "Let's set up the optimizers as before:"
 },
 {
 "cell_type": "code",
 "execution_count": 53,
 "metadata": {
 "id": "Zer3mvWGK89C",
 "executionInfo": {
 "status": "ok",
 "timestamp": 1759712569885,
 "user_tz": 360,
 "elapsed": 15,
 "user": {
 "displayName": "Ryan C",
 "userId": "13217471319505539255"
 }
 }
 },
 "outputs": [],
 "source": "adam_optimizer_base_model = torch.optim.AdamW(token_prediction_network_base_model.parameters(), lr=0.00001)\nadam_optimizer_addtl_pretrain_model = torch.optim.AdamW(token_prediction_network_addnl_pretrain_model.parameters(), lr=0.00001)"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "wA2F_LP-5Wqe"
 },
 "source": "Here is the new training loop. Please fill in the lines for optimizer zeroing, the prediction calculation, and the loss."
 },
 {
 "cell_type": "code",
 "execution_count": 54,
 "metadata": {
 "id": "a5B2VrbX1uLN",
 "executionInfo": {
 "status": "ok",
 "timestamp": 1759712690333,
 "user_tz": 360,
 "elapsed": 17,
 "user": {
 "displayName": "Ryan C",
 "userId": "13217471319505539255"
 }
 }
 },
 "outputs": [],
 "source": "def train_loop(dataloader, model, loss_fn, optimizer, reporting_interval=100, steps=None):\n\n \"\"\"\n Write the training loop to fine-tune the model for sentiment\n classification using the final next-token-prediction task.\n In particular, you need to:\n - initialize the epoch_loss to 0 and set the model into training mode\n - iterate over the batches:\n - break if you are at 'steps' number of batches\n - get the inputs X and labels y (which in this case will be the actual next token)\n - get the model outputs\n - reshape y and model outputs in proper format for cross entropy calculation\n - zero out the gradient\n - calculate loss\n - propagate loss (loss.backward) and apply optimizer step\n - add the loss to the epoch_loss\n Reporting:\n - report the current average loss every 'reporting_interval' batches\n - report the average loss at the end of the epoch (done for you)\n\n \"\"\"\n\n # initialize the epoch_loss to 0 and set the model into training mode\n epoch_loss = 0\n model.train()\n\n # iterate over the batches\n for batch, example in enumerate(dataloader):\n\n # get the inputs X and labels y (which in this case will be the actual next token)\n X = example['input_ids'].to(device)\n y = example['label'].to(device)\n\n # break if you are at 'steps' number of batches\n if steps is not None:\n if batch >= steps:\n break\n\n # Compute prediction and loss\n pred = model(X)\n\n # zero out the gradient\n optimizer.zero_grad() # the gradients need to be zeroed out after the gradients are applied by the optimizer\n\n # calculate loss\n loss = loss_fn(pred, y)\n\n # Backpropagation\n # propagate loss (loss.backward) and apply optimizer step\n loss.backward()\n optimizer.step()\n\n epoch_loss += loss.item()\n\n if int(batch + 1) % reporting_interval == 0:\n print('\\tFinished batches: ', str(batch + 1))\n print('\\tCurrent average loss: ', epoch_loss/batch)\n\n print(f\"Training Results: \\n Avg train loss: {epoch_loss/batch:>8f} \\n\")\n\ndef test_loop(dataloader, model, loss_fn, reporting_interval=100, contrast_pair=None,steps=None):\n \"\"\"\n Write the test loop to fine-tune the model for sentiment classification using the final next-token-prediction task.\n In particular, you need to:\n\n - set the model into eval mode and initialize the test_loss to 0. Also, set the number of correct &\n total test examples to 0, like:\n 'test_loss, correct_token_predictions, correct_label_class, total = 0, 0, 0, 0'\n (See the two approaches for accuracy below for correct_token_predictions\n and correct_label_class)\n\n - use torch.no_grad to iterate over the batches:\n - break if you are at 'steps' number of batches\n - from the batch, get the test inputs X and labels y (which in this case will be the actual next token). You may want to look at the format of batches by using 'next(iter(imdb_test_loader))' in a separate cell\n - get the model outputs\n - calculate loss and add to test_loss (reshaping should not be necessary)\n - For the accuracy, we can try two approaches (and in this case they should turn out to be\n probably the same in the end):\n i) Test Class Accuracy:\n - Define the predicted class (I call it selected class) by comparing the logits for our two\n 'evaluating tokens' (like 'good', 'bad'). if the logit for (in this example) 'good' is higher,\n then the predicted class is the positive one, etc.\n ii) Token Prediction Accuracy:\n - see how often the correct 'evaluation token' is predicted. I.e., here we do not compare\n whether the model believes that 'good' is a more likely next token than 'bad', but was\n 'good' the actual next token prediction (and vise versa).\n - get these numbers for each batch and add to the totals\n\n - add the loss to the epoch_loss\n\n Reporting:\n\n - report on the average token accuracy, average class accuracy and average test loss every 'reporting_interval' batches\n - report on the same at the end (done for you)\n\n \"\"\"\n\n # let's get the proper class ids for the 'evaluating next tokens' (like 'good', 'bad')\n\n if contrast_pair is not None:\n class_1, class_2 = contrast_pair\n class_1_id, class_2_id = gpt_2_tokenizer.encode(' ' + class_1 + ' ' + class_2)\n\n # now the loop starts:\n\n # - set the model into eval mode and initialize the test_loss to 0. Also, set the number of correct &\n # total test examples to 0, like:\n # 'test_loss, correct_token_predictions, correct_label_class, total = 0, 0, 0, 0'\n # (See the two approaches for accuracy below for correct_token_predictions\n # and correct_label_class)\n model.eval()\n test_loss, correct_token_predictions, correct_label_class, total = 0, 0, 0, 0\n\n # use torch.no_grad to iterate over the batches:\n with torch.no_grad():\n # break if you are at 'steps' number of batches\n for batch, example in enumerate(dataloader):\n if steps is not None:\n if int(batch) > steps:\n break\n\n X = example['input_ids']\n y = example['label']\n\n # get the model outputs\n pred = model(X)\n\n # calculate loss and add to test_loss (reshaping should not be necessary)\n test_loss += loss_fn(pred, y).item()\n\n # Approach 1: Class Accuracy (compare 'good' vs 'bad' logits)\n if contrast_pair is not None:\n logit_class_1 = pred[:, class_1_id] # Logits for 'good'\n logit_class_2 = pred[:, class_2_id] # Logits for 'bad'\n predicted_class = (logit_class_1 > logit_class_2).long() # 1 if good>bad, else 0\n\n # Convert y (token IDs) to binary labels (0 or 1)\n true_class = (y == class_1_id).long() # 1 if label is class_1 ('good'), 0 if class_2 ('bad')\n\n correct_label_class += (predicted_class == true_class).sum().item()\n\n # Approach 2: Token Prediction Accuracy (argmax across all vocab)\n predicted_token = torch.argmax(pred, dim=-1) # Which token has highest logit?\n correct_token_predictions += (predicted_token == y).sum().item()\n\n # Update total\n total += y.size(0)\n\n # Reporting\n if (batch + 1) % reporting_interval == 0:\n print(f'\\tBatch {batch + 1}:')\n print(f'\\t Token Accuracy: {correct_token_predictions/total:.4f}')\n print(f'\\t Class Accuracy: {correct_label_class/total:.4f}')\n print(f'\\t Avg Loss: {test_loss/(batch+1):.4f}')\n\n # Final results\n test_loss /= (batch + 1)\n correct = correct_token_predictions / total\n correct_label_class = correct_label_class / total\n\n print(correct_label_class)\n print(f\"Test Results: \\n\\t Test Token Accuracy: {(100*correct):>0.1f}% \\n\\t Test Class Accuracy: {(100*correct_label_class):>0.1f}% \\n\\t Avg test loss: {test_loss:>8f} \\n\")"
 },
 {
 "cell_type": "code",
 "execution_count": 55,
 "metadata": {
 "id": "tfYYfoCK1uOX",
 "colab": {
 "base_uri": "https://localhost:8080/"
 },
 "executionInfo": {
 "status": "ok",
 "timestamp": 1759713342093,
 "user_tz": 360,
 "elapsed": 640146,
 "user": {
 "displayName": "Ryan C",
 "userId": "13217471319505539255"
 }
 },
 "outputId": "a6e8a1de-e8a1-48a9-b664-712b0eba6248"
 },
 "outputs": [
 {
 "output_type": "stream",
 "name": "stdout",
 "text": [
 "Epoch 1\n",
 "-------------------------------\n",
 "\tFinished batches: 100\n",
 "\tCurrent average loss: 0.7811682582503617\n",
 "\tFinished batches: 200\n",
 "\tCurrent average loss: 0.6736215971522594\n",
 "\tFinished batches: 300\n",
 "\tCurrent average loss: 0.6333440633920523\n",
 "\tFinished batches: 400\n",
 "\tCurrent average loss: 0.582923167107398\n",
 "\tFinished batches: 500\n",
 "\tCurrent average loss: 0.5483978362250065\n",
 "\tFinished batches: 600\n",
 "\tCurrent average loss: 0.5260849337844499\n",
 "\tFinished batches: 700\n",
 "\tCurrent average loss: 0.5032580876949434\n",
 "\tFinished batches: 800\n",
 "\tCurrent average loss: 0.49099839197585965\n",
 "\tFinished batches: 900\n",
 "\tCurrent average loss: 0.4802822139035409\n",
 "\tFinished batches: 1000\n",
 "\tCurrent average loss: 0.4746616259590164\n",
 "\tFinished batches: 1100\n",
 "\tCurrent average loss: 0.4682099017415457\n",
 "\tFinished batches: 1200\n",
 "\tCurrent average loss: 0.46888057547182566\n",
 "\tFinished batches: 1300\n",
 "\tCurrent average loss: 0.4626554859625756\n",
 "\tFinished batches: 1400\n",
 "\tCurrent average loss: 0.4566656412893615\n",
 "\tFinished batches: 1500\n",
 "\tCurrent average loss: 0.4523689461383643\n",
 "\tFinished batches: 1600\n",
 "\tCurrent average loss: 0.4460410417125719\n",
 "\tFinished batches: 1700\n",
 "\tCurrent average loss: 0.44328512885049204\n",
 "\tFinished batches: 1800\n",
 "\tCurrent average loss: 0.44110352264767955\n",
 "\tFinished batches: 1900\n",
 "\tCurrent average loss: 0.4375069360569975\n",
 "\tFinished batches: 2000\n",
 "\tCurrent average loss: 0.43450564369391564\n",
 "Training Results: \n",
 " Avg train loss: 0.434288 \n",
 "\n",
 "\tBatch 100:\n",
 "\t Token Accuracy: 0.8700\n",
 "\t Class Accuracy: 0.8700\n",
 "\t Avg Loss: 0.2978\n",
 "\tBatch 200:\n",
 "\t Token Accuracy: 0.8675\n",
 "\t Class Accuracy: 0.8675\n",
 "\t Avg Loss: 0.3115\n",
 "\tBatch 300:\n",
 "\t Token Accuracy: 0.8667\n",
 "\t Class Accuracy: 0.8667\n",
 "\t Avg Loss: 0.3076\n",
 "\tBatch 400:\n",
 "\t Token Accuracy: 0.8675\n",
 "\t Class Accuracy: 0.8675\n",
 "\t Avg Loss: 0.3034\n",
 "\tBatch 500:\n",
 "\t Token Accuracy: 0.8660\n",
 "\t Class Accuracy: 0.8660\n",
 "\t Avg Loss: 0.3114\n",
 "0.8664650698603\n",
 "Test Results: \n",
 "\t Test Token Accuracy: 86.6% \n",
 "\t Test Class Accuracy: 86.6% \n",
 "\t Avg test loss: 0.310352 \n",
 "\n",
 "Done!\n"
 ]
 }
 ],
 "source": "epochs = 1\nfor t in range(epochs):\n print(f\"Epoch {t+1}\\n-------------------------------\")\n train_loop(imdb_train_loader, token_prediction_network_base_model, loss_fn, adam_optimizer_base_model, steps=2000)\n test_loop(imdb_test_loader, token_prediction_network_base_model, loss_fn, contrast_pair=('good', 'bad'),\n steps=500\n ) # no optimizer use here!\nprint(\"Done!\")"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "MkfPjUBE8JWu"
 },
 "source": "**2.b. What was your test accuracy after fine-tuning, when starting with the base model**\n"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "cxWvZSg4WAzj"
 },
 "source": "Now we redo this for the model that saw the additional pre-training:"
 },
 {
 "cell_type": "code",
 "execution_count": 57,
 "metadata": {
 "id": "O20xrf0gmscR",
 "colab": {
 "base_uri": "https://localhost:8080/"
 },
 "executionInfo": {
 "status": "ok",
 "timestamp": 1759714145369,
 "user_tz": 360,
 "elapsed": 641130,
 "user": {
 "displayName": "Ryan C",
 "userId": "13217471319505539255"
 }
 },
 "outputId": "2535fbc5-5113-4228-bd82-4aa9225fcba3"
 },
 "outputs": [
 {
 "output_type": "stream",
 "name": "stdout",
 "text": [
 "Epoch 1\n",
 "-------------------------------\n",
 "\tFinished batches: 100\n",
 "\tCurrent average loss: 0.789243405816531\n",
 "\tFinished batches: 200\n",
 "\tCurrent average loss: 0.6751770183968184\n",
 "\tFinished batches: 300\n",
 "\tCurrent average loss: 0.627073577066329\n",
 "\tFinished batches: 400\n",
 "\tCurrent average loss: 0.5865007371158528\n",
 "\tFinished batches: 500\n",
 "\tCurrent average loss: 0.5655358547945778\n",
 "\tFinished batches: 600\n",
 "\tCurrent average loss: 0.5435359177548121\n",
 "\tFinished batches: 700\n",
 "\tCurrent average loss: 0.5210298326223535\n",
 "\tFinished batches: 800\n",
 "\tCurrent average loss: 0.5081120903728453\n",
 "\tFinished batches: 900\n",
 "\tCurrent average loss: 0.49517747114419736\n",
 "\tFinished batches: 1000\n",
 "\tCurrent average loss: 0.48645565905668836\n",
 "\tFinished batches: 1100\n",
 "\tCurrent average loss: 0.4806669885097741\n",
 "\tFinished batches: 1200\n",
 "\tCurrent average loss: 0.4708087187729546\n",
 "\tFinished batches: 1300\n",
 "\tCurrent average loss: 0.4640757692669496\n",
 "\tFinished batches: 1400\n",
 "\tCurrent average loss: 0.45518255997861895\n",
 "\tFinished batches: 1500\n",
 "\tCurrent average loss: 0.45099530424587203\n",
 "\tFinished batches: 1600\n",
 "\tCurrent average loss: 0.44468594366933\n",
 "\tFinished batches: 1700\n",
 "\tCurrent average loss: 0.439416553036887\n",
 "\tFinished batches: 1800\n",
 "\tCurrent average loss: 0.43537405742126\n",
 "\tFinished batches: 1900\n",
 "\tCurrent average loss: 0.43156254741256805\n",
 "\tFinished batches: 2000\n",
 "\tCurrent average loss: 0.4275371373909199\n",
 "Training Results: \n",
 " Avg train loss: 0.427323 \n",
 "\n",
 "\tBatch 100:\n",
 "\t Token Accuracy: 0.8850\n",
 "\t Class Accuracy: 0.8850\n",
 "\t Avg Loss: 0.2881\n",
 "\tBatch 200:\n",
 "\t Token Accuracy: 0.8512\n",
 "\t Class Accuracy: 0.8512\n",
 "\t Avg Loss: 0.3536\n",
 "\tBatch 300:\n",
 "\t Token Accuracy: 0.8500\n",
 "\t Class Accuracy: 0.8500\n",
 "\t Avg Loss: 0.3463\n",
 "\tBatch 400:\n",
 "\t Token Accuracy: 0.8575\n",
 "\t Class Accuracy: 0.8575\n",
 "\t Avg Loss: 0.3374\n",
 "\tBatch 500:\n",
 "\t Token Accuracy: 0.8540\n",
 "\t Class Accuracy: 0.8540\n",
 "\t Avg Loss: 0.3395\n",
 "0.8537924151696606\n",
 "Test Results: \n",
 "\t Test Token Accuracy: 85.4% \n",
 "\t Test Class Accuracy: 85.4% \n",
 "\t Avg test loss: 0.339061 \n",
 "\n",
 "Done!\n"
 ]
 }
 ],
 "source": "epochs = 1\nfor t in range(epochs):\n print(f\"Epoch {t+1}\\n-------------------------------\")\n train_loop(imdb_train_loader, token_prediction_network_addnl_pretrain_model, loss_fn, adam_optimizer_addtl_pretrain_model, steps=2000)\n test_loop(imdb_test_loader, token_prediction_network_addnl_pretrain_model, loss_fn, contrast_pair=('good', 'bad'),\n steps=500\n ) # no optimizer use here!\nprint(\"Done!\")"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "221hDSFSaiee"
 },
 "source": "This looks good! So we had better movie review sentiment classification using the model had had seen the additional pretraining."
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "RGt5EUN2iV1i"
 },
 "source": "**2.c. What was your test accuracy after fine-tuning, when starting with the model that had additional pre-training**\n"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "onqyZaW1iWC_"
 },
 "source": "**2.d. Based on this and what we saw in the previous section (and, as there are statistical fluctuations, based on what 'should' be the case), what would be your expectation for these two starting models when used for sentiment analysis tasks that deal with data **inside** the movie domain? ('base model slightly better', or 'additional pretrain model slightly better')**\n"
 },
 {
 "cell_type": "code",
 "execution_count": 79,
 "metadata": {
 "id": "SBQdmmNfiWLZ",
 "colab": {
 "base_uri": "https://localhost:8080/",
 "height": 35
 },
 "executionInfo": {
 "status": "ok",
 "timestamp": 1759716303813,
 "user_tz": 360,
 "elapsed": 59,
 "user": {
 "displayName": "Ryan C",
 "userId": "13217471319505539255"
 }
 },
 "outputId": "43d26474-01c5-4a65-8796-17f8a8babba2"
 },
 "outputs": [
 {
 "output_type": "execute_result",
 "data": {
 "text/plain": [
 "'\\nThe additional pretrain model performed slightly better than the base model.\\nThis is what we expected, as it was further trained on data inside the movie domain.\\n'"
 ],
 "application/vnd.google.colaboratory.intrinsic+json": {
 "type": "string"
 }
 },
 "metadata": {},
 "execution_count": 79
 }
 ],
 "source": "\"\"\"\nThe additional pretrain model performed slightly better than the base model.\nThis is what we expected, as it was further trained on data inside the movie domain.\n\"\"\""
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "GrpNZT03iWS2"
 },
 "source": "**2.e. Based on this and what we saw in the previous section (and, as there are statistical fluctuations, based on what 'should' be the case), what would be your expectation for these two starting models when used for sentiment analysis tasks that deal with data **outside** the movie domain? ('base model slightly better', or 'additional pretrain model slightly better')**\n"
 },
 {
 "cell_type": "code",
 "execution_count": 80,
 "metadata": {
 "id": "58K0RCyqiWdL",
 "colab": {
 "base_uri": "https://localhost:8080/",
 "height": 35
 },
 "executionInfo": {
 "status": "ok",
 "timestamp": 1759716308494,
 "user_tz": 360,
 "elapsed": 16,
 "user": {
 "displayName": "Ryan C",
 "userId": "13217471319505539255"
 }
 },
 "outputId": "3b16259f-75e9-460b-b1be-89f21f6b4a73"
 },
 "outputs": [
 {
 "output_type": "execute_result",
 "data": {
 "text/plain": [
 "'\\nThe base model should perform slightly slightly better when dealing with data outside the movie domain.\\n'"
 ],
 "application/vnd.google.colaboratory.intrinsic+json": {
 "type": "string"
 }
 },
 "metadata": {},
 "execution_count": 80
 }
 ],
 "source": "\"\"\"\nThe base model should perform slightly slightly better when dealing with data outside the movie domain.\n\"\"\""
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "UyUulgoDuf0_"
 },
 "source": "## 3. Sentiment Classification with BERT\n\nNow we will see how well the classification with BERT works in comparison. We will get the model tokenizer for that model, then - as discussed in class - use the output of the initial [CLS] token to classify the sentiment. Will it be better? Or worse?\n\nSee https://huggingface.co/docs/transformers/model_doc/bert#transformers.BertModel for more details around the model."
 },
 {
 "cell_type": "code",
 "execution_count": 62,
 "metadata": {
 "id": "vU5Aa6nY1vrM",
 "executionInfo": {
 "status": "ok",
 "timestamp": 1759714233785,
 "user_tz": 360,
 "elapsed": 7578,
 "user": {
 "displayName": "Ryan C",
 "userId": "13217471319505539255"
 }
 }
 },
 "outputs": [],
 "source": "%%capture\n\nfrom transformers import AutoTokenizer, BertModel\n\nbert_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\nbert_model = BertModel.from_pretrained(\"bert-base-cased\").to(device)"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "ZrILbM7Tuf0_"
 },
 "source": "Let us look at a simple bert tokenization"
 },
 {
 "cell_type": "code",
 "execution_count": 64,
 "metadata": {
 "id": "d63Uoyx_gbiP",
 "executionInfo": {
 "status": "ok",
 "timestamp": 1759714777601,
 "user_tz": 360,
 "elapsed": 417,
 "user": {
 "displayName": "Ryan C",
 "userId": "13217471319505539255"
 }
 },
 "colab": {
 "base_uri": "https://localhost:8080/"
 },
 "outputId": "f785f39a-7bc3-4b39-9a9a-6dc859d4b5b1"
 },
 "outputs": [
 {
 "output_type": "execute_result",
 "data": {
 "text/plain": [
 "torch.Size([1, 5, 768])"
 ]
 },
 "metadata": {},
 "execution_count": 64
 }
 ],
 "source": "bert_toy_inputs = bert_tokenizer(\"This is new\", return_tensors=\"pt\").to(device)\n\nbert_toy_outputs = bert_model(**bert_toy_inputs)\n\nlast_hidden_states = bert_toy_outputs.last_hidden_state\n\nlast_hidden_states.shape"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "7g-wccv3STm0"
 },
 "source": "Play with decode method of the tokenizer to see why the shape is ... x 5 x ... . Then identify the first value of the output of the [CLS] token."
 },
 {
 "cell_type": "code",
 "execution_count": 65,
 "metadata": {
 "id": "_W0c0wJ1Rwjf",
 "executionInfo": {
 "status": "ok",
 "timestamp": 1759714779662,
 "user_tz": 360,
 "elapsed": 14,
 "user": {
 "displayName": "Ryan C",
 "userId": "13217471319505539255"
 }
 },
 "colab": {
 "base_uri": "https://localhost:8080/"
 },
 "outputId": "d997ed2d-dc86-4471-c618-ea0261b30f77"
 },
 "outputs": [
 {
 "output_type": "stream",
 "name": "stdout",
 "text": [
 "The tokens after tokenization: [CLS] This is new [SEP]\n"
 ]
 }
 ],
 "source": "# Decode the tokenization. Call it bert_toy_tokens.\nbert_toy_tokens = bert_tokenizer.decode(bert_toy_inputs['input_ids'][0])\nprint('The tokens after tokenization: ', bert_toy_tokens)"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "I8QxOlH7XtoT"
 },
 "source": "**Demonstration:**\n3.a (2 pt) Why is the shape .. x 5 x ... and not .. x 3 x ... ? Explain. (You may need to to look up what the purpose is of one of the extra tokens. Don't write more than 2-3 lines.)"
 },
 {
 "cell_type": "code",
 "execution_count": 81,
 "metadata": {
 "id": "Q-Lc1ysqi9_n",
 "executionInfo": {
 "status": "ok",
 "timestamp": 1759716318106,
 "user_tz": 360,
 "elapsed": 29,
 "user": {
 "displayName": "Ryan C",
 "userId": "13217471319505539255"
 }
 },
 "colab": {
 "base_uri": "https://localhost:8080/",
 "height": 53
 },
 "outputId": "e90a656f-c74d-4aab-fe13-d84b32ada661"
 },
 "outputs": [
 {
 "output_type": "execute_result",
 "data": {
 "text/plain": [
 "'\\nThere are 5 total tokens in our sentence even though there are 3 words - [CLS] This is new [SEP]\\n[CLS] refers to classification and is designed to capture information about the entire sentence. (Usually used for sentiment analysis etc.)\\n[SEP] stands for \"separator\" - it marks the boundary between sentences (or the end of the input).\\n'"
 ],
 "application/vnd.google.colaboratory.intrinsic+json": {
 "type": "string"
 }
 },
 "metadata": {},
 "execution_count": 81
 }
 ],
 "source": "\"\"\"\nThere are 5 total tokens in our sentence even though there are 3 words - [CLS] This is new [SEP]\n[CLS] refers to classification and is designed to capture information about the entire sentence. (Usually used for sentiment analysis etc.)\n[SEP] stands for \"separator\" - it marks the boundary between sentences (or the end of the input).\n\"\"\""
 },
 {
 "cell_type": "code",
 "execution_count": 67,
 "metadata": {
 "id": "lIjgqCXdRwvL",
 "executionInfo": {
 "status": "ok",
 "timestamp": 1759714838801,
 "user_tz": 360,
 "elapsed": 56,
 "user": {
 "displayName": "Ryan C",
 "userId": "13217471319505539255"
 }
 },
 "colab": {
 "base_uri": "https://localhost:8080/"
 },
 "outputId": "70b8a7c8-cc4a-4e15-fcaa-e2b3414a859c"
 },
 "outputs": [
 {
 "output_type": "stream",
 "name": "stdout",
 "text": [
 "First output of [CLS] token: tensor([ 4.7248e-01, 1.7685e-01, 5.4234e-01, -5.4906e-01, -1.1503e-01,\n",
 " 1.1630e-01, 4.2311e-01, 2.2489e-02, -1.9720e-01, -9.8925e-01,\n",
 " -4.3665e-01, 3.0220e-01, -4.5189e-01, 1.0892e-01, -6.0474e-01,\n",
 " 2.3494e-01, 1.0624e-01, 3.3463e-01, 9.6197e-02, -7.3822e-02,\n",
 " 1.8399e-01, -2.7128e-01, 6.3856e-01, -2.2248e-01, 4.8449e-01,\n",
 " -2.6155e-01, 6.0228e-01, 1.4545e-01, 4.9358e-02, 4.4853e-01,\n",
 " -5.3474e-02, 6.7214e-02, 4.8560e-02, -6.9567e-02, 3.0451e-02,\n",
 " -1.2569e-01, 1.5189e-02, -6.5966e-01, -5.1185e-03, -1.5514e-01,\n",
 " -5.0256e-01, 2.3907e-01, 2.9280e-01, -1.1793e-01, 5.0254e-01,\n",
 " -7.8381e-01, -8.3310e-02, -1.2723e-01, -4.2026e-01, 1.1898e-01,\n",
 " 7.1867e-02, 1.9298e-01, -5.5773e-02, 3.2795e-01, -5.6385e-03,\n",
 " -1.5043e-01, -5.4290e-01, 3.0932e-01, -5.1025e-01, 2.8927e-01,\n",
 " 1.8317e-01, -4.7179e-03, 4.0482e-01, -2.3335e-03, -5.9042e-02,\n",
 " -9.0398e-02, -8.2895e-02, 1.5881e-01, -4.7864e-01, -2.8077e-01,\n",
 " -1.0255e-01, 1.9993e-01, 4.3991e-01, 1.0241e+00, 4.6022e-01,\n",
 " -2.7858e-01, 3.4326e-01, 3.9601e-01, 5.6149e-02, 8.6239e-02,\n",
 " 2.6598e-01, 4.4734e-01, -2.9577e-02, -2.6248e-01, 9.4554e-02,\n",
 " -8.9572e-03, 4.1349e-01, -5.2663e-02, -1.4059e-01, 3.5523e-01,\n",
 " 2.5458e-01, -1.1831e-01, -7.5502e-01, -2.9882e-01, 4.3147e-02,\n",
 " 3.6292e-01, 1.7790e-02, 1.8058e-01, 6.1829e+00, -1.4481e-01,\n",
 " 1.3347e-01, -9.4680e-02, 7.5315e-01, 5.9179e-02, 6.0372e-01,\n",
 " -9.9446e-02, -1.6166e-02, -1.1691e-01, 2.7946e-01, 4.6337e-01,\n",
 " 2.8236e-01, -3.4459e-01, 9.7477e-02, -2.9536e-01, -2.5241e-01,\n",
 " -2.4220e-01, -6.8551e-02, 4.2699e-01, 1.9218e-01, -3.3008e-01,\n",
 " 1.5981e-01, 1.1538e-01, 1.3907e+00, 3.4374e-01, -3.4655e-01,\n",
 " -1.4068e-01, -2.8949e-01, 3.7642e-02, 2.3393e-01, -3.2027e-01,\n",
 " -4.8399e-01, -4.5009e-01, -1.4058e-01, -3.3540e-02, 1.8438e-01,\n",
 " 1.3968e-02, 1.6821e-03, -5.5849e-02, -8.0941e-01, 3.8881e-01,\n",
 " 3.4365e-01, -4.1117e-01, -2.0996e-01, -1.0344e-01, -4.0679e-01,\n",
 " 2.8167e+00, -9.6546e-02, -2.9524e-01, -9.5186e-02, -2.2905e-01,\n",
 " 2.4526e-01, -1.6460e-01, -5.5657e-01, -3.6732e-02, -2.2867e-01,\n",
 " -1.6067e-01, 1.3233e-01, -2.2837e-01, 2.5995e-01, -8.4863e-03,\n",
 " -1.0144e+00, 1.2996e-01, -1.2879e-01, 3.8767e-01, 1.8762e-01,\n",
 " -3.8737e-01, 3.1376e-01, -7.3456e-01, 4.6178e-01, 4.6601e-01,\n",
 " -5.7279e-02, -2.1166e-01, -1.9663e+00, 3.8102e-01, 2.2589e-01,\n",
 " 3.0810e-01, 2.5124e-01, 1.6685e-01, -1.7925e-01, -2.7407e-01,\n",
 " 4.5863e-02, 4.5874e-01, -1.2928e-01, -1.5197e-01, -4.1277e-01,\n",
 " 3.1108e-01, -3.5875e-02, 2.4647e-01, 9.1209e-02, 1.3496e-01,\n",
 " 2.0606e-02, -1.1449e-01, -2.7051e-02, 1.6689e-01, 3.3853e-03,\n",
 " -9.7869e-03, 1.1335e-01, -3.1098e-01, 1.7264e-01, -8.8281e-02,\n",
 " -2.2569e-01, -5.5168e-02, -2.6836e-01, 1.4090e-01, 1.7592e-01,\n",
 " 4.9786e-03, -1.6281e-01, 2.6746e-01, 3.9191e-01, -1.0610e-01,\n",
 " -3.8932e-01, -1.3837e-01, -1.3374e-01, 6.3297e-02, 2.2207e-01,\n",
 " -3.7249e-01, -1.2220e-01, 2.8922e-01, 3.5506e-02, -2.0174e-01,\n",
 " -3.7285e-01, 2.4903e-01, -2.5731e-01, -4.3991e-01, 2.3476e-01,\n",
 " 2.5616e-02, 3.2749e-01, 1.6465e-01, 5.0752e-02, -6.7244e-01,\n",
 " 2.1953e-01, 4.1756e-02, -2.8575e-01, -4.6060e-01, 1.1920e-02,\n",
 " 1.3773e-01, -1.2792e-01, 9.1088e-02, -2.2541e-01, 2.5843e-01,\n",
 " -1.4994e-01, -3.9694e-01, 4.7565e-01, -8.6203e-02, 9.6953e-03,\n",
 " 5.5563e-01, 3.6184e-01, 2.4583e-01, -1.5536e-01, 2.8015e-01,\n",
 " 1.1784e+00, 2.6259e-01, -4.8109e-01, -4.6087e-01, -4.1646e-01,\n",
 " -1.5751e-01, 3.6877e-02, -1.2398e+00, -3.4839e-01, -1.7673e-01,\n",
 " 3.1261e-02, -3.2313e+00, 5.2254e-01, 3.0759e-01, 2.4908e-01,\n",
 " 4.2150e-02, 1.4895e-01, 1.3280e-01, -5.6454e-01, 1.9543e-01,\n",
 " -2.3949e-01, -4.9030e-01, -3.9484e-01, -7.7504e-02, 3.0822e-01,\n",
 " 2.3232e-01, -2.8774e-01, 1.8830e-01, -1.3519e-01, -3.8548e-01,\n",
 " -1.9003e-01, 3.1351e-01, -3.6871e-01, 6.1885e-01, -1.6753e-01,\n",
 " 6.8572e-01, 4.0731e-01, 1.6437e-01, 2.6510e-01, 3.9992e+00,\n",
 " 4.7597e-02, 9.1097e-02, 1.0259e-01, -2.5647e-01, -2.8696e-01,\n",
 " -8.6141e-02, -4.3966e-01, -2.6102e-01, 2.1829e-01, 1.0150e-01,\n",
 " -1.8444e-01, -1.2895e-01, -4.9873e-01, -1.0664e-01, -3.5579e-01,\n",
 " -2.3180e-01, 8.6542e-03, 2.1243e-01, -7.6511e-01, -1.2854e-01,\n",
 " 4.3679e-01, 1.5846e-01, -1.3408e-01, 1.1934e-01, -3.6677e-01,\n",
 " -5.5662e-01, -5.4969e-01, 3.6808e-01, 2.4230e-02, -1.0814e+00,\n",
 " 1.7460e-01, 1.7868e-01, -1.4528e-01, 8.4023e-02, 3.3936e-01,\n",
 " -3.1761e-01, 3.3891e-01, -1.7439e-01, -1.7383e-01, -1.4517e-02,\n",
 " 9.4919e-03, -3.4670e-01, -2.2801e-01, -4.0658e-01, 9.4699e-02,\n",
 " 4.1574e-02, -5.4721e-01, 1.6076e-01, 5.7045e-02, -1.5887e-02,\n",
 " -1.8827e-01, -1.5159e-01, -2.9561e-01, 1.7302e-01, 3.4877e-01,\n",
 " 1.7287e-01, 3.9085e-02, -7.3917e-02, -1.1163e-01, 7.2686e-02,\n",
 " 2.6188e-01, 1.4374e-01, -6.8238e-02, 4.1183e-01, 1.9411e-01,\n",
 " 1.0892e-01, -3.2740e-01, -1.5114e-01, -3.2178e-02, 3.1989e-01,\n",
 " -1.9256e-01, -2.0294e+00, 2.6039e-01, 5.3210e-01, -3.4126e-01,\n",
 " 1.0440e-01, 2.1031e-01, 1.0412e-01, -1.9871e-01, 1.9260e-01,\n",
 " 4.6711e-01, 2.5916e-01, 3.8663e-01, 1.6859e-01, -4.9086e-02,\n",
 " 1.1726e-01, 1.2388e-01, 6.6973e-01, 3.2925e-01, -4.2917e-01,\n",
 " -3.5519e-01, -7.8366e-02, 4.1667e-01, -1.6898e-02, -1.3027e-01,\n",
 " 5.8862e-01, -1.2800e-01, 5.3281e-02, 5.1495e-03, 1.1792e-01,\n",
 " 3.0008e-01, 1.7620e-01, -4.8659e-02, -2.2432e-01, -2.6372e-01,\n",
 " -2.1650e-01, -1.7877e-01, 3.3201e-01, 3.8241e-01, -1.5593e-01,\n",
 " -1.8369e-01, 1.4587e-01, 4.8484e-02, -2.2366e-01, -2.9394e-01,\n",
 " 1.0221e-01, -5.1559e-02, -1.3901e-01, -1.5145e+00, -7.9092e-02,\n",
 " -4.8847e-02, -3.0673e-01, -2.2894e-01, 7.9123e-02, -2.0714e-01,\n",
 " -8.5067e-02, 1.4560e-01, 4.0261e-01, 3.2169e-01, 2.5772e-01,\n",
 " 1.1420e-01, -5.3751e-02, -2.0642e-02, -3.5173e-01, -4.2789e-01,\n",
 " 2.6998e-01, 2.6421e-03, -3.6448e-02, 4.2779e-01, 2.7270e-01,\n",
 " -5.1822e-01, -3.7839e-01, 2.2932e-02, -2.7526e-01, -3.4890e-01,\n",
 " -5.4904e-01, 2.9176e-01, -8.7513e-02, -1.2797e-01, 5.2686e+00,\n",
 " -7.2348e-01, 3.2521e-01, 1.5254e-01, 1.0246e-02, 6.7198e-02,\n",
 " 1.6410e-01, 1.5005e-01, 5.7457e-01, -1.6311e-01, -4.0099e-01,\n",
 " -3.5496e-01, 2.2200e-01, 1.6090e-01, -3.2830e-01, 9.6469e-02,\n",
 " -3.1262e-01, 6.8539e-02, 9.7217e-02, -3.4155e-01, -4.0651e-01,\n",
 " 5.5636e-02, 1.6953e-01, -2.2583e-01, 7.7536e-01, 5.3596e-01,\n",
 " -1.2859e-01, -5.2086e-01, -1.4417e-01, -3.5269e-01, -3.1791e-01,\n",
 " 1.9880e-01, 1.5808e-02, 5.8676e-01, 3.2680e-01, -1.9595e-01,\n",
 " 1.0380e-01, 1.0469e-01, 4.0632e-01, -8.3488e-02, 2.8314e-01,\n",
 " 1.9067e-01, 9.7493e-01, -2.1578e-01, 4.0659e-01, 3.2358e-01,\n",
 " -5.2701e-02, 3.6179e-03, 4.5459e-02, 4.6516e-01, 6.7060e-02,\n",
 " 2.1880e-01, 2.2476e-01, -5.6227e-01, 1.8627e-01, -9.3659e-02,\n",
 " -1.0859e-01, -4.3540e-02, -2.3e-01, 2.0044e-01, 6.0795e-02,\n",
 " 2.8609e-02, 3.3630e-03, -4.3769e-01, -4.9710e-02, -3.6383e-01,\n",
 " 3.6268e-01, -2.9623e-01, -2.4136e-01, -6.0605e-01, -5.0104e-01,\n",
 " -9.5289e-02, -8.2130e-01, -4.3117e-01, 1.3454e-01, 2.6575e-01,\n",
 " -5.7185e-01, 3.4002e-02, -1.3778e-01, -4.4542e-03, 1.7165e-01,\n",
 " 6.2143e-02, 1.1474e-01, -7.5025e-01, 6.9798e-02, 3.2467e-01,\n",
 " -6.9431e-01, -3.0386e-01, 4.4554e-01, 4.8391e-01, -2.5542e-01,\n",
 " -3.6392e-01, 2.7007e-02, 2.2393e-02, 1.8885e-01, 1.2165e-02,\n",
 " -2.9450e-01, 7.7751e-02, -2.9465e-01, 1.7627e-01, 5.1255e-01,\n",
 " 9.3333e-02, -7.7583e-02, 2.3686e-01, -6.0814e-02, -8.5554e-02,\n",
 " -1.2325e-01, 1.3434e-01, 1.6336e-02, -2.4914e-01, 2.5172e-01,\n",
 " 2.8847e-02, 8.0886e-03, -2.8119e-01, 2.3125e-01, -2.0573e-02,\n",
 " -1.9181e-01, -1.3482e-01, -6.8912e+00, 5.5886e-01, 1.5422e-01,\n",
 " 9.5756e-02, -2.6526e-01, -1.1458e-01, 5.2413e-01, 4.1520e-01,\n",
 " -1.9809e-01, 1.0812e-01, -1.7872e-01, -2.8758e-01, -2.2845e+00,\n",
 " 5.0803e-02, -2.1907e-01, -7.6227e-02, 2.0509e-01, -8.7298e-01,\n",
 " -8.5449e-02, 1.4827e-01, -2.9821e-01, 4.3970e-01, -1.1378e-01,\n",
 " 5.5606e-01, 4.0387e-01, 2.0326e-01, 8.5839e-02, -3.7614e-01,\n",
 " 2.4857e-01, -3.1706e-01, 1.1883e-02, -1.8205e-01, 4.7037e-01,\n",
 " -1.7306e-01, -1.8071e-01, -4.4879e-01, 1.1347e-01, -2.2446e-01,\n",
 " 1.4078e-01, -7.3016e-01, 9.6628e-02, -3.4889e-01, 3.5453e-01,\n",
 " 3.2981e-01, -7.4592e-01, 1.9603e-01, -6.4582e-01, -2.5627e+00,\n",
 " 2.1793e-01, -4.2047e-02, 5.1064e-01, 3.1593e-01, -1.7099e-01,\n",
 " -2.6736e-01, -6.1461e-02, 1.1684e-01, 3.5648e-01, -1.9143e-01,\n",
 " 4.0081e-01, -2.8117e-01, -4.0522e-01, 2.2744e-02, 3.6856e-01,\n",
 " -1.5640e-01, -3.2261e-01, -1.3640e-01, -3.8982e-01, -2.4817e-01,\n",
 " -2.3443e-01, -2.3304e-01, -2.8962e-01, -9.8832e-02, -2.9107e-01,\n",
 " 6.2877e-03, 3.3252e-01, 4.1405e-01, -1.1586e-01, -1.1099e-01,\n",
 " -1.1027e-01, 1.6092e-01, -3.8889e-01, -7.4945e-02, 5.7417e-01,\n",
 " 2.8122e-01, 1.1946e-01, -2.1556e-01, -4.1677e-01, -2.5173e-01,\n",
 " 2.9708e-03, -1.3603e-01, 3.4512e-02, -6.2017e-02, -1.7346e-01,\n",
 " -2.4494e-02, -2.0006e-01, 3.9713e-01, -2.4897e-02, -4.7831e-02,\n",
 " -3.9623e-01, -2.7633e-01, -1.4263e-01, 2.8597e-01, -3.9340e-01,\n",
 " -2.3304e-01, -1.5023e-01, 2.2876e-01, 2.4928e+00, 3.5083e-01,\n",
 " -2.0310e-01, -3.2030e-01, -4.5908e-01, 5.6730e-01, -1.1396e-01,\n",
 " 4.4320e-01, 1.7212e+00, -2.2990e-01, -4.4117e-02, -1.9487e-01,\n",
 " 9.4585e-02, 3.2772e-01, -3.4377e-01, 5.5012e-01, -5.5770e-02,\n",
 " -6.3253e-02, 7.3237e-02, 2.6462e-01, 2.6804e-01, -3.8656e-01,\n",
 " 5.1245e-01, 7.8772e-02, -2.5365e-01, 1.8276e-01, 1.8112e-01,\n",
 " -2.3906e-01, 3.7088e-02, -3.9700e-01, 6.5618e-02, -7.2794e-02,\n",
 " -3.1432e-02, 1.0966e+00, -1.0924e-01, 1.0694e-01, 4.1502e-01,\n",
 " 3.5799e-01, 4.0251e-02, -2.1573e-01, -1.9640e-01, 1.0356e-01,\n",
 " -4.0683e-01, 5.6883e-01, -2.9617e-03, -4.3482e-02, -6.5816e-01,\n",
 " -3.8862e-01, 2.3494e-01, 6.2381e-03, 6.4747e-02, -3.5057e-01,\n",
 " -2.2235e-01, 1.7590e-01, -7.6272e-02, -3.1965e-02, -6.5666e-01,\n",
 " 8.9846e-02, -5.1593e-03, 1.0866e-01, -2.1952e-01, 1.3246e-01,\n",
 " -2.8440e-01, -9.0166e-03, -1.5787e-01, -2.5948e-01, 4.6393e-01,\n",
 " -5.9641e-01, -1.7103e-01, -2.4834e-01, 3.0567e-01, -5.9610e-02,\n",
 " -2.1686e+00, -1.0826e-01, -2.5444e-01, -2.3829e-01, -3.9330e-01,\n",
 " -2.6673e-01, 5.6668e-01, -5.6014e-04, -1.4325e-01, 1.5828e-01,\n",
 " 3.3742e-03, 1.8766e+00, 1.4102e-01, -2.3894e-02, -2.1943e-01,\n",
 " -5.8150e-02, 2.2983e-02, -1.9514e-01, 1.3331e-01, -4.4473e-01,\n",
 " -2.6641e-01, 1.4212e+00, -1.4298e-02, -3.4e-02, 1.3462e-01,\n",
 " 1.1318e-01, 1.2614e-01, 1.8576e-01, -1.2887e-01, 1.3839e-01,\n",
 " 1.1759e-01, 5.5201e-01, 2.1542e-02], device='cuda:0',\n",
 " grad_fn=<SliceBackward0>)\n"
 ]
 }
 ],
 "source": "# Get the output for the [CLS] token. Call it cls_first_out.\ncls_first_out = last_hidden_states[0, 0, :]\n\nprint('First output of [CLS] token: ', cls_first_out)"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "DMiyr5izQPaC"
 },
 "source": "**Demonstration:**\n3.b (1 pt) What is the first value of the output of the [CLS] token?"
 },
 {
 "cell_type": "code",
 "execution_count": 68,
 "metadata": {
 "id": "tFtukAZgjZjr",
 "executionInfo": {
 "status": "ok",
 "timestamp": 1759714848387,
 "user_tz": 360,
 "elapsed": 26,
 "user": {
 "displayName": "Ryan C",
 "userId": "13217471319505539255"
 }
 },
 "colab": {
 "base_uri": "https://localhost:8080/"
 },
 "outputId": "7888e45a-612e-432f-b713-3d8175c4bfc0"
 },
 "outputs": [
 {
 "output_type": "execute_result",
 "data": {
 "text/plain": [
 "tensor(0.4725, device='cuda:0', grad_fn=<SelectBackward0>)"
 ]
 },
 "metadata": {},
 "execution_count": 68
 }
 ],
 "source": "first_value_output = last_hidden_states[0, 0, :][0]\nfirst_value_output"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "ABg5ttdeWfo4"
 },
 "source": "Now we construct the dataset and the dataloader. The BERT dataset class is defined at the beginning."
 },
 {
 "cell_type": "code",
 "execution_count": 69,
 "metadata": {
 "id": "9USxNoQ_WgIg",
 "executionInfo": {
 "status": "ok",
 "timestamp": 1759714952371,
 "user_tz": 360,
 "elapsed": 91232,
 "user": {
 "displayName": "Ryan C",
 "userId": "13217471319505539255"
 }
 },
 "colab": {
 "base_uri": "https://localhost:8080/"
 },
 "outputId": "9b596d9e-53e8-46f7-b48e-0e2b238ddab9"
 },
 "outputs": [
 {
 "output_type": "stream",
 "name": "stderr",
 "text": [
 "Token indices sequence length is longer than the specified maximum sequence length for this model (621 > 512). Running this sequence through the model will result in indexing errors\n"
 ]
 }
 ],
 "source": "bert_train_data = BERTClassificationData(imdb_train_set,\n tokenizer=bert_tokenizer,\n max_len=100,\n num_examples=-1\n )\n\nbert_test_data = BERTClassificationData(imdb_test_set,\n tokenizer=bert_tokenizer,\n max_len=100,\n num_examples=-1\n )\n\nbatch_size = 4\nbert_imdb_train_loader = DataLoader(bert_train_data, batch_size=batch_size, shuffle=True)\nbert_imdb_test_loader = DataLoader(bert_test_data, batch_size=batch_size, shuffle=True)"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "OeghP0kJTqpB"
 },
 "source": "Now build the classification network that uses the output of the [CLS] token for the classification."
 },
 {
 "cell_type": "code",
 "execution_count": 70,
 "metadata": {
 "id": "Yr-vOerxuf0_",
 "executionInfo": {
 "status": "ok",
 "timestamp": 1759715006305,
 "user_tz": 360,
 "elapsed": 20,
 "user": {
 "displayName": "Ryan C",
 "userId": "13217471319505539255"
 }
 }
 },
 "outputs": [],
 "source": "%%capture\n\nclass BERTClassificationNetworkClass(torch.nn.Module):\n \"\"\"\n Write the class for the classification network using\n the Masked Language Model BERT.\n Specificaly, you will need to extract the output of the [CLS] token\n from the BERT model (i.e., the very first token), apply a suitable linear layer,\n and apply the sigmoid function.\n \"\"\"\n\n def __init__(self):\n super().__init__()\n self.lm = bert_model\n self.linear = torch.nn.Linear(768,1)\n self.activation = torch.nn.Sigmoid()\n\n def forward(self, x):\n # Get the forward pass. Apply the BERT model, then the linear layer, and\n # then apply the sigmoid\n model_out = self.lm(**x)['last_hidden_state']\n\n # Specificaly, you will need to extract the output of the [CLS] token\n last_vector = model_out[:, 0]\n\n # apply a suitable linear layer\n linear_out = self.linear(last_vector)\n\n # apply the sigmoid function\n\n sigmoid_output = self.activation(linear_out)\n\n return torch.squeeze(sigmoid_output) # removing 'x 1 x ' dimensions\n\nloss_fn = torch.nn.BCELoss()\n\nbert_classification_model = BERTClassificationNetworkClass().to(device)"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "Jja3T50uYlde"
 },
 "source": "Let's test it. Is the structure correct?"
 },
 {
 "cell_type": "code",
 "execution_count": 71,
 "metadata": {
 "id": "_K1PVgiVYT23",
 "executionInfo": {
 "status": "ok",
 "timestamp": 1759715013997,
 "user_tz": 360,
 "elapsed": 113,
 "user": {
 "displayName": "Ryan C",
 "userId": "13217471319505539255"
 }
 },
 "colab": {
 "base_uri": "https://localhost:8080/"
 },
 "outputId": "68dd2cf4-5a17-44fe-f48f-ac4365431605"
 },
 "outputs": [
 {
 "output_type": "stream",
 "name": "stdout",
 "text": [
 "Output: tensor([0.4382, 0.4113, 0.4126, 0.4069], device='cuda:0',\n",
 " grad_fn=<SqueezeBackward0>)\n",
 "Loss: tensor(0.6023, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n"
 ]
 }
 ],
 "source": "test = next(iter(bert_imdb_train_loader))\n\nout = bert_classification_model({'input_ids': test['input_ids']})\n\nloss = loss_fn(out.float(), test['label'].float())\n\nprint('Output: ', out)\nprint('Loss: ', loss)"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "iPcaxhduYzxK"
 },
 "source": "Good. Finally, we need train and test loops:"
 },
 {
 "cell_type": "code",
 "execution_count": 72,
 "metadata": {
 "id": "OydE2YjmYM6p",
 "executionInfo": {
 "status": "ok",
 "timestamp": 1759715092826,
 "user_tz": 360,
 "elapsed": 15,
 "user": {
 "displayName": "Ryan C",
 "userId": "13217471319505539255"
 }
 }
 },
 "outputs": [],
 "source": "def bert_train_loop(dataloader, model, loss_fn, optimizer, reporting_interval=100, steps=None):\n \"\"\"\n Following the same logic as above, write the training loop to use the\n Masked Language Model BERT for the sentiment classification task. You\n only need to report the average loss after the reporting interval\n and end of each epoch.\n \"\"\"\n\n # initialize the epoch_loss to 0 and set the model into training mode\n epoch_loss = 0\n model.train()\n\n # iterate over the batches\n for batch, example in enumerate(dataloader):\n\n # get the inputs X and labels y (which in this case will be the actual next token)\n X = {k: v for k, v in example.items() if k != 'label'}\n y = example['label'].float()\n\n # break if you are at 'steps' number of batches\n if steps is not None:\n if batch >= steps:\n break\n\n # Compute prediction and loss\n pred = model(X)\n\n # zero out the gradient\n optimizer.zero_grad() # the gradients need to be zeroed out after the gradients are applied by the optimizer\n\n # calculate loss\n loss = loss_fn(pred, y)\n\n # Backpropagation\n # propagate loss (loss.backward) and apply optimizer step\n loss.backward()\n optimizer.step()\n\n epoch_loss += loss.item()\n\n if int(batch + 1) % reporting_interval == 0:\n print('\\tFinished batches: ', str(batch + 1))\n print('\\tCurrent average loss: ', epoch_loss/batch)\n\n print(f\"Training Results: \\n Avg train loss: {epoch_loss/batch:>8f} \\n\")\n\ndef bert_test_loop(dataloader, model, loss_fn, reporting_interval=100, contrast_pair=None,steps=None):\n \"\"\"\n Following the same logic as above, write the test loop to use the\n Masked Language Model BERT for the sentiment classification task.\n Please report on the accuracy after the reporting interval and end of each epoch.\n \"\"\"\n\n model.eval()\n test_loss, correct, total = 0, 0, 0\n\n # use torch.no_grad to iterate over the batches:\n with torch.no_grad():\n # break if you are at 'steps' number of batches\n for batch, example in enumerate(dataloader):\n if steps is not None:\n if int(batch) > steps:\n break\n\n X = {k: v for k, v in example.items() if k != 'label'}\n y = example['label'].float()\n\n # get the model outputs\n pred = model(X)\n\n # calculate loss and add to test_loss (reshaping should not be necessary)\n test_loss += loss_fn(pred, y).item()\n\n # Token Prediction Accuracy\n predictions = (pred > 0.5).long()\n labels_binary = (y > 0.5).long()\n correct += (predictions == labels_binary).sum()\n total += y.size(0)\n\n if int(batch + 1) % reporting_interval == 0:\n print('Accuracy after', str(batch + 1), 'batches:', str(correct/total))\n\n correct = float(correct.cpu().detach().numpy())\n\n test_loss /= batch\n correct /= total\n print(correct)\n\n print(f\"Test Results: \\n Test Accuracy: {(100*correct):>0.1f}%, Avg test loss: {test_loss:>8f} \\n\")"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "x7L9eebiuf0_"
 },
 "source": "Now let's see:"
 },
 {
 "cell_type": "code",
 "execution_count": 73,
 "metadata": {
 "id": "weVynJqHuf0_",
 "executionInfo": {
 "status": "ok",
 "timestamp": 1759715324036,
 "user_tz": 360,
 "elapsed": 222211,
 "user": {
 "displayName": "Ryan C",
 "userId": "13217471319505539255"
 }
 },
 "colab": {
 "base_uri": "https://localhost:8080/"
 },
 "outputId": "2da6e079-6ea3-44ae-eed0-46c409a37f01"
 },
 "outputs": [
 {
 "output_type": "stream",
 "name": "stdout",
 "text": [
 "Epoch 1\n",
 "-------------------------------\n",
 "\tFinished batches: 100\n",
 "\tCurrent average loss: 0.6792303402014453\n",
 "\tFinished batches: 200\n",
 "\tCurrent average loss: 0.6444078345394614\n",
 "\tFinished batches: 300\n",
 "\tCurrent average loss: 0.5909578904360034\n",
 "\tFinished batches: 400\n",
 "\tCurrent average loss: 0.5555226392857263\n",
 "\tFinished batches: 500\n",
 "\tCurrent average loss: 0.5358857868280343\n",
 "\tFinished batches: 600\n",
 "\tCurrent average loss: 0.5178506249788906\n",
 "\tFinished batches: 700\n",
 "\tCurrent average loss: 0.5026920657279836\n",
 "\tFinished batches: 800\n",
 "\tCurrent average loss: 0.49212058256840674\n",
 "\tFinished batches: 900\n",
 "\tCurrent average loss: 0.48639475735312576\n",
 "\tFinished batches: 1000\n",
 "\tCurrent average loss: 0.4797831916803116\n",
 "\tFinished batches: 1100\n",
 "\tCurrent average loss: 0.46711884505429846\n",
 "\tFinished batches: 1200\n",
 "\tCurrent average loss: 0.4608921402610149\n",
 "\tFinished batches: 1300\n",
 "\tCurrent average loss: 0.4592364237093875\n",
 "\tFinished batches: 1400\n",
 "\tCurrent average loss: 0.45157044921190165\n",
 "\tFinished batches: 1500\n",
 "\tCurrent average loss: 0.444323821423241\n",
 "\tFinished batches: 1600\n",
 "\tCurrent average loss: 0.44236278283602565\n",
 "\tFinished batches: 1700\n",
 "\tCurrent average loss: 0.43744370041128955\n",
 "\tFinished batches: 1800\n",
 "\tCurrent average loss: 0.43506019731456536\n",
 "\tFinished batches: 1900\n",
 "\tCurrent average loss: 0.4306683246342523\n",
 "\tFinished batches: 2000\n",
 "\tCurrent average loss: 0.42819525123484853\n",
 "Training Results: \n",
 " Avg train loss: 0.427981 \n",
 "\n",
 "Accuracy after 100 batches: tensor(0.8625, device='cuda:0')\n",
 "Accuracy after 200 batches: tensor(0.8550, device='cuda:0')\n",
 "Accuracy after 300 batches: tensor(0.8517, device='cuda:0')\n",
 "Accuracy after 400 batches: tensor(0.8400, device='cuda:0')\n",
 "Accuracy after 500 batches: tensor(0.8455, device='cuda:0')\n",
 "0.845309381237525\n",
 "Test Results: \n",
 " Test Accuracy: 84.5%, Avg test loss: 0.344226 \n",
 "\n",
 "Done!\n"
 ]
 }
 ],
 "source": "adam_optimizer_bert = torch.optim.AdamW(bert_classification_model.parameters(), lr=0.00001)\n\nepochs = 1\nfor t in range(epochs):\n print(f\"Epoch {t+1}\\n-------------------------------\")\n bert_train_loop(bert_imdb_train_loader, bert_classification_model, loss_fn, adam_optimizer_bert, steps=2000)\n bert_test_loop(bert_imdb_test_loader, bert_classification_model, loss_fn,\n steps=500\n ) # no optimizer use here!\nprint(\"Done!\")"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "_U8i12wAuf1A"
 },
 "source": "**Demonstration:**\n3.c (10 pt) What was the test accuracy you got for the BERT model?\n\nAnd that is it. Congratulations!"
 }
 ],
 "metadata": {
 "accelerator": "GPU",
 "colab": {
 "gpuType": "T4",
 "provenance": []
 },
 "kernelspec": {
 "display_name": "Python 3 (ipykernel)",
 "language": "python",
 "name": "python3"
 },
 "language_info": {
 "codemirror_mode": {
 "name": "ipython",
 "version": 3
 },
 "file_extension": ".py",
 "mimetype": "text/x-python",
 "name": "python",
 "nbconvert_exporter": "python",
 "pygments_lexer": "ipython3",
 "version": "3.11.5"
 },
 "widgets": {
 "application/vnd.jupyter.widget-state+json": {
 "bc84adf3ec66498b882c8e34de94e1b0": {
 "model_module": "@jupyter-widgets/controls",
 "model_name": "HBoxModel",
 "model_module_version": "1.5.0",
 "state": {
 "_dom_classes": [],
 "_model_module": "@jupyter-widgets/controls",
 "_model_module_version": "1.5.0",
 "_model_name": "HBoxModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/controls",
 "_view_module_version": "1.5.0",
 "_view_name": "HBoxView",
 "box_style": "",
 "children": [
 "IPY_MODEL_a014098616a745b88fe25a028c17ae64",
 "IPY_MODEL_eb4783a56c734ed39c8dcebc4c8b1392",
 "IPY_MODEL_c3661fde103544eaaba881176fc62844"
 ],
 "layout": "IPY_MODEL_2e966e3c71964548871627efc7807b17"
 }
 },
 "a014098616a745b88fe25a028c17ae64": {
 "model_module": "@jupyter-widgets/controls",
 "model_name": "HTMLModel",
 "model_module_version": "1.5.0",
 "state": {
 "_dom_classes": [],
 "_model_module": "@jupyter-widgets/controls",
 "_model_module_version": "1.5.0",
 "_model_name": "HTMLModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/controls",
 "_view_module_version": "1.5.0",
 "_view_name": "HTMLView",
 "description": "",
 "description_tooltip": null,
 "layout": "IPY_MODEL_d08af4be29854fad95e00685f86eb370",
 "placeholder": "\u200b",
 "style": "IPY_MODEL_5a6544cdb9994ce48b6f0087076bd905",
 "value": "README.md:\u2007"
 }
 },
 "eb4783a56c734ed39c8dcebc4c8b1392": {
 "model_module": "@jupyter-widgets/controls",
 "model_name": "FloatProgressModel",
 "model_module_version": "1.5.0",
 "state": {
 "_dom_classes": [],
 "_model_module": "@jupyter-widgets/controls",
 "_model_module_version": "1.5.0",
 "_model_name": "FloatProgressModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/controls",
 "_view_module_version": "1.5.0",
 "_view_name": "ProgressView",
 "bar_style": "success",
 "description": "",
 "description_tooltip": null,
 "layout": "IPY_MODEL_c7efff7ea52b41d28b39400ca25a2f47",
 "max": 1,
 "min": 0,
 "orientation": "horizontal",
 "style": "IPY_MODEL_d9b1c26cf91645febb90c9e9c9eaf4fa",
 "value": 1
 }
 },
 "c3661fde103544eaaba881176fc62844": {
 "model_module": "@jupyter-widgets/controls",
 "model_name": "HTMLModel",
 "model_module_version": "1.5.0",
 "state": {
 "_dom_classes": [],
 "_model_module": "@jupyter-widgets/controls",
 "_model_module_version": "1.5.0",
 "_model_name": "HTMLModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/controls",
 "_view_module_version": "1.5.0",
 "_view_name": "HTMLView",
 "description": "",
 "description_tooltip": null,
 "layout": "IPY_MODEL_d4c07c3b76ca4aecafaa7e53f6d03904",
 "placeholder": "\u200b",
 "style": "IPY_MODEL_be8d317b7da745e9907d572f9d674372",
 "value": "\u20077.81k/?\u2007[00:00&lt;00:00,\u2007118kB/s]"
 }
 },
 "2e966e3c71964548871627efc7807b17": {
 "model_module": "@jupyter-widgets/base",
 "model_name": "LayoutModel",
 "model_module_version": "1.2.0",
 "state": {
 "_model_module": "@jupyter-widgets/base",
 "_model_module_version": "1.2.0",
 "_model_name": "LayoutModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/base",
 "_view_module_version": "1.2.0",
 "_view_name": "LayoutView",
 "align_content": null,
 "align_items": null,
 "align_self": null,
 "border": null,
 "bottom": null,
 "display": null,
 "flex": null,
 "flex_flow": null,
 "grid_area": null,
 "grid_auto_columns": null,
 "grid_auto_flow": null,
 "grid_auto_rows": null,
 "grid_column": null,
 "grid_gap": null,
 "grid_row": null,
 "grid_template_areas": null,
 "grid_template_columns": null,
 "grid_template_rows": null,
 "height": null,
 "justify_content": null,
 "justify_items": null,
 "left": null,
 "margin": null,
 "max_height": null,
 "max_width": null,
 "min_height": null,
 "min_width": null,
 "object_fit": null,
 "object_position": null,
 "order": null,
 "overflow": null,
 "overflow_x": null,
 "overflow_y": null,
 "padding": null,
 "right": null,
 "top": null,
 "visibility": null,
 "width": null
 }
 },
 "d08af4be29854fad95e00685f86eb370": {
 "model_module": "@jupyter-widgets/base",
 "model_name": "LayoutModel",
 "model_module_version": "1.2.0",
 "state": {
 "_model_module": "@jupyter-widgets/base",
 "_model_module_version": "1.2.0",
 "_model_name": "LayoutModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/base",
 "_view_module_version": "1.2.0",
 "_view_name": "LayoutView",
 "align_content": null,
 "align_items": null,
 "align_self": null,
 "border": null,
 "bottom": null,
 "display": null,
 "flex": null,
 "flex_flow": null,
 "grid_area": null,
 "grid_auto_columns": null,
 "grid_auto_flow": null,
 "grid_auto_rows": null,
 "grid_column": null,
 "grid_gap": null,
 "grid_row": null,
 "grid_template_areas": null,
 "grid_template_columns": null,
 "grid_template_rows": null,
 "height": null,
 "justify_content": null,
 "justify_items": null,
 "left": null,
 "margin": null,
 "max_height": null,
 "max_width": null,
 "min_height": null,
 "min_width": null,
 "object_fit": null,
 "object_position": null,
 "order": null,
 "overflow": null,
 "overflow_x": null,
 "overflow_y": null,
 "padding": null,
 "right": null,
 "top": null,
 "visibility": null,
 "width": null
 }
 },
 "5a6544cdb9994ce48b6f0087076bd905": {
 "model_module": "@jupyter-widgets/controls",
 "model_name": "DescriptionStyleModel",
 "model_module_version": "1.5.0",
 "state": {
 "_model_module": "@jupyter-widgets/controls",
 "_model_module_version": "1.5.0",
 "_model_name": "DescriptionStyleModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/base",
 "_view_module_version": "1.2.0",
 "_view_name": "StyleView",
 "description_width": ""
 }
 },
 "c7efff7ea52b41d28b39400ca25a2f47": {
 "model_module": "@jupyter-widgets/base",
 "model_name": "LayoutModel",
 "model_module_version": "1.2.0",
 "state": {
 "_model_module": "@jupyter-widgets/base",
 "_model_module_version": "1.2.0",
 "_model_name": "LayoutModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/base",
 "_view_module_version": "1.2.0",
 "_view_name": "LayoutView",
 "align_content": null,
 "align_items": null,
 "align_self": null,
 "border": null,
 "bottom": null,
 "display": null,
 "flex": null,
 "flex_flow": null,
 "grid_area": null,
 "grid_auto_columns": null,
 "grid_auto_flow": null,
 "grid_auto_rows": null,
 "grid_column": null,
 "grid_gap": null,
 "grid_row": null,
 "grid_template_areas": null,
 "grid_template_columns": null,
 "grid_template_rows": null,
 "height": null,
 "justify_content": null,
 "justify_items": null,
 "left": null,
 "margin": null,
 "max_height": null,
 "max_width": null,
 "min_height": null,
 "min_width": null,
 "object_fit": null,
 "object_position": null,
 "order": null,
 "overflow": null,
 "overflow_x": null,
 "overflow_y": null,
 "padding": null,
 "right": null,
 "top": null,
 "visibility": null,
 "width": "20px"
 }
 },
 "d9b1c26cf91645febb90c9e9c9eaf4fa": {
 "model_module": "@jupyter-widgets/controls",
 "model_name": "ProgressStyleModel",
 "model_module_version": "1.5.0",
 "state": {
 "_model_module": "@jupyter-widgets/controls",
 "_model_module_version": "1.5.0",
 "_model_name": "ProgressStyleModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/base",
 "_view_module_version": "1.2.0",
 "_view_name": "StyleView",
 "bar_color": null,
 "description_width": ""
 }
 },
 "d4c07c3b76ca4aecafaa7e53f6d03904": {
 "model_module": "@jupyter-widgets/base",
 "model_name": "LayoutModel",
 "model_module_version": "1.2.0",
 "state": {
 "_model_module": "@jupyter-widgets/base",
 "_model_module_version": "1.2.0",
 "_model_name": "LayoutModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/base",
 "_view_module_version": "1.2.0",
 "_view_name": "LayoutView",
 "align_content": null,
 "align_items": null,
 "align_self": null,
 "border": null,
 "bottom": null,
 "display": null,
 "flex": null,
 "flex_flow": null,
 "grid_area": null,
 "grid_auto_columns": null,
 "grid_auto_flow": null,
 "grid_auto_rows": null,
 "grid_column": null,
 "grid_gap": null,
 "grid_row": null,
 "grid_template_areas": null,
 "grid_template_columns": null,
 "grid_template_rows": null,
 "height": null,
 "justify_content": null,
 "justify_items": null,
 "left": null,
 "margin": null,
 "max_height": null,
 "max_width": null,
 "min_height": null,
 "min_width": null,
 "object_fit": null,
 "object_position": null,
 "order": null,
 "overflow": null,
 "overflow_x": null,
 "overflow_y": null,
 "padding": null,
 "right": null,
 "top": null,
 "visibility": null,
 "width": null
 }
 },
 "be8d317b7da745e9907d572f9d674372": {
 "model_module": "@jupyter-widgets/controls",
 "model_name": "DescriptionStyleModel",
 "model_module_version": "1.5.0",
 "state": {
 "_model_module": "@jupyter-widgets/controls",
 "_model_module_version": "1.5.0",
 "_model_name": "DescriptionStyleModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/base",
 "_view_module_version": "1.2.0",
 "_view_name": "StyleView",
 "description_width": ""
 }
 },
 "ec38cb69815144e1b83fb7746b51c1d0": {
 "model_module": "@jupyter-widgets/controls",
 "model_name": "HBoxModel",
 "model_module_version": "1.5.0",
 "state": {
 "_dom_classes": [],
 "_model_module": "@jupyter-widgets/controls",
 "_model_module_version": "1.5.0",
 "_model_name": "HBoxModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/controls",
 "_view_module_version": "1.5.0",
 "_view_name": "HBoxView",
 "box_style": "",
 "children": [
 "IPY_MODEL_6666b239894542f0a986a28ea371f86c",
 "IPY_MODEL_d51d7493c196439eb584a1ec3e8ca72e",
 "IPY_MODEL_39ee523c293a4664a138071ba4005444"
 ],
 "layout": "IPY_MODEL_9ad85ba54c864cf7b7b577617df3a812"
 }
 },
 "6666b239894542f0a986a28ea371f86c": {
 "model_module": "@jupyter-widgets/controls",
 "model_name": "HTMLModel",
 "model_module_version": "1.5.0",
 "state": {
 "_dom_classes": [],
 "_model_module": "@jupyter-widgets/controls",
 "_model_module_version": "1.5.0",
 "_model_name": "HTMLModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/controls",
 "_view_module_version": "1.5.0",
 "_view_name": "HTMLView",
 "description": "",
 "description_tooltip": null,
 "layout": "IPY_MODEL_c1f4a04c1b334efdb1ea738c015c60cd",
 "placeholder": "\u200b",
 "style": "IPY_MODEL_b602f5a61e1742d9b55e58edcbc79bb7",
 "value": "plain_text/train-00000-of-00001.parquet:\u2007100%"
 }
 },
 "d51d7493c196439eb584a1ec3e8ca72e": {
 "model_module": "@jupyter-widgets/controls",
 "model_name": "FloatProgressModel",
 "model_module_version": "1.5.0",
 "state": {
 "_dom_classes": [],
 "_model_module": "@jupyter-widgets/controls",
 "_model_module_version": "1.5.0",
 "_model_name": "FloatProgressModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/controls",
 "_view_module_version": "1.5.0",
 "_view_name": "ProgressView",
 "bar_style": "success",
 "description": "",
 "description_tooltip": null,
 "layout": "IPY_MODEL_d9dbeaef230946b9ba0df7a01ca5b5b6",
 "max": 20979968,
 "min": 0,
 "orientation": "horizontal",
 "style": "IPY_MODEL_dd4038bfb4b2488a8cb1cfa829c4bc45",
 "value": 20979968
 }
 },
 "39ee523c293a4664a138071ba4005444": {
 "model_module": "@jupyter-widgets/controls",
 "model_name": "HTMLModel",
 "model_module_version": "1.5.0",
 "state": {
 "_dom_classes": [],
 "_model_module": "@jupyter-widgets/controls",
 "_model_module_version": "1.5.0",
 "_model_name": "HTMLModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/controls",
 "_view_module_version": "1.5.0",
 "_view_name": "HTMLView",
 "description": "",
 "description_tooltip": null,
 "layout": "IPY_MODEL_7fb759783a7948d48b08043d149aca7e",
 "placeholder": "\u200b",
 "style": "IPY_MODEL_7dd1be48587d4059ab5416a29854ab40",
 "value": "\u200721.0M/21.0M\u2007[00:00&lt;00:00,\u200725.0MB/s]"
 }
 },
 "9ad85ba54c864cf7b7b577617df3a812": {
 "model_module": "@jupyter-widgets/base",
 "model_name": "LayoutModel",
 "model_module_version": "1.2.0",
 "state": {
 "_model_module": "@jupyter-widgets/base",
 "_model_module_version": "1.2.0",
 "_model_name": "LayoutModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/base",
 "_view_module_version": "1.2.0",
 "_view_name": "LayoutView",
 "align_content": null,
 "align_items": null,
 "align_self": null,
 "border": null,
 "bottom": null,
 "display": null,
 "flex": null,
 "flex_flow": null,
 "grid_area": null,
 "grid_auto_columns": null,
 "grid_auto_flow": null,
 "grid_auto_rows": null,
 "grid_column": null,
 "grid_gap": null,
 "grid_row": null,
 "grid_template_areas": null,
 "grid_template_columns": null,
 "grid_template_rows": null,
 "height": null,
 "justify_content": null,
 "justify_items": null,
 "left": null,
 "margin": null,
 "max_height": null,
 "max_width": null,
 "min_height": null,
 "min_width": null,
 "object_fit": null,
 "object_position": null,
 "order": null,
 "overflow": null,
 "overflow_x": null,
 "overflow_y": null,
 "padding": null,
 "right": null,
 "top": null,
 "visibility": null,
 "width": null
 }
 },
 "c1f4a04c1b334efdb1ea738c015c60cd": {
 "model_module": "@jupyter-widgets/base",
 "model_name": "LayoutModel",
 "model_module_version": "1.2.0",
 "state": {
 "_model_module": "@jupyter-widgets/base",
 "_model_module_version": "1.2.0",
 "_model_name": "LayoutModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/base",
 "_view_module_version": "1.2.0",
 "_view_name": "LayoutView",
 "align_content": null,
 "align_items": null,
 "align_self": null,
 "border": null,
 "bottom": null,
 "display": null,
 "flex": null,
 "flex_flow": null,
 "grid_area": null,
 "grid_auto_columns": null,
 "grid_auto_flow": null,
 "grid_auto_rows": null,
 "grid_column": null,
 "grid_gap": null,
 "grid_row": null,
 "grid_template_areas": null,
 "grid_template_columns": null,
 "grid_template_rows": null,
 "height": null,
 "justify_content": null,
 "justify_items": null,
 "left": null,
 "margin": null,
 "max_height": null,
 "max_width": null,
 "min_height": null,
 "min_width": null,
 "object_fit": null,
 "object_position": null,
 "order": null,
 "overflow": null,
 "overflow_x": null,
 "overflow_y": null,
 "padding": null,
 "right": null,
 "top": null,
 "visibility": null,
 "width": null
 }
 },
 "b602f5a61e1742d9b55e58edcbc79bb7": {
 "model_module": "@jupyter-widgets/controls",
 "model_name": "DescriptionStyleModel",
 "model_module_version": "1.5.0",
 "state": {
 "_model_module": "@jupyter-widgets/controls",
 "_model_module_version": "1.5.0",
 "_model_name": "DescriptionStyleModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/base",
 "_view_module_version": "1.2.0",
 "_view_name": "StyleView",
 "description_width": ""
 }
 },
 "d9dbeaef230946b9ba0df7a01ca5b5b6": {
 "model_module": "@jupyter-widgets/base",
 "model_name": "LayoutModel",
 "model_module_version": "1.2.0",
 "state": {
 "_model_module": "@jupyter-widgets/base",
 "_model_module_version": "1.2.0",
 "_model_name": "LayoutModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/base",
 "_view_module_version": "1.2.0",
 "_view_name": "LayoutView",
 "align_content": null,
 "align_items": null,
 "align_self": null,
 "border": null,
 "bottom": null,
 "display": null,
 "flex": null,
 "flex_flow": null,
 "grid_area": null,
 "grid_auto_columns": null,
 "grid_auto_flow": null,
 "grid_auto_rows": null,
 "grid_column": null,
 "grid_gap": null,
 "grid_row": null,
 "grid_template_areas": null,
 "grid_template_columns": null,
 "grid_template_rows": null,
 "height": null,
 "justify_content": null,
 "justify_items": null,
 "left": null,
 "margin": null,
 "max_height": null,
 "max_width": null,
 "min_height": null,
 "min_width": null,
 "object_fit": null,
 "object_position": null,
 "order": null,
 "overflow": null,
 "overflow_x": null,
 "overflow_y": null,
 "padding": null,
 "right": null,
 "top": null,
 "visibility": null,
 "width": null
 }
 },
 "dd4038bfb4b2488a8cb1cfa829c4bc45": {
 "model_module": "@jupyter-widgets/controls",
 "model_name": "ProgressStyleModel",
 "model_module_version": "1.5.0",
 "state": {
 "_model_module": "@jupyter-widgets/controls",
 "_model_module_version": "1.5.0",
 "_model_name": "ProgressStyleModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/base",
 "_view_module_version": "1.2.0",
 "_view_name": "StyleView",
 "bar_color": null,
 "description_width": ""
 }
 },
 "7fb759783a7948d48b08043d149aca7e": {
 "model_module": "@jupyter-widgets/base",
 "model_name": "LayoutModel",
 "model_module_version": "1.2.0",
 "state": {
 "_model_module": "@jupyter-widgets/base",
 "_model_module_version": "1.2.0",
 "_model_name": "LayoutModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/base",
 "_view_module_version": "1.2.0",
 "_view_name": "LayoutView",
 "align_content": null,
 "align_items": null,
 "align_self": null,
 "border": null,
 "bottom": null,
 "display": null,
 "flex": null,
 "flex_flow": null,
 "grid_area": null,
 "grid_auto_columns": null,
 "grid_auto_flow": null,
 "grid_auto_rows": null,
 "grid_column": null,
 "grid_gap": null,
 "grid_row": null,
 "grid_template_areas": null,
 "grid_template_columns": null,
 "grid_template_rows": null,
 "height": null,
 "justify_content": null,
 "justify_items": null,
 "left": null,
 "margin": null,
 "max_height": null,
 "max_width": null,
 "min_height": null,
 "min_width": null,
 "object_fit": null,
 "object_position": null,
 "order": null,
 "overflow": null,
 "overflow_x": null,
 "overflow_y": null,
 "padding": null,
 "right": null,
 "top": null,
 "visibility": null,
 "width": null
 }
 },
 "7dd1be48587d4059ab5416a29854ab40": {
 "model_module": "@jupyter-widgets/controls",
 "model_name": "DescriptionStyleModel",
 "model_module_version": "1.5.0",
 "state": {
 "_model_module": "@jupyter-widgets/controls",
 "_model_module_version": "1.5.0",
 "_model_name": "DescriptionStyleModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/base",
 "_view_module_version": "1.2.0",
 "_view_name": "StyleView",
 "description_width": ""
 }
 },
 "d1f553c47c454a23b634800ec608fddd": {
 "model_module": "@jupyter-widgets/controls",
 "model_name": "HBoxModel",
 "model_module_version": "1.5.0",
 "state": {
 "_dom_classes": [],
 "_model_module": "@jupyter-widgets/controls",
 "_model_module_version": "1.5.0",
 "_model_name": "HBoxModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/controls",
 "_view_module_version": "1.5.0",
 "_view_name": "HBoxView",
 "box_style": "",
 "children": [
 "IPY_MODEL_6a215073fad4492cabec3369ae8e0291",
 "IPY_MODEL_796f1284bc7341a199c4784538a64d88",
 "IPY_MODEL_b81d9444396e42e7a59e053a04716050"
 ],
 "layout": "IPY_MODEL_b4b73048f8824c5c92201afa0ef06f4a"
 }
 },
 "6a215073fad4492cabec3369ae8e0291": {
 "model_module": "@jupyter-widgets/controls",
 "model_name": "HTMLModel",
 "model_module_version": "1.5.0",
 "state": {
 "_dom_classes": [],
 "_model_module": "@jupyter-widgets/controls",
 "_model_module_version": "1.5.0",
 "_model_name": "HTMLModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/controls",
 "_view_module_version": "1.5.0",
 "_view_name": "HTMLView",
 "description": "",
 "description_tooltip": null,
 "layout": "IPY_MODEL_15441cd0b4b2471cb62135f439753f01",
 "placeholder": "\u200b",
 "style": "IPY_MODEL_d9d46e3caf0c481dabc2a000ee34caae",
 "value": "plain_text/test-00000-of-00001.parquet:\u2007100%"
 }
 },
 "796f1284bc7341a199c4784538a64d88": {
 "model_module": "@jupyter-widgets/controls",
 "model_name": "FloatProgressModel",
 "model_module_version": "1.5.0",
 "state": {
 "_dom_classes": [],
 "_model_module": "@jupyter-widgets/controls",
 "_model_module_version": "1.5.0",
 "_model_name": "FloatProgressModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/controls",
 "_view_module_version": "1.5.0",
 "_view_name": "ProgressView",
 "bar_style": "success",
 "description": "",
 "description_tooltip": null,
 "layout": "IPY_MODEL_9eb2817f1a854d09a2f8218c41618ea3",
 "max": 20470363,
 "min": 0,
 "orientation": "horizontal",
 "style": "IPY_MODEL_436128b1dd0c4d8288f83c5e529db39d",
 "value": 20470363
 }
 },
 "b81d9444396e42e7a59e053a04716050": {
 "model_module": "@jupyter-widgets/controls",
 "model_name": "HTMLModel",
 "model_module_version": "1.5.0",
 "state": {
 "_dom_classes": [],
 "_model_module": "@jupyter-widgets/controls",
 "_model_module_version": "1.5.0",
 "_model_name": "HTMLModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/controls",
 "_view_module_version": "1.5.0",
 "_view_name": "HTMLView",
 "description": "",
 "description_tooltip": null,
 "layout": "IPY_MODEL_8b67c74ec0b14b7983252b99077ab30c",
 "placeholder": "\u200b",
 "style": "IPY_MODEL_5bf705b036b94bf998203597bb2511f8",
 "value": "\u200720.5M/20.5M\u2007[00:00&lt;00:00,\u2007152kB/s]"
 }
 },
 "b4b73048f8824c5c92201afa0ef06f4a": {
 "model_module": "@jupyter-widgets/base",
 "model_name": "LayoutModel",
 "model_module_version": "1.2.0",
 "state": {
 "_model_module": "@jupyter-widgets/base",
 "_model_module_version": "1.2.0",
 "_model_name": "LayoutModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/base",
 "_view_module_version": "1.2.0",
 "_view_name": "LayoutView",
 "align_content": null,
 "align_items": null,
 "align_self": null,
 "border": null,
 "bottom": null,
 "display": null,
 "flex": null,
 "flex_flow": null,
 "grid_area": null,
 "grid_auto_columns": null,
 "grid_auto_flow": null,
 "grid_auto_rows": null,
 "grid_column": null,
 "grid_gap": null,
 "grid_row": null,
 "grid_template_areas": null,
 "grid_template_columns": null,
 "grid_template_rows": null,
 "height": null,
 "justify_content": null,
 "justify_items": null,
 "left": null,
 "margin": null,
 "max_height": null,
 "max_width": null,
 "min_height": null,
 "min_width": null,
 "object_fit": null,
 "object_position": null,
 "order": null,
 "overflow": null,
 "overflow_x": null,
 "overflow_y": null,
 "padding": null,
 "right": null,
 "top": null,
 "visibility": null,
 "width": null
 }
 },
 "15441cd0b4b2471cb62135f439753f01": {
 "model_module": "@jupyter-widgets/base",
 "model_name": "LayoutModel",
 "model_module_version": "1.2.0",
 "state": {
 "_model_module": "@jupyter-widgets/base",
 "_model_module_version": "1.2.0",
 "_model_name": "LayoutModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/base",
 "_view_module_version": "1.2.0",
 "_view_name": "LayoutView",
 "align_content": null,
 "align_items": null,
 "align_self": null,
 "border": null,
 "bottom": null,
 "display": null,
 "flex": null,
 "flex_flow": null,
 "grid_area": null,
 "grid_auto_columns": null,
 "grid_auto_flow": null,
 "grid_auto_rows": null,
 "grid_column": null,
 "grid_gap": null,
 "grid_row": null,
 "grid_template_areas": null,
 "grid_template_columns": null,
 "grid_template_rows": null,
 "height": null,
 "justify_content": null,
 "justify_items": null,
 "left": null,
 "margin": null,
 "max_height": null,
 "max_width": null,
 "min_height": null,
 "min_width": null,
 "object_fit": null,
 "object_position": null,
 "order": null,
 "overflow": null,
 "overflow_x": null,
 "overflow_y": null,
 "padding": null,
 "right": null,
 "top": null,
 "visibility": null,
 "width": null
 }
 },
 "d9d46e3caf0c481dabc2a000ee34caae": {
 "model_module": "@jupyter-widgets/controls",
 "model_name": "DescriptionStyleModel",
 "model_module_version": "1.5.0",
 "state": {
 "_model_module": "@jupyter-widgets/controls",
 "_model_module_version": "1.5.0",
 "_model_name": "DescriptionStyleModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/base",
 "_view_module_version": "1.2.0",
 "_view_name": "StyleView",
 "description_width": ""
 }
 },
 "9eb2817f1a854d09a2f8218c41618ea3": {
 "model_module": "@jupyter-widgets/base",
 "model_name": "LayoutModel",
 "model_module_version": "1.2.0",
 "state": {
 "_model_module": "@jupyter-widgets/base",
 "_model_module_version": "1.2.0",
 "_model_name": "LayoutModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/base",
 "_view_module_version": "1.2.0",
 "_view_name": "LayoutView",
 "align_content": null,
 "align_items": null,
 "align_self": null,
 "border": null,
 "bottom": null,
 "display": null,
 "flex": null,
 "flex_flow": null,
 "grid_area": null,
 "grid_auto_columns": null,
 "grid_auto_flow": null,
 "grid_auto_rows": null,
 "grid_column": null,
 "grid_gap": null,
 "grid_row": null,
 "grid_template_areas": null,
 "grid_template_columns": null,
 "grid_template_rows": null,
 "height": null,
 "justify_content": null,
 "justify_items": null,
 "left": null,
 "margin": null,
 "max_height": null,
 "max_width": null,
 "min_height": null,
 "min_width": null,
 "object_fit": null,
 "object_position": null,
 "order": null,
 "overflow": null,
 "overflow_x": null,
 "overflow_y": null,
 "padding": null,
 "right": null,
 "top": null,
 "visibility": null,
 "width": null
 }
 },
 "436128b1dd0c4d8288f83c5e529db39d": {
 "model_module": "@jupyter-widgets/controls",
 "model_name": "ProgressStyleModel",
 "model_module_version": "1.5.0",
 "state": {
 "_model_module": "@jupyter-widgets/controls",
 "_model_module_version": "1.5.0",
 "_model_name": "ProgressStyleModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/base",
 "_view_module_version": "1.2.0",
 "_view_name": "StyleView",
 "bar_color": null,
 "description_width": ""
 }
 },
 "8b67c74ec0b14b7983252b99077ab30c": {
 "model_module": "@jupyter-widgets/base",
 "model_name": "LayoutModel",
 "model_module_version": "1.2.0",
 "state": {
 "_model_module": "@jupyter-widgets/base",
 "_model_module_version": "1.2.0",
 "_model_name": "LayoutModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/base",
 "_view_module_version": "1.2.0",
 "_view_name": "LayoutView",
 "align_content": null,
 "align_items": null,
 "align_self": null,
 "border": null,
 "bottom": null,
 "display": null,
 "flex": null,
 "flex_flow": null,
 "grid_area": null,
 "grid_auto_columns": null,
 "grid_auto_flow": null,
 "grid_auto_rows": null,
 "grid_column": null,
 "grid_gap": null,
 "grid_row": null,
 "grid_template_areas": null,
 "grid_template_columns": null,
 "grid_template_rows": null,
 "height": null,
 "justify_content": null,
 "justify_items": null,
 "left": null,
 "margin": null,
 "max_height": null,
 "max_width": null,
 "min_height": null,
 "min_width": null,
 "object_fit": null,
 "object_position": null,
 "order": null,
 "overflow": null,
 "overflow_x": null,
 "overflow_y": null,
 "padding": null,
 "right": null,
 "top": null,
 "visibility": null,
 "width": null
 }
 },
 "5bf705b036b94bf998203597bb2511f8": {
 "model_module": "@jupyter-widgets/controls",
 "model_name": "DescriptionStyleModel",
 "model_module_version": "1.5.0",
 "state": {
 "_model_module": "@jupyter-widgets/controls",
 "_model_module_version": "1.5.0",
 "_model_name": "DescriptionStyleModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/base",
 "_view_module_version": "1.2.0",
 "_view_name": "StyleView",
 "description_width": ""
 }
 },
 "2b12fb4cf1f44803b830e33338706362": {
 "model_module": "@jupyter-widgets/controls",
 "model_name": "HBoxModel",
 "model_module_version": "1.5.0",
 "state": {
 "_dom_classes": [],
 "_model_module": "@jupyter-widgets/controls",
 "_model_module_version": "1.5.0",
 "_model_name": "HBoxModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/controls",
 "_view_module_version": "1.5.0",
 "_view_name": "HBoxView",
 "box_style": "",
 "children": [
 "IPY_MODEL_f1cab43b91ce481ba21b6902628d7854",
 "IPY_MODEL_06b57c00b8f2453593c4dcd4d31a8bda",
 "IPY_MODEL_cca75623f72a4a76805b235f7f0a0438"
 ],
 "layout": "IPY_MODEL_5693c3d25dc544eeb96e832a3e60f60e"
 }
 },
 "f1cab43b91ce481ba21b6902628d7854": {
 "model_module": "@jupyter-widgets/controls",
 "model_name": "HTMLModel",
 "model_module_version": "1.5.0",
 "state": {
 "_dom_classes": [],
 "_model_module": "@jupyter-widgets/controls",
 "_model_module_version": "1.5.0",
 "_model_name": "HTMLModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/controls",
 "_view_module_version": "1.5.0",
 "_view_name": "HTMLView",
 "description": "",
 "description_tooltip": null,
 "layout": "IPY_MODEL_6454fe3a5c384371b09189aa8680e5e7",
 "placeholder": "\u200b",
 "style": "IPY_MODEL_a886a9c89b874fae96ec631e38593e9c",
 "value": "plain_text/unsupervised-00000-of-00001.p(\u2026):\u2007100%"
 }
 },
 "06b57c00b8f2453593c4dcd4d31a8bda": {
 "model_module": "@jupyter-widgets/controls",
 "model_name": "FloatProgressModel",
 "model_module_version": "1.5.0",
 "state": {
 "_dom_classes": [],
 "_model_module": "@jupyter-widgets/controls",
 "_model_module_version": "1.5.0",
 "_model_name": "FloatProgressModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/controls",
 "_view_module_version": "1.5.0",
 "_view_name": "ProgressView",
 "bar_style": "success",
 "description": "",
 "description_tooltip": null,
 "layout": "IPY_MODEL_37a48a3396464787b2bfc0bd3ce743ac",
 "max": 41996509,
 "min": 0,
 "orientation": "horizontal",
 "style": "IPY_MODEL_97478926957e484981350124714b6c0c",
 "value": 41996509
 }
 },
 "cca75623f72a4a76805b235f7f0a0438": {
 "model_module": "@jupyter-widgets/controls",
 "model_name": "HTMLModel",
 "model_module_version": "1.5.0",
 "state": {
 "_dom_classes": [],
 "_model_module": "@jupyter-widgets/controls",
 "_model_module_version": "1.5.0",
 "_model_name": "HTMLModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/controls",
 "_view_module_version": "1.5.0",
 "_view_name": "HTMLView",
 "description": "",
 "description_tooltip": null,
 "layout": "IPY_MODEL_b0214ab6545649ff8bd7b2cd65f394ff",
 "placeholder": "\u200b",
 "style": "IPY_MODEL_0b570f8445a54c0c86b1d098d7d0e6d4",
 "value": "\u200742.0M/42.0M\u2007[00:00&lt;00:00,\u200778.1MB/s]"
 }
 },
 "5693c3d25dc544eeb96e832a3e60f60e": {
 "model_module": "@jupyter-widgets/base",
 "model_name": "LayoutModel",
 "model_module_version": "1.2.0",
 "state": {
 "_model_module": "@jupyter-widgets/base",
 "_model_module_version": "1.2.0",
 "_model_name": "LayoutModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/base",
 "_view_module_version": "1.2.0",
 "_view_name": "LayoutView",
 "align_content": null,
 "align_items": null,
 "align_self": null,
 "border": null,
 "bottom": null,
 "display": null,
 "flex": null,
 "flex_flow": null,
 "grid_area": null,
 "grid_auto_columns": null,
 "grid_auto_flow": null,
 "grid_auto_rows": null,
 "grid_column": null,
 "grid_gap": null,
 "grid_row": null,
 "grid_template_areas": null,
 "grid_template_columns": null,
 "grid_template_rows": null,
 "height": null,
 "justify_content": null,
 "justify_items": null,
 "left": null,
 "margin": null,
 "max_height": null,
 "max_width": null,
 "min_height": null,
 "min_width": null,
 "object_fit": null,
 "object_position": null,
 "order": null,
 "overflow": null,
 "overflow_x": null,
 "overflow_y": null,
 "padding": null,
 "right": null,
 "top": null,
 "visibility": null,
 "width": null
 }
 },
 "6454fe3a5c384371b09189aa8680e5e7": {
 "model_module": "@jupyter-widgets/base",
 "model_name": "LayoutModel",
 "model_module_version": "1.2.0",
 "state": {
 "_model_module": "@jupyter-widgets/base",
 "_model_module_version": "1.2.0",
 "_model_name": "LayoutModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/base",
 "_view_module_version": "1.2.0",
 "_view_name": "LayoutView",
 "align_content": null,
 "align_items": null,
 "align_self": null,
 "border": null,
 "bottom": null,
 "display": null,
 "flex": null,
 "flex_flow": null,
 "grid_area": null,
 "grid_auto_columns": null,
 "grid_auto_flow": null,
 "grid_auto_rows": null,
 "grid_column": null,
 "grid_gap": null,
 "grid_row": null,
 "grid_template_areas": null,
 "grid_template_columns": null,
 "grid_template_rows": null,
 "height": null,
 "justify_content": null,
 "justify_items": null,
 "left": null,
 "margin": null,
 "max_height": null,
 "max_width": null,
 "min_height": null,
 "min_width": null,
 "object_fit": null,
 "object_position": null,
 "order": null,
 "overflow": null,
 "overflow_x": null,
 "overflow_y": null,
 "padding": null,
 "right": null,
 "top": null,
 "visibility": null,
 "width": null
 }
 },
 "a886a9c89b874fae96ec631e38593e9c": {
 "model_module": "@jupyter-widgets/controls",
 "model_name": "DescriptionStyleModel",
 "model_module_version": "1.5.0",
 "state": {
 "_model_module": "@jupyter-widgets/controls",
 "_model_module_version": "1.5.0",
 "_model_name": "DescriptionStyleModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/base",
 "_view_module_version": "1.2.0",
 "_view_name": "StyleView",
 "description_width": ""
 }
 },
 "37a48a3396464787b2bfc0bd3ce743ac": {
 "model_module": "@jupyter-widgets/base",
 "model_name": "LayoutModel",
 "model_module_version": "1.2.0",
 "state": {
 "_model_module": "@jupyter-widgets/base",
 "_model_module_version": "1.2.0",
 "_model_name": "LayoutModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/base",
 "_view_module_version": "1.2.0",
 "_view_name": "LayoutView",
 "align_content": null,
 "align_items": null,
 "align_self": null,
 "border": null,
 "bottom": null,
 "display": null,
 "flex": null,
 "flex_flow": null,
 "grid_area": null,
 "grid_auto_columns": null,
 "grid_auto_flow": null,
 "grid_auto_rows": null,
 "grid_column": null,
 "grid_gap": null,
 "grid_row": null,
 "grid_template_areas": null,
 "grid_template_columns": null,
 "grid_template_rows": null,
 "height": null,
 "justify_content": null,
 "justify_items": null,
 "left": null,
 "margin": null,
 "max_height": null,
 "max_width": null,
 "min_height": null,
 "min_width": null,
 "object_fit": null,
 "object_position": null,
 "order": null,
 "overflow": null,
 "overflow_x": null,
 "overflow_y": null,
 "padding": null,
 "right": null,
 "top": null,
 "visibility": null,
 "width": null
 }
 },
 "97478926957e484981350124714b6c0c": {
 "model_module": "@jupyter-widgets/controls",
 "model_name": "ProgressStyleModel",
 "model_module_version": "1.5.0",
 "state": {
 "_model_module": "@jupyter-widgets/controls",
 "_model_module_version": "1.5.0",
 "_model_name": "ProgressStyleModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/base",
 "_view_module_version": "1.2.0",
 "_view_name": "StyleView",
 "bar_color": null,
 "description_width": ""
 }
 },
 "b0214ab6545649ff8bd7b2cd65f394ff": {
 "model_module": "@jupyter-widgets/base",
 "model_name": "LayoutModel",
 "model_module_version": "1.2.0",
 "state": {
 "_model_module": "@jupyter-widgets/base",
 "_model_module_version": "1.2.0",
 "_model_name": "LayoutModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/base",
 "_view_module_version": "1.2.0",
 "_view_name": "LayoutView",
 "align_content": null,
 "align_items": null,
 "align_self": null,
 "border": null,
 "bottom": null,
 "display": null,
 "flex": null,
 "flex_flow": null,
 "grid_area": null,
 "grid_auto_columns": null,
 "grid_auto_flow": null,
 "grid_auto_rows": null,
 "grid_column": null,
 "grid_gap": null,
 "grid_row": null,
 "grid_template_areas": null,
 "grid_template_columns": null,
 "grid_template_rows": null,
 "height": null,
 "justify_content": null,
 "justify_items": null,
 "left": null,
 "margin": null,
 "max_height": null,
 "max_width": null,
 "min_height": null,
 "min_width": null,
 "object_fit": null,
 "object_position": null,
 "order": null,
 "overflow": null,
 "overflow_x": null,
 "overflow_y": null,
 "padding": null,
 "right": null,
 "top": null,
 "visibility": null,
 "width": null
 }
 },
 "0b570f8445a54c0c86b1d098d7d0e6d4": {
 "model_module": "@jupyter-widgets/controls",
 "model_name": "DescriptionStyleModel",
 "model_module_version": "1.5.0",
 "state": {
 "_model_module": "@jupyter-widgets/controls",
 "_model_module_version": "1.5.0",
 "_model_name": "DescriptionStyleModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/base",
 "_view_module_version": "1.2.0",
 "_view_name": "StyleView",
 "description_width": ""
 }
 },
 "92e53c2ed1614e249aa9c18cb104122a": {
 "model_module": "@jupyter-widgets/controls",
 "model_name": "HBoxModel",
 "model_module_version": "1.5.0",
 "state": {
 "_dom_classes": [],
 "_model_module": "@jupyter-widgets/controls",
 "_model_module_version": "1.5.0",
 "_model_name": "HBoxModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/controls",
 "_view_module_version": "1.5.0",
 "_view_name": "HBoxView",
 "box_style": "",
 "children": [
 "IPY_MODEL_54a515a77da241269ff3dba57f4cd0ad",
 "IPY_MODEL_e69c7822fda04b8b9741f8985a604d30",
 "IPY_MODEL_78f50c0ee2fc43f7b54f9556088affc1"
 ],
 "layout": "IPY_MODEL_944da8b38150448aa9536b5ff15cacfb"
 }
 },
 "54a515a77da241269ff3dba57f4cd0ad": {
 "model_module": "@jupyter-widgets/controls",
 "model_name": "HTMLModel",
 "model_module_version": "1.5.0",
 "state": {
 "_dom_classes": [],
 "_model_module": "@jupyter-widgets/controls",
 "_model_module_version": "1.5.0",
 "_model_name": "HTMLModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/controls",
 "_view_module_version": "1.5.0",
 "_view_name": "HTMLView",
 "description": "",
 "description_tooltip": null,
 "layout": "IPY_MODEL_048649e44e5a465bb916e967b8ca6",
 "placeholder": "\u200b",
 "style": "IPY_MODEL_544de90c919749b3aae6f646af8f330d",
 "value": "Generating\u2007train\u2007split:\u2007100%"
 }
 },
 "e69c7822fda04b8b9741f8985a604d30": {
 "model_module": "@jupyter-widgets/controls",
 "model_name": "FloatProgressModel",
 "model_module_version": "1.5.0",
 "state": {
 "_dom_classes": [],
 "_model_module": "@jupyter-widgets/controls",
 "_model_module_version": "1.5.0",
 "_model_name": "FloatProgressModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/controls",
 "_view_module_version": "1.5.0",
 "_view_name": "ProgressView",
 "bar_style": "success",
 "description": "",
 "description_tooltip": null,
 "layout": "IPY_MODEL_f6af2055001c426f80ba0b4fe2e999f8",
 "max": 25000,
 "min": 0,
 "orientation": "horizontal",
 "style": "IPY_MODEL_3144484222384495acec2fa709dfa0ad",
 "value": 25000
 }
 },
 "78f50c0ee2fc43f7b54f9556088affc1": {
 "model_module": "@jupyter-widgets/controls",
 "model_name": "HTMLModel",
 "model_module_version": "1.5.0",
 "state": {
 "_dom_classes": [],
 "_model_module": "@jupyter-widgets/controls",
 "_model_module_version": "1.5.0",
 "_model_name": "HTMLModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/controls",
 "_view_module_version": "1.5.0",
 "_view_name": "HTMLView",
 "description": "",
 "description_tooltip": null,
 "layout": "IPY_MODEL_0b45b764f3234680854414866c7550bb",
 "placeholder": "\u200b",
 "style": "IPY_MODEL_0e2751ca97a648d1bf2233185e977c33",
 "value": "\u200725000/25000\u2007[00:00&lt;00:00,\u200791312.39\u2007examples/s]"
 }
 },
 "944da8b38150448aa9536b5ff15cacfb": {
 "model_module": "@jupyter-widgets/base",
 "model_name": "LayoutModel",
 "model_module_version": "1.2.0",
 "state": {
 "_model_module": "@jupyter-widgets/base",
 "_model_module_version": "1.2.0",
 "_model_name": "LayoutModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/base",
 "_view_module_version": "1.2.0",
 "_view_name": "LayoutView",
 "align_content": null,
 "align_items": null,
 "align_self": null,
 "border": null,
 "bottom": null,
 "display": null,
 "flex": null,
 "flex_flow": null,
 "grid_area": null,
 "grid_auto_columns": null,
 "grid_auto_flow": null,
 "grid_auto_rows": null,
 "grid_column": null,
 "grid_gap": null,
 "grid_row": null,
 "grid_template_areas": null,
 "grid_template_columns": null,
 "grid_template_rows": null,
 "height": null,
 "justify_content": null,
 "justify_items": null,
 "left": null,
 "margin": null,
 "max_height": null,
 "max_width": null,
 "min_height": null,
 "min_width": null,
 "object_fit": null,
 "object_position": null,
 "order": null,
 "overflow": null,
 "overflow_x": null,
 "overflow_y": null,
 "padding": null,
 "right": null,
 "top": null,
 "visibility": null,
 "width": null
 }
 },
 "048649e44e5a465bb916e967b8ca6": {
 "model_module": "@jupyter-widgets/base",
 "model_name": "LayoutModel",
 "model_module_version": "1.2.0",
 "state": {
 "_model_module": "@jupyter-widgets/base",
 "_model_module_version": "1.2.0",
 "_model_name": "LayoutModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/base",
 "_view_module_version": "1.2.0",
 "_view_name": "LayoutView",
 "align_content": null,
 "align_items": null,
 "align_self": null,
 "border": null,
 "bottom": null,
 "display": null,
 "flex": null,
 "flex_flow": null,
 "grid_area": null,
 "grid_auto_columns": null,
 "grid_auto_flow": null,
 "grid_auto_rows": null,
 "grid_column": null,
 "grid_gap": null,
 "grid_row": null,
 "grid_template_areas": null,
 "grid_template_columns": null,
 "grid_template_rows": null,
 "height": null,
 "justify_content": null,
 "justify_items": null,
 "left": null,
 "margin": null,
 "max_height": null,
 "max_width": null,
 "min_height": null,
 "min_width": null,
 "object_fit": null,
 "object_position": null,
 "order": null,
 "overflow": null,
 "overflow_x": null,
 "overflow_y": null,
 "padding": null,
 "right": null,
 "top": null,
 "visibility": null,
 "width": null
 }
 },
 "544de90c919749b3aae6f646af8f330d": {
 "model_module": "@jupyter-widgets/controls",
 "model_name": "DescriptionStyleModel",
 "model_module_version": "1.5.0",
 "state": {
 "_model_module": "@jupyter-widgets/controls",
 "_model_module_version": "1.5.0",
 "_model_name": "DescriptionStyleModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/base",
 "_view_module_version": "1.2.0",
 "_view_name": "StyleView",
 "description_width": ""
 }
 },
 "f6af2055001c426f80ba0b4fe2e999f8": {
 "model_module": "@jupyter-widgets/base",
 "model_name": "LayoutModel",
 "model_module_version": "1.2.0",
 "state": {
 "_model_module": "@jupyter-widgets/base",
 "_model_module_version": "1.2.0",
 "_model_name": "LayoutModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/base",
 "_view_module_version": "1.2.0",
 "_view_name": "LayoutView",
 "align_content": null,
 "align_items": null,
 "align_self": null,
 "border": null,
 "bottom": null,
 "display": null,
 "flex": null,
 "flex_flow": null,
 "grid_area": null,
 "grid_auto_columns": null,
 "grid_auto_flow": null,
 "grid_auto_rows": null,
 "grid_column": null,
 "grid_gap": null,
 "grid_row": null,
 "grid_template_areas": null,
 "grid_template_columns": null,
 "grid_template_rows": null,
 "height": null,
 "justify_content": null,
 "justify_items": null,
 "left": null,
 "margin": null,
 "max_height": null,
 "max_width": null,
 "min_height": null,
 "min_width": null,
 "object_fit": null,
 "object_position": null,
 "order": null,
 "overflow": null,
 "overflow_x": null,
 "overflow_y": null,
 "padding": null,
 "right": null,
 "top": null,
 "visibility": null,
 "width": null
 }
 },
 "3144484222384495acec2fa709dfa0ad": {
 "model_module": "@jupyter-widgets/controls",
 "model_name": "ProgressStyleModel",
 "model_module_version": "1.5.0",
 "state": {
 "_model_module": "@jupyter-widgets/controls",
 "_model_module_version": "1.5.0",
 "_model_name": "ProgressStyleModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/base",
 "_view_module_version": "1.2.0",
 "_view_name": "StyleView",
 "bar_color": null,
 "description_width": ""
 }
 },
 "0b45b764f3234680854414866c7550bb": {
 "model_module": "@jupyter-widgets/base",
 "model_name": "LayoutModel",
 "model_module_version": "1.2.0",
 "state": {
 "_model_module": "@jupyter-widgets/base",
 "_model_module_version": "1.2.0",
 "_model_name": "LayoutModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/base",
 "_view_module_version": "1.2.0",
 "_view_name": "LayoutView",
 "align_content": null,
 "align_items": null,
 "align_self": null,
 "border": null,
 "bottom": null,
 "display": null,
 "flex": null,
 "flex_flow": null,
 "grid_area": null,
 "grid_auto_columns": null,
 "grid_auto_flow": null,
 "grid_auto_rows": null,
 "grid_column": null,
 "grid_gap": null,
 "grid_row": null,
 "grid_template_areas": null,
 "grid_template_columns": null,
 "grid_template_rows": null,
 "height": null,
 "justify_content": null,
 "justify_items": null,
 "left": null,
 "margin": null,
 "max_height": null,
 "max_width": null,
 "min_height": null,
 "min_width": null,
 "object_fit": null,
 "object_position": null,
 "order": null,
 "overflow": null,
 "overflow_x": null,
 "overflow_y": null,
 "padding": null,
 "right": null,
 "top": null,
 "visibility": null,
 "width": null
 }
 },
 "0e2751ca97a648d1bf2233185e977c33": {
 "model_module": "@jupyter-widgets/controls",
 "model_name": "DescriptionStyleModel",
 "model_module_version": "1.5.0",
 "state": {
 "_model_module": "@jupyter-widgets/controls",
 "_model_module_version": "1.5.0",
 "_model_name": "DescriptionStyleModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/base",
 "_view_module_version": "1.2.0",
 "_view_name": "StyleView",
 "description_width": ""
 }
 },
 "7f6a7babb5ed49299add9d92372e6946": {
 "model_module": "@jupyter-widgets/controls",
 "model_name": "HBoxModel",
 "model_module_version": "1.5.0",
 "state": {
 "_dom_classes": [],
 "_model_module": "@jupyter-widgets/controls",
 "_model_module_version": "1.5.0",
 "_model_name": "HBoxModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/controls",
 "_view_module_version": "1.5.0",
 "_view_name": "HBoxView",
 "box_style": "",
 "children": [
 "IPY_MODEL_2d273f81e9f84cb19c864e7c530ba529",
 "IPY_MODEL_151ae2c00f554724bbe30e529a9ac06e",
 "IPY_MODEL_d6dc3d0683c540e3b5685c211496e3dd"
 ],
 "layout": "IPY_MODEL_dba0504e3b43437caa26cc0f24a675d8"
 }
 },
 "2d273f81e9f84cb19c864e7c530ba529": {
 "model_module": "@jupyter-widgets/controls",
 "model_name": "HTMLModel",
 "model_module_version": "1.5.0",
 "state": {
 "_dom_classes": [],
 "_model_module": "@jupyter-widgets/controls",
 "_model_module_version": "1.5.0",
 "_model_name": "HTMLModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/controls",
 "_view_module_version": "1.5.0",
 "_view_name": "HTMLView",
 "description": "",
 "description_tooltip": null,
 "layout": "IPY_MODEL_97f1a1d4539e4607be17b6481c69aa25",
 "placeholder": "\u200b",
 "style": "IPY_MODEL_2156400a2d12420da365704c9cb1c668",
 "value": "Generating\u2007test\u2007split:\u2007100%"
 }
 },
 "151ae2c00f554724bbe30e529a9ac06e": {
 "model_module": "@jupyter-widgets/controls",
 "model_name": "FloatProgressModel",
 "model_module_version": "1.5.0",
 "state": {
 "_dom_classes": [],
 "_model_module": "@jupyter-widgets/controls",
 "_model_module_version": "1.5.0",
 "_model_name": "FloatProgressModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/controls",
 "_view_module_version": "1.5.0",
 "_view_name": "ProgressView",
 "bar_style": "success",
 "description": "",
 "description_tooltip": null,
 "layout": "IPY_MODEL_6d759590aa1946e9a51349e38d894c4a",
 "max": 25000,
 "min": 0,
 "orientation": "horizontal",
 "style": "IPY_MODEL_13782fc248b648049a6644270270ca48",
 "value": 25000
 }
 },
 "d6dc3d0683c540e3b5685c211496e3dd": {
 "model_module": "@jupyter-widgets/controls",
 "model_name": "HTMLModel",
 "model_module_version": "1.5.0",
 "state": {
 "_dom_classes": [],
 "_model_module": "@jupyter-widgets/controls",
 "_model_module_version": "1.5.0",
 "_model_name": "HTMLModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/controls",
 "_view_module_version": "1.5.0",
 "_view_name": "HTMLView",
 "description": "",
 "description_tooltip": null,
 "layout": "IPY_MODEL_3cf7ec49da6546d5a566c092551bd5db",
 "placeholder": "\u200b",
 "style": "IPY_MODEL_5817efaf444229ad4de149038ca0e",
 "value": "\u200725000/25000\u2007[00:00&lt;00:00,\u2007113857.84\u2007examples/s]"
 }
 },
 "dba0504e3b43437caa26cc0f24a675d8": {
 "model_module": "@jupyter-widgets/base",
 "model_name": "LayoutModel",
 "model_module_version": "1.2.0",
 "state": {
 "_model_module": "@jupyter-widgets/base",
 "_model_module_version": "1.2.0",
 "_model_name": "LayoutModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/base",
 "_view_module_version": "1.2.0",
 "_view_name": "LayoutView",
 "align_content": null,
 "align_items": null,
 "align_self": null,
 "border": null,
 "bottom": null,
 "display": null,
 "flex": null,
 "flex_flow": null,
 "grid_area": null,
 "grid_auto_columns": null,
 "grid_auto_flow": null,
 "grid_auto_rows": null,
 "grid_column": null,
 "grid_gap": null,
 "grid_row": null,
 "grid_template_areas": null,
 "grid_template_columns": null,
 "grid_template_rows": null,
 "height": null,
 "justify_content": null,
 "justify_items": null,
 "left": null,
 "margin": null,
 "max_height": null,
 "max_width": null,
 "min_height": null,
 "min_width": null,
 "object_fit": null,
 "object_position": null,
 "order": null,
 "overflow": null,
 "overflow_x": null,
 "overflow_y": null,
 "padding": null,
 "right": null,
 "top": null,
 "visibility": null,
 "width": null
 }
 },
 "97f1a1d4539e4607be17b6481c69aa25": {
 "model_module": "@jupyter-widgets/base",
 "model_name": "LayoutModel",
 "model_module_version": "1.2.0",
 "state": {
 "_model_module": "@jupyter-widgets/base",
 "_model_module_version": "1.2.0",
 "_model_name": "LayoutModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/base",
 "_view_module_version": "1.2.0",
 "_view_name": "LayoutView",
 "align_content": null,
 "align_items": null,
 "align_self": null,
 "border": null,
 "bottom": null,
 "display": null,
 "flex": null,
 "flex_flow": null,
 "grid_area": null,
 "grid_auto_columns": null,
 "grid_auto_flow": null,
 "grid_auto_rows": null,
 "grid_column": null,
 "grid_gap": null,
 "grid_row": null,
 "grid_template_areas": null,
 "grid_template_columns": null,
 "grid_template_rows": null,
 "height": null,
 "justify_content": null,
 "justify_items": null,
 "left": null,
 "margin": null,
 "max_height": null,
 "max_width": null,
 "min_height": null,
 "min_width": null,
 "object_fit": null,
 "object_position": null,
 "order": null,
 "overflow": null,
 "overflow_x": null,
 "overflow_y": null,
 "padding": null,
 "right": null,
 "top": null,
 "visibility": null,
 "width": null
 }
 },
 "2156400a2d12420da365704c9cb1c668": {
 "model_module": "@jupyter-widgets/controls",
 "model_name": "DescriptionStyleModel",
 "model_module_version": "1.5.0",
 "state": {
 "_model_module": "@jupyter-widgets/controls",
 "_model_module_version": "1.5.0",
 "_model_name": "DescriptionStyleModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/base",
 "_view_module_version": "1.2.0",
 "_view_name": "StyleView",
 "description_width": ""
 }
 },
 "6d759590aa1946e9a51349e38d894c4a": {
 "model_module": "@jupyter-widgets/base",
 "model_name": "LayoutModel",
 "model_module_version": "1.2.0",
 "state": {
 "_model_module": "@jupyter-widgets/base",
 "_model_module_version": "1.2.0",
 "_model_name": "LayoutModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/base",
 "_view_module_version": "1.2.0",
 "_view_name": "LayoutView",
 "align_content": null,
 "align_items": null,
 "align_self": null,
 "border": null,
 "bottom": null,
 "display": null,
 "flex": null,
 "flex_flow": null,
 "grid_area": null,
 "grid_auto_columns": null,
 "grid_auto_flow": null,
 "grid_auto_rows": null,
 "grid_column": null,
 "grid_gap": null,
 "grid_row": null,
 "grid_template_areas": null,
 "grid_template_columns": null,
 "grid_template_rows": null,
 "height": null,
 "justify_content": null,
 "justify_items": null,
 "left": null,
 "margin": null,
 "max_height": null,
 "max_width": null,
 "min_height": null,
 "min_width": null,
 "object_fit": null,
 "object_position": null,
 "order": null,
 "overflow": null,
 "overflow_x": null,
 "overflow_y": null,
 "padding": null,
 "right": null,
 "top": null,
 "visibility": null,
 "width": null
 }
 },
 "13782fc248b648049a6644270270ca48": {
 "model_module": "@jupyter-widgets/controls",
 "model_name": "ProgressStyleModel",
 "model_module_version": "1.5.0",
 "state": {
 "_model_module": "@jupyter-widgets/controls",
 "_model_module_version": "1.5.0",
 "_model_name": "ProgressStyleModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/base",
 "_view_module_version": "1.2.0",
 "_view_name": "StyleView",
 "bar_color": null,
 "description_width": ""
 }
 },
 "3cf7ec49da6546d5a566c092551bd5db": {
 "model_module": "@jupyter-widgets/base",
 "model_name": "LayoutModel",
 "model_module_version": "1.2.0",
 "state": {
 "_model_module": "@jupyter-widgets/base",
 "_model_module_version": "1.2.0",
 "_model_name": "LayoutModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/base",
 "_view_module_version": "1.2.0",
 "_view_name": "LayoutView",
 "align_content": null,
 "align_items": null,
 "align_self": null,
 "border": null,
 "bottom": null,
 "display": null,
 "flex": null,
 "flex_flow": null,
 "grid_area": null,
 "grid_auto_columns": null,
 "grid_auto_flow": null,
 "grid_auto_rows": null,
 "grid_column": null,
 "grid_gap": null,
 "grid_row": null,
 "grid_template_areas": null,
 "grid_template_columns": null,
 "grid_template_rows": null,
 "height": null,
 "justify_content": null,
 "justify_items": null,
 "left": null,
 "margin": null,
 "max_height": null,
 "max_width": null,
 "min_height": null,
 "min_width": null,
 "object_fit": null,
 "object_position": null,
 "order": null,
 "overflow": null,
 "overflow_x": null,
 "overflow_y": null,
 "padding": null,
 "right": null,
 "top": null,
 "visibility": null,
 "width": null
 }
 },
 "5817efaf444229ad4de149038ca0e": {
 "model_module": "@jupyter-widgets/controls",
 "model_name": "DescriptionStyleModel",
 "model_module_version": "1.5.0",
 "state": {
 "_model_module": "@jupyter-widgets/controls",
 "_model_module_version": "1.5.0",
 "_model_name": "DescriptionStyleModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/base",
 "_view_module_version": "1.2.0",
 "_view_name": "StyleView",
 "description_width": ""
 }
 },
 "31faf891378745e1a6ef7dfda80efcd3": {
 "model_module": "@jupyter-widgets/controls",
 "model_name": "HBoxModel",
 "model_module_version": "1.5.0",
 "state": {
 "_dom_classes": [],
 "_model_module": "@jupyter-widgets/controls",
 "_model_module_version": "1.5.0",
 "_model_name": "HBoxModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/controls",
 "_view_module_version": "1.5.0",
 "_view_name": "HBoxView",
 "box_style": "",
 "children": [
 "IPY_MODEL_f0bbec25c2c24b6a848a1c9f028438d4",
 "IPY_MODEL_16506adebb4847c9b88590d8e0beccc1",
 "IPY_MODEL_98099a10026b41d2b19fb39af907776a"
 ],
 "layout": "IPY_MODEL_b920cea588304cfb9376497dd8197131"
 }
 },
 "f0bbec25c2c24b6a848a1c9f028438d4": {
 "model_module": "@jupyter-widgets/controls",
 "model_name": "HTMLModel",
 "model_module_version": "1.5.0",
 "state": {
 "_dom_classes": [],
 "_model_module": "@jupyter-widgets/controls",
 "_model_module_version": "1.5.0",
 "_model_name": "HTMLModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/controls",
 "_view_module_version": "1.5.0",
 "_view_name": "HTMLView",
 "description": "",
 "description_tooltip": null,
 "layout": "IPY_MODEL_00ceb88d890d4e0c97764a76c69265d3",
 "placeholder": "\u200b",
 "style": "IPY_MODEL_13313d5809ee405c9238f4967d0afd87",
 "value": "Generating\u2007unsupervised\u2007split:\u2007100%"
 }
 },
 "16506adebb4847c9b88590d8e0beccc1": {
 "model_module": "@jupyter-widgets/controls",
 "model_name": "FloatProgressModel",
 "model_module_version": "1.5.0",
 "state": {
 "_dom_classes": [],
 "_model_module": "@jupyter-widgets/controls",
 "_model_module_version": "1.5.0",
 "_model_name": "FloatProgressModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/controls",
 "_view_module_version": "1.5.0",
 "_view_name": "ProgressView",
 "bar_style": "success",
 "description": "",
 "description_tooltip": null,
 "layout": "IPY_MODEL_b86fa4e9261544e39c98a72e616301a1",
 "max": 50000,
 "min": 0,
 "orientation": "horizontal",
 "style": "IPY_MODEL_d524b2751f0340e2b7093087b593c651",
 "value": 50000
 }
 },
 "98099a10026b41d2b19fb39af907776a": {
 "model_module": "@jupyter-widgets/controls",
 "model_name": "HTMLModel",
 "model_module_version": "1.5.0",
 "state": {
 "_dom_classes": [],
 "_model_module": "@jupyter-widgets/controls",
 "_model_module_version": "1.5.0",
 "_model_name": "HTMLModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/controls",
 "_view_module_version": "1.5.0",
 "_view_name": "HTMLView",
 "description": "",
 "description_tooltip": null,
 "layout": "IPY_MODEL_5e4c30a7d4a94c93a1e6950c8e0d3d8a",
 "placeholder": "\u200b",
 "style": "IPY_MODEL_659bbd2593d44017b113b31f0d0bb145",
 "value": "\u200750000/50000\u2007[00:00&lt;00:00,\u2007135577.98\u2007examples/s]"
 }
 },
 "b920cea588304cfb9376497dd8197131": {
 "model_module": "@jupyter-widgets/base",
 "model_name": "LayoutModel",
 "model_module_version": "1.2.0",
 "state": {
 "_model_module": "@jupyter-widgets/base",
 "_model_module_version": "1.2.0",
 "_model_name": "LayoutModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/base",
 "_view_module_version": "1.2.0",
 "_view_name": "LayoutView",
 "align_content": null,
 "align_items": null,
 "align_self": null,
 "border": null,
 "bottom": null,
 "display": null,
 "flex": null,
 "flex_flow": null,
 "grid_area": null,
 "grid_auto_columns": null,
 "grid_auto_flow": null,
 "grid_auto_rows": null,
 "grid_column": null,
 "grid_gap": null,
 "grid_row": null,
 "grid_template_areas": null,
 "grid_template_columns": null,
 "grid_template_rows": null,
 "height": null,
 "justify_content": null,
 "justify_items": null,
 "left": null,
 "margin": null,
 "max_height": null,
 "max_width": null,
 "min_height": null,
 "min_width": null,
 "object_fit": null,
 "object_position": null,
 "order": null,
 "overflow": null,
 "overflow_x": null,
 "overflow_y": null,
 "padding": null,
 "right": null,
 "top": null,
 "visibility": null,
 "width": null
 }
 },
 "00ceb88d890d4e0c97764a76c69265d3": {
 "model_module": "@jupyter-widgets/base",
 "model_name": "LayoutModel",
 "model_module_version": "1.2.0",
 "state": {
 "_model_module": "@jupyter-widgets/base",
 "_model_module_version": "1.2.0",
 "_model_name": "LayoutModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/base",
 "_view_module_version": "1.2.0",
 "_view_name": "LayoutView",
 "align_content": null,
 "align_items": null,
 "align_self": null,
 "border": null,
 "bottom": null,
 "display": null,
 "flex": null,
 "flex_flow": null,
 "grid_area": null,
 "grid_auto_columns": null,
 "grid_auto_flow": null,
 "grid_auto_rows": null,
 "grid_column": null,
 "grid_gap": null,
 "grid_row": null,
 "grid_template_areas": null,
 "grid_template_columns": null,
 "grid_template_rows": null,
 "height": null,
 "justify_content": null,
 "justify_items": null,
 "left": null,
 "margin": null,
 "max_height": null,
 "max_width": null,
 "min_height": null,
 "min_width": null,
 "object_fit": null,
 "object_position": null,
 "order": null,
 "overflow": null,
 "overflow_x": null,
 "overflow_y": null,
 "padding": null,
 "right": null,
 "top": null,
 "visibility": null,
 "width": null
 }
 },
 "13313d5809ee405c9238f4967d0afd87": {
 "model_module": "@jupyter-widgets/controls",
 "model_name": "DescriptionStyleModel",
 "model_module_version": "1.5.0",
 "state": {
 "_model_module": "@jupyter-widgets/controls",
 "_model_module_version": "1.5.0",
 "_model_name": "DescriptionStyleModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/base",
 "_view_module_version": "1.2.0",
 "_view_name": "StyleView",
 "description_width": ""
 }
 },
 "b86fa4e9261544e39c98a72e616301a1": {
 "model_module": "@jupyter-widgets/base",
 "model_name": "LayoutModel",
 "model_module_version": "1.2.0",
 "state": {
 "_model_module": "@jupyter-widgets/base",
 "_model_module_version": "1.2.0",
 "_model_name": "LayoutModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/base",
 "_view_module_version": "1.2.0",
 "_view_name": "LayoutView",
 "align_content": null,
 "align_items": null,
 "align_self": null,
 "border": null,
 "bottom": null,
 "display": null,
 "flex": null,
 "flex_flow": null,
 "grid_area": null,
 "grid_auto_columns": null,
 "grid_auto_flow": null,
 "grid_auto_rows": null,
 "grid_column": null,
 "grid_gap": null,
 "grid_row": null,
 "grid_template_areas": null,
 "grid_template_columns": null,
 "grid_template_rows": null,
 "height": null,
 "justify_content": null,
 "justify_items": null,
 "left": null,
 "margin": null,
 "max_height": null,
 "max_width": null,
 "min_height": null,
 "min_width": null,
 "object_fit": null,
 "object_position": null,
 "order": null,
 "overflow": null,
 "overflow_x": null,
 "overflow_y": null,
 "padding": null,
 "right": null,
 "top": null,
 "visibility": null,
 "width": null
 }
 },
 "d524b2751f0340e2b7093087b593c651": {
 "model_module": "@jupyter-widgets/controls",
 "model_name": "ProgressStyleModel",
 "model_module_version": "1.5.0",
 "state": {
 "_model_module": "@jupyter-widgets/controls",
 "_model_module_version": "1.5.0",
 "_model_name": "ProgressStyleModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/base",
 "_view_module_version": "1.2.0",
 "_view_name": "StyleView",
 "bar_color": null,
 "description_width": ""
 }
 },
 "5e4c30a7d4a94c93a1e6950c8e0d3d8a": {
 "model_module": "@jupyter-widgets/base",
 "model_name": "LayoutModel",
 "model_module_version": "1.2.0",
 "state": {
 "_model_module": "@jupyter-widgets/base",
 "_model_module_version": "1.2.0",
 "_model_name": "LayoutModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/base",
 "_view_module_version": "1.2.0",
 "_view_name": "LayoutView",
 "align_content": null,
 "align_items": null,
 "align_self": null,
 "border": null,
 "bottom": null,
 "display": null,
 "flex": null,
 "flex_flow": null,
 "grid_area": null,
 "grid_auto_columns": null,
 "grid_auto_flow": null,
 "grid_auto_rows": null,
 "grid_column": null,
 "grid_gap": null,
 "grid_row": null,
 "grid_template_areas": null,
 "grid_template_columns": null,
 "grid_template_rows": null,
 "height": null,
 "justify_content": null,
 "justify_items": null,
 "left": null,
 "margin": null,
 "max_height": null,
 "max_width": null,
 "min_height": null,
 "min_width": null,
 "object_fit": null,
 "object_position": null,
 "order": null,
 "overflow": null,
 "overflow_x": null,
 "overflow_y": null,
 "padding": null,
 "right": null,
 "top": null,
 "visibility": null,
 "width": null
 }
 },
 "659bbd2593d44017b113b31f0d0bb145": {
 "model_module": "@jupyter-widgets/controls",
 "model_name": "DescriptionStyleModel",
 "model_module_version": "1.5.0",
 "state": {
 "_model_module": "@jupyter-widgets/controls",
 "_model_module_version": "1.5.0",
 "_model_name": "DescriptionStyleModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/base",
 "_view_module_version": "1.2.0",
 "_view_name": "StyleView",
 "description_width": ""
 }
 }
 }
 }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}