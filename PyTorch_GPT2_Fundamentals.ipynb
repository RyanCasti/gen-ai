{
 "cells": [
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "9hZNSDOBJcBm"
 },
 "source": "# PyTorch & GPT-2 Language Model Fundamentals\n\nThis notebook demonstrates foundational skills in PyTorch and language model manipulation using OpenAI's GPT-2.\n\n**Skills Demonstrated:**\n- PyTorch tensor operations and neural network construction\n- Building classification networks with multiple hidden layers\n- GPT-2 tokenization and vocabulary handling\n- Next-word prediction and probability calculations\n- Cross-entropy loss computation and softmax temperature scaling\n\n**Technologies:** PyTorch, Hugging Face Transformers, GPT-2"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "Vtkgx-8fl1ha"
 },
 "source": "## 0. Setup\n\nLet us first install a few required packages. (You may want to comment this out in case you use a local environment that already has the suitable packages installed.)\n\nNote our use of `%%capture` in the cell below absorbs all of the output when the model(s) are loading. You can comment it out if you want to see that output."
 },
 {
 "cell_type": "code",
 "execution_count": 192,
 "metadata": {
 "id": "GuN074G-JK_T",
 "executionInfo": {
 "status": "ok",
 "timestamp": 1757795890450,
 "user_tz": 360,
 "elapsed": 29,
 "user": {
 "displayName": "Ryan C",
 "userId": "13217471319505539255"
 }
 }
 },
 "outputs": [],
 "source": "# %%capture\n\n# #!pip install torch. # commented because it is pre-installed in Colab\n# !pip install torchtext\n# !pip install transformers\n# #!pip install numpy # commented because it is pre-installed in Colab\n# !pip install portalocker\n# !pip install pandas"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "_9Lv5JrRl1hb"
 },
 "source": "Next, we will import required libraries"
 },
 {
 "cell_type": "code",
 "execution_count": 193,
 "metadata": {
 "id": "kgVjAiytl1hb",
 "executionInfo": {
 "status": "ok",
 "timestamp": 1757795890455,
 "user_tz": 360,
 "elapsed": 2,
 "user": {
 "displayName": "Ryan C",
 "userId": "13217471319505539255"
 }
 }
 },
 "outputs": [],
 "source": "import copy\nimport random\n\nimport torch\nimport numpy as np\nimport pandas as pd\n\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch import nn\n\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import AutoTokenizer, GPT2Model, GPT2ForSequenceClassification, GPT2LMHeadModel"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "collapsed": false,
 "id": "LZmkkCXxuf0o",
 "jupyter": {
 "outputs_hidden": false
 }
 },
 "source": "Let's make sure we will later put data and models on the proper device:"
 },
 {
 "cell_type": "code",
 "execution_count": 194,
 "metadata": {
 "colab": {
 "base_uri": "https://localhost:8080/"
 },
 "id": "9piY_5XnuVa9",
 "outputId": "cb3217b8-f4b7-41ee-ff8f-d6c1f17abca5",
 "executionInfo": {
 "status": "ok",
 "timestamp": 1757795890467,
 "user_tz": 360,
 "elapsed": 10,
 "user": {
 "displayName": "Ryan C",
 "userId": "13217471319505539255"
 }
 }
 },
 "outputs": [
 {
 "output_type": "execute_result",
 "data": {
 "text/plain": [
 "device(type='cpu')"
 ]
 },
 "metadata": {},
 "execution_count": 194
 }
 ],
 "source": "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ndevice"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "VZt63SBLl1hc"
 },
 "source": "This should say 'cpu' if using a CPU, or 'cuda', if a GPU is used. We are using just the CPU in this assignment.\n\nNow let's get started!\n\n## 1. PyTorch Basics: Common Operations\n\nLet's get started with some simple operations. For reference you should use the PyTorch documentation at https://pytorch.org/docs/stable/index.html .\n\n Your goal is to create a simple neural net with two connected layers that takes a (random) input that we will create and 'classifies' imagining a 3-class prediction problem. (We will not train the model, so the purpose is simply to test dimensions, expressions, etc., but not real values. We do however want you to execute the cells consecutively in the proper order so that we can compare the final (randomly generated) outcomes. They should always be the same given the manual seed that we set.) \n\nWe start with setting the seed which insures that the answers are more deteministic."
 },
 {
 "cell_type": "code",
 "execution_count": 195,
 "metadata": {
 "colab": {
 "base_uri": "https://localhost:8080/"
 },
 "id": "Tf6nwBQsl1hc",
 "outputId": "51a1037e-266f-4229-f4dd-ea7de5e07110",
 "pycharm": {
 "is_executing": true
 },
 "executionInfo": {
 "status": "ok",
 "timestamp": 1757795890477,
 "user_tz": 360,
 "elapsed": 11,
 "user": {
 "displayName": "Ryan C",
 "userId": "13217471319505539255"
 }
 }
 },
 "outputs": [
 {
 "output_type": "execute_result",
 "data": {
 "text/plain": [
 "<torch._C.Generator at 0x7cc3751e80d0>"
 ]
 },
 "metadata": {},
 "execution_count": 195
 }
 ],
 "source": "torch.manual_seed(10)"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "MMvs5BWal1hd"
 },
 "source": "### 1.a Tensor Manipulation\n\nLet's generate a random input dataset that mimics 4 examples with 6 features each. Consider using torch.rand()."
 },
 {
 "cell_type": "code",
 "execution_count": 196,
 "metadata": {
 "colab": {
 "base_uri": "https://localhost:8080/"
 },
 "id": "z2XSpoPil1hd",
 "outputId": "fd50184b-a575-4bce-e5fa-f8b575eca3d4",
 "executionInfo": {
 "status": "ok",
 "timestamp": 1757795890515,
 "user_tz": 360,
 "elapsed": 41,
 "user": {
 "displayName": "Ryan C",
 "userId": "13217471319505539255"
 }
 }
 },
 "outputs": [
 {
 "output_type": "execute_result",
 "data": {
 "text/plain": [
 "tensor([[0.4581, 0.4829, 0.3125, 0.6150, 0.2139, 0.4118],\n",
 " [0.6938, 0.9693, 0.6178, 0.3304, 0.5479, 0.4440],\n",
 " [0.7041, 0.5573, 0.6959, 0.9849, 0.2924, 0.4823],\n",
 " [0.6150, 0.4967, 0.4521, 0.0575, 0.0687, 0.0501]])"
 ]
 },
 "metadata": {},
 "execution_count": 196
 }
 ],
 "source": "input_dim = 6\nn_examples = 4\nn_classes = 3\n#call your generated input tensor 'input_data'\n\ninput_data = torch.rand([n_examples, input_dim])\n\ninput_data"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "K7YBqgxnl1hd"
 },
 "source": "**1.a. Examining the value of the input_data[0,0]**\n\n\n**(**"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "emZnD87dl1hd"
 },
 "source": "Let's now do a few simple exercises. Using torch.argmax (https://pytorch.org/docs/stable/generated/torch.argmax.html), find the index of the maximum element for each row and each column."
 },
 {
 "cell_type": "code",
 "execution_count": 198,
 "metadata": {
 "colab": {
 "base_uri": "https://localhost:8080/"
 },
 "id": "OW9srHfCl1hd",
 "outputId": "ba240f87-f5fe-4fd9-ac27-ed7f26357e03",
 "executionInfo": {
 "status": "ok",
 "timestamp": 1757795890549,
 "user_tz": 360,
 "elapsed": 18,
 "user": {
 "displayName": "Ryan C",
 "userId": "13217471319505539255"
 }
 }
 },
 "outputs": [
 {
 "output_type": "stream",
 "name": "stdout",
 "text": [
 "Index of argmax for each row tensor([3, 1, 3, 0])\n",
 "Index of argmax for each column tensor([2, 1, 2, 2, 1, 2])\n"
 ]
 }
 ],
 "source": "# call your indices row_ind_max_arg and col_ind_max_arg\n\ntorch.argmax(input_data, dim=1)\ntorch.argmax(input_data, dim=0)\nrow_ind_max_arg = torch.argmax(input_data, dim=1)\ncol_ind_max_arg = torch.argmax(input_data, dim=0)\n\nprint('Index of argmax for each row', row_ind_max_arg)\nprint('Index of argmax for each column', col_ind_max_arg)"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "Gf12HlYAl1hd"
 },
 "source": "**1.b. Examining the indices of the elements with the largest value in each row? Copy the list of indices to the answer cell and represent them as a list e.g. [55, 77, 99, 11].**\n"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "BveXAeJW-JPH"
 },
 "source": "**1.c. Examining the indices of the elements with the largest value in each column? Again, copy the list of indices to the answer cell and represent them as a list e.g. [55, 77, 99, 11].**\n"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "V6oejR97l1hd"
 },
 "source": "You can get the values of a tensor just like you can for numpy. For example, the values for the last column (i.e., fixed second dimension) can be obtained through:"
 },
 {
 "cell_type": "code",
 "execution_count": 201,
 "metadata": {
 "colab": {
 "base_uri": "https://localhost:8080/"
 },
 "id": "sILEH-Ycl1hd",
 "outputId": "23dc502f-ded8-4274-e14e-5291540ae0fc",
 "executionInfo": {
 "status": "ok",
 "timestamp": 1757795890574,
 "user_tz": 360,
 "elapsed": 16,
 "user": {
 "displayName": "Ryan C",
 "userId": "13217471319505539255"
 }
 }
 },
 "outputs": [
 {
 "output_type": "stream",
 "name": "stdout",
 "text": [
 "tensor([0.4118, 0.4440, 0.4823, 0.0501])\n",
 "tensor([0.4118, 0.4440, 0.4823, 0.0501])\n"
 ]
 }
 ],
 "source": "print(input_data[:, -1])\nprint(input_data[:, 5])"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "vfNjSiv_l1hd"
 },
 "source": "Similarly, get the values of the last row (first index 'last', second index unconstrained):"
 },
 {
 "cell_type": "code",
 "execution_count": 202,
 "metadata": {
 "colab": {
 "base_uri": "https://localhost:8080/"
 },
 "id": "z3IgW_vhl1hd",
 "outputId": "83dc0029-de63-4a96-d2fc-a31700d7ce3f",
 "executionInfo": {
 "status": "ok",
 "timestamp": 1757795890583,
 "user_tz": 360,
 "elapsed": 11,
 "user": {
 "displayName": "Ryan C",
 "userId": "13217471319505539255"
 }
 }
 },
 "outputs": [
 {
 "output_type": "stream",
 "name": "stdout",
 "text": [
 "Values of last row: tensor([0.6150, 0.4967, 0.4521, 0.0575, 0.0687, 0.0501])\n"
 ]
 }
 ],
 "source": "# call your values for the last row last_row\n\nlast_row = input_data[-1,:]\n\nprint('Values of last row: ', last_row)"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "alVM9425l1hd"
 },
 "source": "**1.d. Result: tensor of the last row into the answers.**\n"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "ZkRmzM6Ll1hd"
 },
 "source": "Next, reshape input_data into a 2x12 tensor using \\<tensor>.reshape"
 },
 {
 "cell_type": "code",
 "execution_count": 204,
 "metadata": {
 "colab": {
 "base_uri": "https://localhost:8080/"
 },
 "id": "nbC8tIzql1he",
 "outputId": "c9c5ecde-96ba-4d97-b5a5-e8926125fecc",
 "executionInfo": {
 "status": "ok",
 "timestamp": 1757795890649,
 "user_tz": 360,
 "elapsed": 26,
 "user": {
 "displayName": "Ryan C",
 "userId": "13217471319505539255"
 }
 }
 },
 "outputs": [
 {
 "output_type": "stream",
 "name": "stdout",
 "text": [
 "Reshaped data shape: torch.Size([2, 12])\n"
 ]
 }
 ],
 "source": "# call your reshaped tensor reshaped_input_data\n\ninput_data.reshape(2,12)\nreshaped_input_data = input_data.reshape(2,12)\n\nprint('Reshaped data shape: ', reshaped_input_data.shape)"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "DI2lh_t2l1he"
 },
 "source": "**1.e. Write the shape of the reshaped tensor as a tuple.**\n"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "NrpCxcLSl1he"
 },
 "source": "### b. The Simple Classification Network\n\nNow construct the network. Fill in your code for the __init__ and forward methods. Again, we want to have two hidden layers (dims: hidden_dim_1, hidden_dim_2) with relu activation functions, and output layer of dimension n_classes (and softmax activation function). The model should return both the probabilities (probs) and the logits (logits), as you can tell from the return statement."
 },
 {
 "cell_type": "code",
 "execution_count": 206,
 "metadata": {
 "id": "VZw1UUqpl1he",
 "executionInfo": {
 "status": "ok",
 "timestamp": 1757795890654,
 "user_tz": 360,
 "elapsed": 3,
 "user": {
 "displayName": "Ryan C",
 "userId": "13217471319505539255"
 }
 }
 },
 "outputs": [],
 "source": "class SimpleClassificationNertwork(nn.Module):\n def __init__(self, input_dim, hidden_dim_1, hidden_dim_2, n_classes):\n super().__init__()\n self.linear1 = nn.Linear(input_dim, hidden_dim_1)\n self.linear2 = nn.Linear(hidden_dim_1, hidden_dim_2)\n self.linear3 = nn.Linear(hidden_dim_2, n_classes)\n self.relu = nn.ReLU()\n\n def forward(self, x):\n # Hidden layer 1 with relu activation\n z1 = self.linear1(x)\n a1 = self.relu(z1)\n\n # Hidden layer 2 with relu activation\n z2 = self.linear2(a1)\n a2 = self.relu(z2)\n\n # Output layer with softmax activation\n logits = self.linear3(a2)\n probs = nn.Softmax(dim=1)(logits)\n\n return probs, logits"
 },
 {
 "cell_type": "code",
 "execution_count": 207,
 "metadata": {
 "id": "pXhIyuu4l1he",
 "executionInfo": {
 "status": "ok",
 "timestamp": 1757795890658,
 "user_tz": 360,
 "elapsed": 3,
 "user": {
 "displayName": "Ryan C",
 "userId": "13217471319505539255"
 }
 }
 },
 "outputs": [],
 "source": "mySimpleClassificationNertwork = SimpleClassificationNertwork(input_dim=input_dim,\n hidden_dim_1=7,\n hidden_dim_2=10,\n n_classes=n_classes)"
 },
 {
 "cell_type": "code",
 "execution_count": 208,
 "metadata": {
 "colab": {
 "base_uri": "https://localhost:8080/"
 },
 "id": "jAEmtIWel1he",
 "outputId": "58def21f-3084-4010-8e22-d45679e9314e",
 "executionInfo": {
 "status": "ok",
 "timestamp": 1757795890688,
 "user_tz": 360,
 "elapsed": 28,
 "user": {
 "displayName": "Ryan C",
 "userId": "13217471319505539255"
 }
 }
 },
 "outputs": [
 {
 "output_type": "stream",
 "name": "stdout",
 "text": [
 "Probabilities:\n",
 "\t tensor([[0.4503, 0.2337, 0.3160],\n",
 " [0.4515, 0.2499, 0.2985],\n",
 " [0.4563, 0.2303, 0.3134],\n",
 " [0.4513, 0.2440, 0.3047]], grad_fn=<SoftmaxBackward0>)\n",
 "\n",
 "Logits:\n",
 "\t tensor([[ 0.4086, -0.2475, 0.0545],\n",
 " [ 0.4077, -0.1839, -0.0061],\n",
 " [ 0.4272, -0.2564, 0.0514],\n",
 " [ 0.4109, -0.2040, 0.0180]], grad_fn=<AddmmBackward0>)\n"
 ]
 }
 ],
 "source": "probs,logits = mySimpleClassificationNertwork(input_data)\n\nprint('Probabilities:\\n\\t', probs)\nprint('\\nLogits:\\n\\t', logits)"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "C0Ur9b8Al1he"
 },
 "source": "**NOTE: Once everything works please rerun the cells starting with setting the manual seed up to this cell to make sure that the numbers (if everything is correct) can be !**"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "BXeztSdkl1he"
 },
 "source": "**1.f. Result: tensor for the probabilities into the answer file.**\n"
 },
 {
 "cell_type": "code",
 "execution_count": 209,
 "metadata": {
 "colab": {
 "base_uri": "https://localhost:8080/"
 },
 "id": "dEhclh8Z-7wk",
 "outputId": "28589e17-1ed6-43e2-9dd9-6e019520a253",
 "executionInfo": {
 "status": "ok",
 "timestamp": 1757795890696,
 "user_tz": 360,
 "elapsed": 12,
 "user": {
 "displayName": "Ryan C",
 "userId": "13217471319505539255"
 }
 }
 },
 "outputs": [
 {
 "output_type": "execute_result",
 "data": {
 "text/plain": [
 "[[0.4503, 0.2337, 0.316],\n",
 " [0.4515, 0.2499, 0.2985],\n",
 " [0.4563, 0.2303, 0.3134],\n",
 " [0.4513, 0.244, 0.3047]]"
 ]
 },
 "metadata": {},
 "execution_count": 209
 }
 ],
 "source": "[[0.4503, 0.2337, 0.3160],\n [0.4515, 0.2499, 0.2985],\n [0.4563, 0.2303, 0.3134],\n [0.4513, 0.2440, 0.3047]]"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "eK2_dOGa-747"
 },
 "source": "**1.g. Result: tensor for the logits into the answers file.**\n"
 },
 {
 "cell_type": "code",
 "execution_count": 210,
 "metadata": {
 "colab": {
 "base_uri": "https://localhost:8080/"
 },
 "id": "UhTwADop-8Bo",
 "outputId": "40b991c8-c2b1-4e9b-a13c-2b63572fbbd4",
 "executionInfo": {
 "status": "ok",
 "timestamp": 1757795890705,
 "user_tz": 360,
 "elapsed": 7,
 "user": {
 "displayName": "Ryan C",
 "userId": "13217471319505539255"
 }
 }
 },
 "outputs": [
 {
 "output_type": "execute_result",
 "data": {
 "text/plain": [
 "[[0.4086, -0.2475, 0.0545],\n",
 " [0.4077, -0.1839, -0.0061],\n",
 " [0.4272, -0.2564, 0.0514],\n",
 " [0.4109, -0.204, 0.018]]"
 ]
 },
 "metadata": {},
 "execution_count": 210
 }
 ],
 "source": "[[ 0.4086, -0.2475, 0.0545],\n [ 0.4077, -0.1839, -0.0061],\n [ 0.4272, -0.2564, 0.0514],\n [ 0.4109, -0.2040, 0.0180]]"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "GiL8z3tNl1he"
 },
 "source": "Great. Next do a calculation that *manually* verifies that the Softmax calculation is correct. Specifically, please recalculate the probability of the first class of the first example (use np.exp()... And if you don't want to use the numbers above, but the expressions probs and logits in a suitable way, use \\<tensor\\>.detach().numpy() to convert to numpy!)"
 },
 {
 "cell_type": "code",
 "execution_count": 211,
 "metadata": {
 "colab": {
 "base_uri": "https://localhost:8080/"
 },
 "id": "ntAbVVg1l1hf",
 "outputId": "b78bd071-d711-4d91-ce62-77131bbf7a55",
 "executionInfo": {
 "status": "ok",
 "timestamp": 1757795890720,
 "user_tz": 360,
 "elapsed": 14,
 "user": {
 "displayName": "Ryan C",
 "userId": "13217471319505539255"
 }
 }
 },
 "outputs": [
 {
 "output_type": "execute_result",
 "data": {
 "text/plain": [
 "np.float32(0.45032188)"
 ]
 },
 "metadata": {},
 "execution_count": 211
 }
 ],
 "source": "# call your probability of the first class for the first example p_1_1\n\np_1_1 = np.exp(logits.detach().numpy()[0,0])/(np.exp(logits.detach().numpy()[0][0]) + np.exp(logits.detach().numpy()[0][1]) + np.exp(logits.detach().numpy()[0][2]))\n\np_1_1"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "A5j_aT9fl1hf"
 },
 "source": "**1.h. Result: value of `p_1_1` into the answers file. (Note that the first class has index 0.)**\n"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "wZDCGjcol1hf"
 },
 "source": "Great. Now imagine that the correct classes are 0, 1, 2, 0 for the four examples. What is the average loss? For that we will first define the loss function and then calculate the loss. Note that the input to the CrossEntropyLoss() function are i) the un-normalized logits, and ii) either the class probabilities or the actual classes (better in this case as each example belongs to a class)."
 },
 {
 "cell_type": "code",
 "execution_count": 213,
 "metadata": {
 "colab": {
 "base_uri": "https://localhost:8080/"
 },
 "id": "1IErI7-zl1hf",
 "outputId": "0deb1416-4bae-4def-dcde-7367a2fd1bfd",
 "executionInfo": {
 "status": "ok",
 "timestamp": 1757795890770,
 "user_tz": 360,
 "elapsed": 13,
 "user": {
 "displayName": "Ryan C",
 "userId": "13217471319505539255"
 }
 }
 },
 "outputs": [
 {
 "output_type": "execute_result",
 "data": {
 "text/plain": [
 "tensor(1.0351, grad_fn=<NllLossBackward0>)"
 ]
 },
 "metadata": {},
 "execution_count": 213
 }
 ],
 "source": "loss_fn = torch.nn.CrossEntropyLoss()\n\nloss = loss_fn(logits, torch.tensor([0, 1, 2, 0]))\nloss"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "lSqViTkFl1hf"
 },
 "source": "Now verify that this loss agrees with the manual calculation. Recall from that\n\n$$ CE \\rightarrow -\\frac{1}{N}\\sum_k \\log(q^k_{{correct \\ class}}) $$\n\nwhere k refers to the example, N is the number of examples, and\n\n$$q_{{correct \\ class}}$$\n\nis the model probability for the correct class for a given example."
 },
 {
 "cell_type": "code",
 "execution_count": 214,
 "metadata": {
 "colab": {
 "base_uri": "https://localhost:8080/"
 },
 "id": "yvjQBsDPl1hf",
 "outputId": "2f1c07ba-0684-4558-d73e-57edeb25dbf0",
 "executionInfo": {
 "status": "ok",
 "timestamp": 1757795890778,
 "user_tz": 360,
 "elapsed": 6,
 "user": {
 "displayName": "Ryan C",
 "userId": "13217471319505539255"
 }
 }
 },
 "outputs": [
 {
 "output_type": "execute_result",
 "data": {
 "text/plain": [
 "np.float64(1.0351084035063098)"
 ]
 },
 "metadata": {},
 "execution_count": 214
 }
 ],
 "source": "# call your manual loss calculation manual_loss\n\nmanual_loss = 1/4 * (-np.log(0.4503) - np.log(0.2499) - np.log(0.3134) - np.log(0.4513))\n\nmanual_loss"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "W8fQeSmEl1hf"
 },
 "source": "**1.i. Write out the complete Cross-Entropy loss calculation as a single mathematical expression, substituting the values (the floating point numbers) you arrived at in your earlier work on this assignment. Your answer should be a single line showing the entire calculation with all necessary values inserted.**\n\n\nNote: Do not perform any calculations or simplify the expression. Simply write out the formula with the appropriate numeric values inserted."
 },
 {
 "cell_type": "code",
 "execution_count": 215,
 "metadata": {
 "id": "fQSzigUx_qCQ",
 "executionInfo": {
 "status": "ok",
 "timestamp": 1757795890781,
 "user_tz": 360,
 "elapsed": 2,
 "user": {
 "displayName": "Ryan C",
 "userId": "13217471319505539255"
 }
 }
 },
 "outputs": [],
 "source": "manual_loss = 1/4 * (-np.log(0.4503) - np.log(0.2499) - np.log(0.3134) - np.log(0.4513))"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "owiME_Khl1hf"
 },
 "source": "Great. Now we are ready to move to Language Models.\n\n## 2. Basic GPT-2 Usage\n\nWe are now downloading GPT-2 from Hugging Face. We will get the Tokenizer and the model. We will make sure that it is on the proper device."
 },
 {
 "cell_type": "code",
 "execution_count": 216,
 "metadata": {
 "id": "ysYdbU-2l1hr",
 "executionInfo": {
 "status": "ok",
 "timestamp": 1757795892579,
 "user_tz": 360,
 "elapsed": 1796,
 "user": {
 "displayName": "Ryan C",
 "userId": "13217471319505539255"
 }
 }
 },
 "outputs": [],
 "source": "%%capture\n\ngpt_2_tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n#gpt_2_tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n\ngpt2_model = GPT2LMHeadModel.from_pretrained(\"gpt2\").to(device)"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "ZJarqs_kl1hr"
 },
 "source": "We can simply apply the tokenizer to a sentence to see how words are converted into word indices (which will be model inputs and, as first order of business for the model, be converted to word vectors). (The tokenizer is model-specific and various tokenizers have some special considerations/quirks. You should always take a look at how a specific tokenizer works. Consult the Hugging Face docs and try some examples.)"
 },
 {
 "cell_type": "code",
 "execution_count": 217,
 "metadata": {
 "id": "bXbiGPo0l1hr",
 "executionInfo": {
 "status": "ok",
 "timestamp": 1757795892598,
 "user_tz": 360,
 "elapsed": 7,
 "user": {
 "displayName": "Ryan C",
 "userId": "13217471319505539255"
 }
 },
 "colab": {
 "base_uri": "https://localhost:8080/"
 },
 "outputId": "e1e182d4-305e-4e06-f56f-55ce8a8a73c1"
 },
 "outputs": [
 {
 "output_type": "execute_result",
 "data": {
 "text/plain": [
 "{'input_ids': tensor([[2061, 257, 3621, 1110]]), 'attention_mask': tensor([[1, 1, 1, 1]])}"
 ]
 },
 "metadata": {},
 "execution_count": 217
 }
 ],
 "source": "gpt_2_tokenizer(\"What a nice day\", return_tensors='pt')"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "OgcEzG4Rl1hr"
 },
 "source": "Note the difference between encodings of a word when the word is at the very beginning of a sentence vs a word that occurs later in the sentence. (The attention masks become important if you have examples of varying length and padding tokens are used to make sure that model inputs are of the same length. The return_tensors option is used to get the tokenization into a format that is suitable for model input, if desired. Following, we will omit the return_tensors option as we don't need it here.)\n\nBelow, consider the embedding for 'I' in the three tokenizations:"
 },
 {
 "cell_type": "code",
 "execution_count": 218,
 "metadata": {
 "id": "kk3irKCgl1hr",
 "executionInfo": {
 "status": "ok",
 "timestamp": 1757795892607,
 "user_tz": 360,
 "elapsed": 7,
 "user": {
 "displayName": "Ryan C",
 "userId": "13217471319505539255"
 }
 },
 "colab": {
 "base_uri": "https://localhost:8080/"
 },
 "outputId": "550fb732-b430-4b66-8e21-44f4c61cc986"
 },
 "outputs": [
 {
 "output_type": "stream",
 "name": "stdout",
 "text": [
 "[40, 716]\n",
 "[321, 314]\n",
 "[314]\n"
 ]
 }
 ],
 "source": "print(gpt_2_tokenizer(\"I am\")['input_ids'])\nprint(gpt_2_tokenizer(\"am I\")['input_ids'])\nprint(gpt_2_tokenizer(\" I\")['input_ids'])"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "ldDOXOb_l1hr"
 },
 "source": "Decoding (e.g. turn your input_id back into the coresponding string) is done with \\<tokenizer>.decode():"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "O6VIV_Qxl1hr"
 },
 "source": "Please tokenize the longest word that Shakespeare used: 'honorificabilitudinitatibus' (when not at the beginning of a sentence), and find the first token (not the id, but the corresponding token string):"
 },
 {
 "cell_type": "code",
 "execution_count": 220,
 "metadata": {
 "id": "Qto1E5mql1hr",
 "executionInfo": {
 "status": "ok",
 "timestamp": 1757795892625,
 "user_tz": 360,
 "elapsed": 11,
 "user": {
 "displayName": "Ryan C",
 "userId": "13217471319505539255"
 }
 },
 "colab": {
 "base_uri": "https://localhost:8080/"
 },
 "outputId": "0945033e-083a-4886-f2dc-25fdb76595cf"
 },
 "outputs": [
 {
 "output_type": "stream",
 "name": "stdout",
 "text": [
 "Tokenized long word: [7522, 811, 397, 6392, 463, 15003, 265, 26333]\n",
 "Length of tokenized long word: 8\n",
 "First index: 7522\n",
 "First token: honor\n"
 ]
 }
 ],
 "source": "# Name your tokenization tokenized_long_word, the index of the first token first_index, and the first token first_token\n\ntokenized_long_word = gpt_2_tokenizer(\" honorificabilitudinitatibus\")['input_ids']\nfirst_index = tokenized_long_word[0]\nfirst_token = gpt_2_tokenizer.decode(first_index)\n\nprint('Tokenized long word: ', tokenized_long_word)\nprint('Length of tokenized long word: ', len(tokenized_long_word))\nprint('First index: ', first_index)\nprint('First token: ', first_token)"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "94iz3CUjl1hr"
 },
 "source": "**2.a. Analyzing how many tokens is the word honorificabilitudinitatibus split when not in the beginning of the sentence? (You can either create a sentence with this word where it is not in the beginning, or you need to make sure that there is a space in front of the word.)**\n"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "XlTS1Cbz_6qe"
 },
 "source": "**2.b. Examining the first token of the tokenization**\n"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "c_dG50rFl1hs"
 },
 "source": "Now redo the same, but imagine the word 'honorificabilitudinitatibus' at the very start of a sentence/doc (never mind the capitalization) as in\n'honorificabilitudinitatibus is a state I am in'."
 },
 {
 "cell_type": "code",
 "execution_count": 223,
 "metadata": {
 "id": "ttvKEXbGl1hs",
 "executionInfo": {
 "status": "ok",
 "timestamp": 1757795892698,
 "user_tz": 360,
 "elapsed": 20,
 "user": {
 "displayName": "Ryan C",
 "userId": "13217471319505539255"
 }
 },
 "colab": {
 "base_uri": "https://localhost:8080/"
 },
 "outputId": "66f4db28-49ae-4d6a-b44e-5a96c424fc18"
 },
 "outputs": [
 {
 "output_type": "stream",
 "name": "stdout",
 "text": [
 "Tokenized long word: [24130, 273, 811, 397, 6392, 463, 15003, 265, 26333]\n",
 "First index: 24130\n",
 "First token string: hon\n"
 ]
 }
 ],
 "source": "# Now name your tokenization beg_tokenized_long_word, the index of the first token beg_first_index, and the first token beg_first_token\n\nbeg_tokenized_long_word = gpt_2_tokenizer(\"honorificabilitudinitatibus\")['input_ids']\nbeg_first_index = beg_tokenized_long_word[0]\nbeg_first_token = gpt_2_tokenizer.decode(beg_first_index)\n\nprint('Tokenized long word: ', beg_tokenized_long_word)\nprint('First index: ', beg_first_index)\nprint('First token string: ', beg_first_token)"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "pQHssnbwl1hs"
 },
 "source": "**2.c. Analyzing how many tokens is the word honorificabilitudinitatibus now be split (when **in the beginning** of the sentence)**\n"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "I8Pd-V7UAMNR"
 },
 "source": "**2.d. Examining now the first token string of the tokenization**\n"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "hjJQGYNOl1hs"
 },
 "source": "Now apply the gpt2_model to a sentence in order to predict the most likely next word after 'The movement started in Italy. From there it went to France and Switzerland. Soon it spread throughout'. You may want to and the documentation.\n\nPlease get i) the shape of the output, ii) the values of the logits of the last token, iii) the index of the token with largest logit, and iv) the token that belongs to it. Same for the token with the second most largest logit."
 },
 {
 "cell_type": "code",
 "execution_count": 226,
 "metadata": {
 "id": "MpFr-U49l1hs",
 "executionInfo": {
 "status": "ok",
 "timestamp": 1757795892807,
 "user_tz": 360,
 "elapsed": 67,
 "user": {
 "displayName": "Ryan C",
 "userId": "13217471319505539255"
 }
 },
 "colab": {
 "base_uri": "https://localhost:8080/"
 },
 "outputId": "3e7251c9-68bb-4ac0-a9f7-ab0ede00614b"
 },
 "outputs": [
 {
 "output_type": "stream",
 "name": "stdout",
 "text": [
 "Output shape: torch.Size([1, 20, 50257])\n",
 "Logits of output for last token: tensor([ -95.2277, -96.9647, -101.5158, ..., -102.7920, -104.4167,\n",
 " -97.2480], grad_fn=<SelectBackward0>)\n",
 "Index of token with largest logit: tensor(262)\n",
 "Token with largest logit: the\n",
 "Logit of token with largest logit: tensor(-84.7964, grad_fn=<MaxBackward1>)\n",
 "Index of token with second largest logit: tensor(2031)\n",
 "Token with second largest logit: Europe\n",
 "Logit of token with second largest logit: tensor(-84.9965, grad_fn=<SelectBackward0>)\n"
 ]
 }
 ],
 "source": "text = \"The movement started in Italy. From there it went to France and Switzerland. Soon it spread throughout\"\ninput = gpt_2_tokenizer(text, return_tensors='pt')\n\ngpt_out = gpt2_model(**input)\n\n# Please call your model output shape, output_shape, and the logits for the last position last_logits,\n# and the index for the token with the largest last_logit value max_logit_index, and the corresponding token max_logit_token.\n# Name the corresponding values for the second largest logit second_logit_index, second_logit_token, and second_logit.\n\n# Outputs\nlogits = gpt_out.logits\noutput_shape = logits.shape\noutput_logits = logits[0][-1]\n\n#Max token\nmax_logit_index = torch.argmax(output_logits)\nmax_logit = torch.max(output_logits)\nmax_logit_token = gpt_2_tokenizer.decode(max_logit_index)\n\ntop_2_values, top_2_indices = torch.topk(output_logits, 2)\nsecond_logit_index = top_2_indices[1]\nsecond_logit = top_2_values[1]\nsecond_logit_token = gpt_2_tokenizer.decode(second_logit_index)\n\nprint('Output shape: ', output_shape)\nprint('Logits of output for last token: ', output_logits)\nprint('Index of token with largest logit: ', max_logit_index)\nprint('Token with largest logit: ', max_logit_token)\nprint('Logit of token with largest logit: ', max_logit)\nprint('Index of token with second largest logit: ', second_logit_index)\nprint('Token with second largest logit: ', second_logit_token)\nprint('Logit of token with second largest logit: ', second_logit)"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "YVqWVn7Tl1hs"
 },
 "source": "**2.e. Examining the shape of the output**\n"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "CqCZ4hEKAyxm"
 },
 "source": "**2.f. What do the three numbers shape refer to**\n"
 },
 {
 "cell_type": "code",
 "execution_count": 228,
 "metadata": {
 "id": "VcuNcrUEAy4Y",
 "executionInfo": {
 "status": "ok",
 "timestamp": 1757795892838,
 "user_tz": 360,
 "elapsed": 7,
 "user": {
 "displayName": "Ryan C",
 "userId": "13217471319505539255"
 }
 },
 "colab": {
 "base_uri": "https://localhost:8080/"
 },
 "outputId": "3879fb57-4d19-4fe7-f315-240c6359141f"
 },
 "outputs": [
 {
 "output_type": "stream",
 "name": "stdout",
 "text": [
 "\n",
 "The output is the size of the logits we produce.\n",
 "The first number (1) is the batch size - we're processing one text sequence.\n",
 "The second number (20) is the length of our input sequence in tokens.\n",
 "The third number (50257) is the vocabulary size - GPT-2 produces logits for every possible token in its vocabulary at each position.\n",
 "\n"
 ]
 }
 ],
 "source": "print(\"\"\"\nThe output is the size of the logits we produce.\nThe first number (1) is the batch size - we're processing one text sequence.\nThe second number (20) is the length of our input sequence in tokens.\nThe third number (50257) is the vocabulary size - GPT-2 produces logits for every possible token in its vocabulary at each position.\n\"\"\"\n)"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "xoU6I9UFAy_j"
 },
 "source": "**2.g. Examining the index of the word with the largest logit**\n"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "dMEv0UNeAzN9"
 },
 "source": "**2.h. Examining the token string associated with the largest logit**\n"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "BwK9cDX7Azfl"
 },
 "source": "**2.i. Examining the second most likely token id**\n"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "rvpI-4QQAzuD"
 },
 "source": "**2.j. Examining the second most likely word**\n"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "UC3up6pwl1hs"
 },
 "source": "Now we will translate the logits into relative token probabilities depending on the chosen temperature. Use numpy or pytorch calculations. (But remember to use .detach() etc if you want to use numpy.)"
 },
 {
 "cell_type": "code",
 "execution_count": 233,
 "metadata": {
 "id": "rqVBaCEtl1hs",
 "executionInfo": {
 "status": "ok",
 "timestamp": 1757795892979,
 "user_tz": 360,
 "elapsed": 62,
 "user": {
 "displayName": "Ryan C",
 "userId": "13217471319505539255"
 }
 },
 "colab": {
 "base_uri": "https://localhost:8080/"
 },
 "outputId": "dfba8975-06d7-4358-96aa-a034edc3cd5c"
 },
 "outputs": [
 {
 "output_type": "stream",
 "name": "stdout",
 "text": [
 "Logit ratio for T1: tensor(1.2215, grad_fn=<DivBackward0>)\n",
 "Logit ratio for T2: tensor(1.0202, grad_fn=<DivBackward0>)\n",
 "Logit ratio for T3: tensor(7.3922, grad_fn=<DivBackward0>)\n"
 ]
 }
 ],
 "source": "T_1 = 1.\nT_2 = 10.\nT_3 = 0.1\n\n# Please call your relative probabilities between the most likely token and the second most likely token p_t1, p_t2, p_t3, depending\n# on the temperature\n\n# Apply softmax with different temperatures to the last position logits\nprobs_t1 = torch.softmax(output_logits / T_1, dim=0)\nprobs_t2 = torch.softmax(output_logits / T_2, dim=0)\nprobs_t3 = torch.softmax(output_logits / T_3, dim=0)\n\n# Get the probabilities for the top 2 tokens\np_t1 = probs_t1[max_logit_index] / probs_t1[second_logit_index]\np_t2 = probs_t2[max_logit_index] / probs_t2[second_logit_index]\np_t3 = probs_t3[max_logit_index] / probs_t3[second_logit_index]\n\nprint('Logit ratio for T1: ', p_t1)\nprint('Logit ratio for T2: ', p_t2)\nprint('Logit ratio for T3: ', p_t3)"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "Ap07xAI-l1hs"
 },
 "source": "**2.k. Examining the ratio of probabilities between the most likely token and the second most likely token if T=1**\n"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "c6SIoGPfBlf1"
 },
 "source": "**2.l. Examining the ratio of probabilities between the most likely token and the second most likely token if T=10**\n"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "kQ1B2JuWBlvF"
 },
 "source": "**2.m. Examining the ratio of probabilities between the most likely token and the second most likely token if T=0.1? (Hint: to avoid a NaN you may want to use a simple mathematical identity to deal with the low temperature: $e^a/e^b = e^{(a-b)}$)**\n"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "oLhK3eKfl1hs"
 },
 "source": "And that is it! Congratulations!"
 }
 ],
 "metadata": {
 "colab": {
 "provenance": []
 },
 "kernelspec": {
 "display_name": "Python 3 (ipykernel)",
 "language": "python",
 "name": "python3"
 },
 "language_info": {
 "codemirror_mode": {
 "name": "ipython",
 "version": 3
 },
 "file_extension": ".py",
 "mimetype": "text/x-python",
 "name": "python",
 "nbconvert_exporter": "python",
 "pygments_lexer": "ipython3",
 "version": "3.11.5"
 }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}