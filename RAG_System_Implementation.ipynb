{
 "cells": [
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "czxW4C_gf5Eo"
 },
 "source": "# RAG System: Retrieval-Augmented Generation Implementation\n\nThis notebook demonstrates a complete Retrieval-Augmented Generation (RAG) system implementation for enterprise question-answering.\n\n**Skills Demonstrated:**\n- RAG architecture design and implementation\n- Vector database integration for document retrieval\n- LangChain/LangGraph pipeline construction\n- Embedding models and similarity search\n- LLM integration for answer generation\n- RAG evaluation metrics\n\n**Technologies:** LangChain, Vector Databases, Embedding Models, LLMs"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "dylSkMC5ajr_"
 },
 "source": "# LLM Usage Clarification in A5\n\nPLEASE READ THESE DIRECTIONS CAREFULLY\n\nWe want to clarify the boundaries for using LLMs (commercial and open source) in your completion of a5. Beyond the obvious use of the Mistral and Cohere LLMs as the question answering portion of your RAG pipeline, you may use an LLM to assist in certain portions of the project. For example, you may use an LLM to assist with re-writing portions of your report if you wish. We would discourage you from using the LLM to write code.\n\n**You MUST NOT:**\n\n* You must **NOT** use an LLM to answer any questions we posed. For example, do not use it to answer the questions in sections 4 or 5 of the notebook.\n\n* You must **NOT** use the LLM to generate the initial draft of your final report.\n\n* You must **NOT** use the LLM to generate or assist with your metrics justification. We want to know what you think in your own words.\n\n**You CAN:**\n\n* You can use an LLM to help you with the report writing as far as formulations and phrasing go, but obviously not to construct core content.\n\n* You should turn off the use of your input to help with training commercial LLMs. For example, in OpenAI\u2019s ChatGPT you can go to Settings in your account and then Data Controls and then make sure that \u2018Improve the model for everyone\u2019 is set to off.\n\n**You MUST:**\n\n* You **MUST** tell us how you have used an LLM by specifying which model(s) you used, how you used the model, and which portion of your output it helped to generate. You can include this in the references section and put citations into the body of your final report."
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "luSQWNX9XvGN"
 },
 "source": "## 1. Setup\n\nWe will first install a number of libraries and import what we will need."
 },
 {
 "cell_type": "code",
 "execution_count": 2,
 "metadata": {
 "colab": {
 "base_uri": "https://localhost:8080/"
 },
 "executionInfo": {
 "elapsed": 15373,
 "status": "ok",
 "timestamp": 1765128208880,
 "user": {
 "displayName": "Ryan C",
 "userId": "13217471319505539255"
 },
 "user_tz": 420
 },
 "id": "ZcFPKbw1zoVr",
 "outputId": "064fc2d2-94d6-45bd-d7d8-f364980adcb0"
 },
 "outputs": [
 {
 "output_type": "stream",
 "name": "stdout",
 "text": [
 "Mounted at /content/drive\n"
 ]
 }
 ],
 "source": "import os\nfrom google.colab import drive\ndrive.mount('/content/drive')"
 },
 {
 "cell_type": "code",
 "execution_count": 3,
 "metadata": {
 "id": "WIlhfQj-KUlZ",
 "executionInfo": {
 "status": "ok",
 "timestamp": 1765128345121,
 "user_tz": 420,
 "elapsed": 136242,
 "user": {
 "displayName": "Ryan C",
 "userId": "13217471319505539255"
 }
 }
 },
 "outputs": [],
 "source": "%%capture\n!pip install -q -U transformers\n!pip install -q -U datasets loralib sentencepiece\n!pip install -q bitsandbytes accelerate\n!pip install -q langchain\n!pip install langchain-core\n!pip install langchain-classic\n!pip install einops\n!pip install faiss-gpu\n!pip install langchain_community\n!pip install --upgrade --quiet chromadb bs4 qdrant-client\n!pip install langchainhub\n!pip install -U langchain-huggingface\n!pip install -U langchain-cohere\n!pip install --upgrade --quiet wikipedia\n!pip install --upgrade --quiet arxiv\n!pip install --upgrade --quiet pymupdf\n\n!pip install xmltodict\n\n!pip install cohere\n!pip install langchain-qdrant"
 },
 {
 "cell_type": "code",
 "execution_count": 4,
 "metadata": {
 "colab": {
 "base_uri": "https://localhost:8080/"
 },
 "executionInfo": {
 "elapsed": 10496,
 "status": "ok",
 "timestamp": 1765128355632,
 "user": {
 "displayName": "Ryan C",
 "userId": "13217471319505539255"
 },
 "user_tz": 420
 },
 "id": "R2xlde906w1L",
 "outputId": "43e277e7-71df-42eb-95c1-ca8696b8f2bb"
 },
 "outputs": [
 {
 "output_type": "stream",
 "name": "stdout",
 "text": [
 "sentence-transformers 5.1.2\n",
 "transformers 4.57.3\n",
 "accelerate 1.12.0\n",
 "langchain 1.1.0\n",
 "langchain-classic 1.0.0\n",
 "langchain-cohere 0.5.0\n",
 "langchain-community 0.4.1\n",
 "langchain-core 1.1.0\n",
 "langchain-huggingface 1.1.0\n",
 "langchain-qdrant 1.1.0\n",
 "langchain-text-splitters 1.0.0\n",
 "langchainhub 0.1.21\n"
 ]
 }
 ],
 "source": "#In case we want to know our installed transformers library version\n!pip list | grep transformers\n!pip list | grep accelerate\n!pip list | grep langchain"
 },
 {
 "cell_type": "code",
 "execution_count": 5,
 "metadata": {
 "id": "3NjcvYABKieZ",
 "colab": {
 "base_uri": "https://localhost:8080/"
 },
 "executionInfo": {
 "status": "ok",
 "timestamp": 1765128416848,
 "user_tz": 420,
 "elapsed": 61194,
 "user": {
 "displayName": "Ryan C",
 "userId": "13217471319505539255"
 }
 },
 "outputId": "1ebea0a7-f981-430c-d243-39fb66aaef64"
 },
 "outputs": [
 {
 "output_type": "stream",
 "name": "stderr",
 "text": [
 "WARNING:langchain_community.utils.user_agent:USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
 ]
 }
 ],
 "source": "import torch\nimport os\nimport bs4\nimport json\nimport numpy as np\nimport time\n\nfrom pprint import pprint\n\nimport locale\n\nfrom transformers import AutoTokenizer , AutoModelForCausalLM\nfrom transformers import pipeline, BitsAndBytesConfig\nfrom langchain_huggingface import HuggingFacePipeline\n\nfrom langchain_cohere import ChatCohere\n\nfrom langchain_core.prompts import PromptTemplate\nfrom langchain_classic.chains import LLMChain\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_community.embeddings import HuggingFaceEmbeddings\nfrom langchain_core.output_parsers import StrOutputParser\n\nfrom langchain_text_splitters import CharacterTextSplitter\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain_classic import hub\nfrom langchain_community.document_loaders import WebBaseLoader\nfrom langchain_community.document_loaders import TextLoader\nfrom langchain_community.vectorstores import FAISS\nfrom langchain_community.vectorstores import Chroma\n#from langchain_community.vectorstores import Qdrant\nfrom langchain_qdrant import QdrantVectorStore as Qdrant\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain_core.runnables import RunnablePassthrough\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\nfrom langchain_community.utils.math import cosine_similarity\n\nfrom langchain_community.document_loaders import ArxivLoader\nfrom langchain_community.document_loaders import TextLoader\nfrom langchain_community.document_loaders import WikipediaLoader\nfrom langchain_community.document_loaders import OnlinePDFLoader\nfrom langchain_community.document_loaders import PyMuPDFLoader\nfrom langchain_community.document_loaders import PubMedLoader\n\nfrom google.colab import userdata\nos.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')"
 },
 {
 "cell_type": "code",
 "execution_count": 6,
 "metadata": {
 "colab": {
 "base_uri": "https://localhost:8080/"
 },
 "executionInfo": {
 "elapsed": 15,
 "status": "ok",
 "timestamp": 1765128416865,
 "user": {
 "displayName": "Ryan C",
 "userId": "13217471319505539255"
 },
 "user_tz": 420
 },
 "id": "92TiTjV0L-WU",
 "outputId": "9358786e-d643-43fa-a1fc-60398699a4dd"
 },
 "outputs": [
 {
 "output_type": "stream",
 "name": "stdout",
 "text": [
 "1.1.0\n"
 ]
 }
 ],
 "source": "import langchain\nprint(langchain.__version__)"
 },
 {
 "cell_type": "code",
 "execution_count": 7,
 "metadata": {
 "id": "faZ5fLk_xxAO",
 "executionInfo": {
 "status": "ok",
 "timestamp": 1765128416885,
 "user_tz": 420,
 "elapsed": 18,
 "user": {
 "displayName": "Ryan C",
 "userId": "13217471319505539255"
 }
 }
 },
 "outputs": [],
 "source": "locale.getpreferredencoding = lambda: \"UTF-8\""
 },
 {
 "cell_type": "code",
 "execution_count": 8,
 "metadata": {
 "id": "Stlb_ciPxxWA",
 "executionInfo": {
 "status": "ok",
 "timestamp": 1765128427880,
 "user_tz": 420,
 "elapsed": 10993,
 "user": {
 "displayName": "Ryan C",
 "userId": "13217471319505539255"
 }
 }
 },
 "outputs": [],
 "source": "%%capture\n!pip install -U sentence_transformers"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "xJWr9TkCa7gG"
 },
 "source": "Add your keys from the secret store (do **NOT** print them out or leave them exposed as plaintext in your notebook!):"
 },
 {
 "cell_type": "code",
 "execution_count": 9,
 "metadata": {
 "id": "ll9IqkVMa7qP",
 "executionInfo": {
 "status": "ok",
 "timestamp": 1765128428194,
 "user_tz": 420,
 "elapsed": 313,
 "user": {
 "displayName": "Ryan C",
 "userId": "13217471319505539255"
 }
 }
 },
 "outputs": [],
 "source": "COHERE_API_KEY = userdata.get('COHERE_API_KEY')"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "xlSHHPW-f3ZL"
 },
 "source": "## 2. Building the Components of our RAG System\n\nLet us introduce and test the base components of our RAG system. We will largely use the Hugging Face and LangChain libraries."
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "6N3fqR5vKV9b"
 },
 "source": "### 2.1 The Embedding Model\n\nWe will need to represent text (pieces) as vectors. For this, we will use the [sentence_transformer](https://sbert.net/docs/sentence_transformer/pretrained_models.html) architecture.\n\n**NOTE:** The embedding models you can use are: 'all-mpnet-base-v2', 'all-MiniLM-L12-v2', 'multi-qa-mpnet-base-dot-v1', 'all-distilroberta-v1', and 'multi-qa-distilbert-cos-v1 '"
 },
 {
 "cell_type": "code",
 "execution_count": 10,
 "metadata": {
 "id": "m_AqjidjKWif",
 "executionInfo": {
 "status": "ok",
 "timestamp": 1765128428213,
 "user_tz": 420,
 "elapsed": 16,
 "user": {
 "displayName": "Ryan C",
 "userId": "13217471319505539255"
 }
 }
 },
 "outputs": [],
 "source": "# %%capture\n# base_embeddings = HuggingFaceEmbeddings(model_name=\"multi-qa-mpnet-base-dot-v1\")"
 },
 {
 "cell_type": "code",
 "execution_count": 11,
 "metadata": {
 "id": "Cgzrqje8PN8S",
 "executionInfo": {
 "status": "ok",
 "timestamp": 1765128428216,
 "user_tz": 420,
 "elapsed": 1,
 "user": {
 "displayName": "Ryan C",
 "userId": "13217471319505539255"
 }
 }
 },
 "outputs": [],
 "source": "# text = \"This is a test document.\"\n# query_result = base_embeddings.embed_query(text)\n# print(f'Embedding dimension: {len(query_result)}')\n\n# doc_result = base_embeddings.embed_documents([\"Germany won the World Cup 4 times.\", \"This is a training document.\"])\n# len(doc_result)"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "CPQGVSGgzG6F"
 },
 "source": "Do those dimensions look correct?\n\nNow lets see if the embedding model is working as we want. Ideally our embeddings go beyond shared words and capture the underlying meaning."
 },
 {
 "cell_type": "code",
 "execution_count": 12,
 "metadata": {
 "id": "zKlCdEeKveqN",
 "executionInfo": {
 "status": "ok",
 "timestamp": 1765128428230,
 "user_tz": 420,
 "elapsed": 13,
 "user": {
 "displayName": "Ryan C",
 "userId": "13217471319505539255"
 }
 }
 },
 "outputs": [],
 "source": "# #Let's see how well our embeddng model works\n# similarity = cosine_similarity([query_result], doc_result)[0]\n\n# similarity"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "fAp4fF5dPhbM"
 },
 "source": "That's how you should define your embedding models.\n\nNext, we turn to text chunks.\n\n### 2.2. Loading and Chunking Texts\n\nWe first need to load the documents. Here is an example:"
 },
 {
 "cell_type": "code",
 "execution_count": 13,
 "metadata": {
 "id": "4ak5XlaUP1cW",
 "executionInfo": {
 "status": "ok",
 "timestamp": 1765128428233,
 "user_tz": 420,
 "elapsed": 1,
 "user": {
 "displayName": "Ryan C",
 "userId": "13217471319505539255"
 }
 }
 },
 "outputs": [],
 "source": "# loader = WebBaseLoader(\n# web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n# bs_kwargs=dict(\n# parse_only=bs4.SoupStrainer(\n# class_=(\"post-content\", \"post-title\", \"post-header\")\n# )\n# ),\n# )\n\n# documents = loader.load()"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "cZXtDqKcQHlT"
 },
 "source": "We will need to split the text in chunks that are 'suitable' as retrieval units. Let's for starters define a chunk size of 128 and have no overlap between the chunks:"
 },
 {
 "cell_type": "code",
 "execution_count": 14,
 "metadata": {
 "id": "qhcWoTajQnw6",
 "executionInfo": {
 "status": "ok",
 "timestamp": 1765128428236,
 "user_tz": 420,
 "elapsed": 2,
 "user": {
 "displayName": "Ryan C",
 "userId": "13217471319505539255"
 }
 }
 },
 "outputs": [],
 "source": "# text_splitter = RecursiveCharacterTextSplitter(chunk_size=128, chunk_overlap=0)\n# splits = text_splitter.split_documents(documents)\n# print('Number of splits/chunks: ', str(len(splits)))"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "OUrNISkVRB_U"
 },
 "source": "Ok, so it looks like we have now many splits (chunks) from one document. Here is how you can get the content:"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "Z-RuP_9xRVtG"
 },
 "source": "Perfect. Now we have the splits and embeddings. Next, the embeddings need to be stored in a vector db.\n\n### 2.3 Storing the Embeddings of Chunks in Vectorstores\n\nAfter loading and chunking the data, we need to save the vector representations of the chunks in a vectorstore. We will use Qdrant here for simplicity. We load the splits (structured chunks) and the embeddings:"
 },
 {
 "cell_type": "code",
 "execution_count": 16,
 "metadata": {
 "id": "9T3FVDoJRglN",
 "executionInfo": {
 "status": "ok",
 "timestamp": 1765128428250,
 "user_tz": 420,
 "elapsed": 1,
 "user": {
 "displayName": "Ryan C",
 "userId": "13217471319505539255"
 }
 }
 },
 "outputs": [],
 "source": "# vectorstore = Qdrant.from_documents(splits,\n# base_embeddings,\n# location=\":memory:\", # Local mode with in-memory storage only\n# collection_name=\"test\",\n# )\n# retriever = vectorstore.as_retriever()"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "ar4bpzJtRrio"
 },
 "source": "The nice thing is that the vector store also does the similarity searches for us:"
 },
 {
 "cell_type": "code",
 "execution_count": 17,
 "metadata": {
 "id": "tiSqastIP1oT",
 "executionInfo": {
 "status": "ok",
 "timestamp": 1765128428252,
 "user_tz": 420,
 "elapsed": 1,
 "user": {
 "displayName": "Ryan C",
 "userId": "13217471319505539255"
 }
 }
 },
 "outputs": [],
 "source": "# query = \"What is Chain of Thought doing?\"\n# docs = vectorstore.similarity_search_by_vector(base_embeddings.embed_query(query)) # will rank the splits"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "yyypf-VZRsxg"
 },
 "source": "Looks good! We have an ordered list of documents that seem to relate to the question. That is what we need.\n\nThe last major component is the actual LLM."
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "lDDom9EbKXCX"
 },
 "source": "### 2.4. The LLM\n\nWe will use one Open Source Model (\"mistralai/Mistral-7B-Instruct-v0.3\") and one Proprietery Model (Cohere) for our tests. Let's first set up the OS model:"
 },
 {
 "cell_type": "code",
 "execution_count": 19,
 "metadata": {
 "id": "XPuC4Kyxg5B-",
 "executionInfo": {
 "status": "ok",
 "timestamp": 1765128428,
 "user_tz": 420,
 "elapsed": 1,
 "user": {
 "displayName": "Ryan C",
 "userId": "13217471319505539255"
 }
 }
 },
 "outputs": [],
 "source": "# #Quantization config\n\n# quantization_config = BitsAndBytesConfig(\n# load_in_4bit=True,\n# bnb_4bit_quant_type=\"nf4\",\n# bnb_4bit_use_double_quant=True,\n# bnb_4bit_compute_dtype=torch.bfloat16\n# )"
 },
 {
 "cell_type": "code",
 "execution_count": 20,
 "metadata": {
 "id": "YooxnCPNOoQ7",
 "executionInfo": {
 "status": "ok",
 "timestamp": 1765128428270,
 "user_tz": 420,
 "elapsed": 1,
 "user": {
 "displayName": "Ryan C",
 "userId": "13217471319505539255"
 }
 }
 },
 "outputs": [],
 "source": "# #%%capture\n\n# llm_mistral_model = AutoModelForCausalLM.from_pretrained(\n# \"mistralai/Mistral-7B-Instruct-v0.3\",\n# dtype=torch.float32,\n# device_map='auto',\n# quantization_config=quantization_config\n# )\n\n# llm_mistral_tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.3\")"
 },
 {
 "cell_type": "code",
 "execution_count": 21,
 "metadata": {
 "id": "cS7q1dJvW6lc",
 "executionInfo": {
 "status": "ok",
 "timestamp": 1765128428309,
 "user_tz": 420,
 "elapsed": 38,
 "user": {
 "displayName": "Ryan C",
 "userId": "13217471319505539255"
 }
 }
 },
 "outputs": [],
 "source": "# # RC - Modified to store and download\n# # Download and save to Google Drive (If running from Colab)\n\n# import os\n# from google.colab import drive\n# drive.mount('/content/drive')\n\n# model_save_path = \"/content/drive/MyDrive/models/mistral-7b-instruct\"\n\n# # Check if already saved\n# if os.path.exists(model_save_path):\n# print(\"Loading model from Google Drive...\")\n# model_source = model_save_path\n# else:\n# print(\"Downloading model from Hugging Face (this will take ~15 min)...\")\n# model_source = \"mistralai/Mistral-7B-Instruct-v0.3\"\n\n# # Load from appropriate source\n# llm_mistral_model = AutoModelForCausalLM.from_pretrained(\n# model_source,\n# dtype=torch.float32,\n# device_map='auto',\n# quantization_config=quantization_config\n# )\n\n# llm_mistral_tokenizer = AutoTokenizer.from_pretrained(model_source)\n\n# # If we downloaded, save it for next time\n# if model_source != model_save_path:\n# print(\"Saving model to Google Drive for future use...\")\n# llm_mistral_model.save_pretrained(model_save_path)\n# llm_mistral_tokenizer.save_pretrained(model_save_path)\n# print(\"Saved!\")\n\n# llm_mistral_tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.3\")"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "ZVGOr2cV_q6I"
 },
 "source": "We use the model first to generate a Hugging Face pipeline. A pipeline simplifies the process of actually generating responses."
 },
 {
 "cell_type": "code",
 "execution_count": 22,
 "metadata": {
 "id": "Rvxo5OKwvjNN",
 "executionInfo": {
 "status": "ok",
 "timestamp": 1765128428311,
 "user_tz": 420,
 "elapsed": 1,
 "user": {
 "displayName": "Ryan C",
 "userId": "13217471319505539255"
 }
 }
 },
 "outputs": [],
 "source": "# mistral_pipe = pipeline(\n# \"text-generation\",\n# model=llm_mistral_model,\n# tokenizer=llm_mistral_tokenizer,\n# max_new_tokens=1000,\n# temperature=0.6,\n# top_p=0.95,\n# do_sample=True,\n# repetition_penalty=1.2\n# )\n# mistral_pipe.model.config.pad_token_id = mistral_pipe.model.config.eos_token_id\n\n# # wrapping the Hugging Face pipeline into a LangChain object"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "kOHYPzDiTRIK"
 },
 "source": "Does it work?"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "Sh-60AEBTODZ"
 },
 "source": "Reasonable!\n\nWe will also use a Cohere model, but will create this below as part of the LangChain framework.\n\n### 2.5 Testing the LLM in a LangChain Chain\n\nChains will be defined and discussed in . In short, they are convenient programmatic ways to deal with 'chains' of actions that involve LLMs. For example, a list of events like 'here is a city name. Plug that city name into prompt template, then generate a story about that city. Lastly, format the model output as a string' can be easily handled by LangChain's Chain framework. In this case, the Chain would consist of the prompt template, the LLM, and the String Formatter. The parameter (the city in this case) will be provided at run time by invocation of the Chain. Let's test that.\n\nTo use a Hugging Face model in a LangChain environment, we need to wrap the model into a LangChain pipeline object:"
 },
 {
 "cell_type": "code",
 "execution_count": 24,
 "metadata": {
 "id": "P1LSyyFmTOYP",
 "executionInfo": {
 "status": "ok",
 "timestamp": 1765128428316,
 "user_tz": 420,
 "elapsed": 1,
 "user": {
 "displayName": "Ryan C",
 "userId": "13217471319505539255"
 }
 }
 },
 "outputs": [],
 "source": "# mistral_llm_lc = HuggingFacePipeline(pipeline=mistral_pipe)"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "3FkQde8yZYqZ"
 },
 "source": "Next, we need to define a template and create a corresponding prompt template that can take any questiion"
 },
 {
 "cell_type": "code",
 "execution_count": 25,
 "metadata": {
 "id": "dh3R9445K5ct",
 "executionInfo": {
 "status": "ok",
 "timestamp": 1765128428318,
 "user_tz": 420,
 "elapsed": 1,
 "user": {
 "displayName": "Ryan C",
 "userId": "13217471319505539255"
 }
 }
 },
 "outputs": [],
 "source": "# test_llm_template = \"\"\"[INST] Give me a two-sentence story about an {object}! [/INST]\"\"\"\n# test_llm_prompt_template = PromptTemplate(template=test_llm_template, input_variables=[\"object\"])"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "_V6PO2LISDbT"
 },
 "source": "Let's define a Chain, a static flow of actions that (usually) involve at least a definition of the variables used in the chain, one or more templates, LLM step(s) and potentially other actions. This would be a chain that declares the variable 'object' to be expected when the chain is invoked, then inserts it into the template, and passes this to our mistral model pipeline (wrapped as a LangChain object):"
 },
 {
 "cell_type": "code",
 "execution_count": 26,
 "metadata": {
 "id": "aSvSoD7hZb4Z",
 "executionInfo": {
 "status": "ok",
 "timestamp": 1765128428435,
 "user_tz": 420,
 "elapsed": 101,
 "user": {
 "displayName": "Ryan C",
 "userId": "13217471319505539255"
 }
 }
 },
 "outputs": [],
 "source": "# test_llm_chain_short = (\n# {\"object\": RunnablePassthrough()}\n# | test_llm_prompt_template\n# | mistral_llm_lc\n# )"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "SYeKmOoMaSKS"
 },
 "source": "Works too. We will use this notation moving forward.\n\nNext, how would we do this with a Cohere Chat Model instead of Mistral?"
 },
 {
 "cell_type": "code",
 "execution_count": 28,
 "metadata": {
 "id": "JROGul-gZcAU",
 "executionInfo": {
 "status": "ok",
 "timestamp": 1765128428441,
 "user_tz": 420,
 "elapsed": 2,
 "user": {
 "displayName": "Ryan C",
 "userId": "13217471319505539255"
 }
 }
 },
 "outputs": [],
 "source": "# cohere_chat_model = ChatCohere(cohere_api_key=COHERE_API_KEY)"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "-h3DW_bBfPsI"
 },
 "source": "This can be plugged straight into the Chain:"
 },
 {
 "cell_type": "code",
 "execution_count": 29,
 "metadata": {
 "id": "AJ5bzWTVaZdO",
 "executionInfo": {
 "status": "ok",
 "timestamp": 1765128428443,
 "user_tz": 420,
 "elapsed": 1,
 "user": {
 "displayName": "Ryan C",
 "userId": "13217471319505539255"
 }
 }
 },
 "outputs": [],
 "source": "# test_cohere_llm_chain_short = (\n# {\"object\": RunnablePassthrough()}\n# | test_llm_prompt_template\n# | cohere_chat_model\n# )"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "1CasSDdMnF8z"
 },
 "source": "Works! (Note: you may want to review the format of the template. The one we used here is the one from Mistral, and the format may or may not be optimal for Cohere.)\n\nHow can we get the output formatting under control? We can add a String Formatter to the chain:"
 },
 {
 "cell_type": "code",
 "execution_count": 31,
 "metadata": {
 "id": "3rSDhR5AeR7_",
 "executionInfo": {
 "status": "ok",
 "timestamp": 1765128428447,
 "user_tz": 420,
 "elapsed": 1,
 "user": {
 "displayName": "Ryan C",
 "userId": "13217471319505539255"
 }
 }
 },
 "outputs": [],
 "source": "# output_parser = StrOutputParser()\n\n# test_cohere_llm_chain_short_formatted = (\n# {\"object\": RunnablePassthrough()}\n# | test_llm_prompt_template\n# | cohere_chat_model\n# | output_parser\n# )\n\n# test_cohere_llm_chain_short_formatted.invoke('apple')"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "ry1S5W_ueSWt"
 },
 "source": "### 2.6 Setting Up a Simple RAG Chain\n\nFor RAG, we will follow the same approach. Except... you will **later** need to change the chain to include the retrieval step.\n\nWe first do a simple test: create a RAG template that takes a question and a pre-defined context as input, and generates the answer based on the provided context:"
 },
 {
 "cell_type": "code",
 "execution_count": 32,
 "metadata": {
 "id": "qrSx3gFlncAO",
 "executionInfo": {
 "status": "ok",
 "timestamp": 1765128428449,
 "user_tz": 420,
 "elapsed": 1,
 "user": {
 "displayName": "Ryan C",
 "userId": "13217471319505539255"
 }
 }
 },
 "outputs": [],
 "source": "# rag_template = \"\"\"[INST] Answer the question based only on the following context:\n# {context}\n\n# Question: {question}\n# [/INST]\n# \"\"\"\n# rag_prompt_template = ChatPromptTemplate.from_template(rag_template)\n\n# base_rag_chain =(\n# {\"context\": RunnablePassthrough(),\n# \"question\": RunnablePassthrough()}\n# | rag_prompt_template\n# | mistral_llm_lc\n# | output_parser\n# )\n\n# predefined_context = \"Germany has won the World Cup 4 times.\"\n# question = \"How many times did Germany win the world cup?\"\n\n# resp = base_rag_chain.invoke({'context': predefined_context,\n# 'question': question})\n# print(resp)"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "Maq5x1jDhJiX"
 },
 "source": "That's great. But of course, the context needs to be created in an earlier retrieval step. More precisely, the documents will be first retrieved as a list, and then they will need to be formatted into one string to pass to the LLM in the context window.\n\nHere is a simple formatting function that can be hooked into the chain, which combines a list of chunks into one string:"
 },
 {
 "cell_type": "code",
 "execution_count": 33,
 "metadata": {
 "id": "3VUMkGithJtY",
 "executionInfo": {
 "status": "ok",
 "timestamp": 1765128428463,
 "user_tz": 420,
 "elapsed": 12,
 "user": {
 "displayName": "Ryan C",
 "userId": "13217471319505539255"
 }
 }
 },
 "outputs": [],
 "source": "# def format_docs(docs):\n# return \"\\n\\n\".join(doc.page_content for doc in docs)"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "xv0wLvGQhJ5R"
 },
 "source": "So how could we build a simple chain? Let's first just get the retrieval done and the formatted retrieved data and the question inserted into the prompt template:"
 },
 {
 "cell_type": "code",
 "execution_count": 34,
 "metadata": {
 "id": "_UiGHgRLhKEZ",
 "executionInfo": {
 "status": "ok",
 "timestamp": 1765128428465,
 "user_tz": 420,
 "elapsed": 1,
 "user": {
 "displayName": "Ryan C",
 "userId": "13217471319505539255"
 }
 }
 },
 "outputs": [],
 "source": "# rag_template = \"\"\"Here is a context:\\n{context} \\n\\nand here is a question: \\n{question}\"\"\"\n\n# rag_prompt = ChatPromptTemplate.from_template(rag_template)\n\n# rag_chain = (\n# {\"context\": retriever | format_docs,\n# \"question\": RunnablePassthrough()}\n# | rag_prompt\n\n# )"
 },
 {
 "cell_type": "code",
 "execution_count": 35,
 "metadata": {
 "id": "sVRdhLzwjYk6",
 "executionInfo": {
 "status": "ok",
 "timestamp": 1765128428467,
 "user_tz": 420,
 "elapsed": 1,
 "user": {
 "displayName": "Ryan C",
 "userId": "13217471319505539255"
 }
 }
 },
 "outputs": [],
 "source": "# output = rag_chain.invoke('What is Chain of Thought?')"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "m8UynrWikgPc"
 },
 "source": "Ok... with some formatting... this looks good:"
 },
 {
 "cell_type": "code",
 "execution_count": 36,
 "metadata": {
 "id": "UjlKfsrljYnn",
 "executionInfo": {
 "status": "ok",
 "timestamp": 1765128428470,
 "user_tz": 420,
 "elapsed": 2,
 "user": {
 "displayName": "Ryan C",
 "userId": "13217471319505539255"
 }
 }
 },
 "outputs": [],
 "source": "# print(output.messages[0].content)"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "kx0ZWB7Ul4x8"
 },
 "source": "Let's complete the RAG Chain:"
 },
 {
 "cell_type": "code",
 "execution_count": 37,
 "metadata": {
 "id": "Qh9ZopW3jYqA",
 "executionInfo": {
 "status": "ok",
 "timestamp": 1765128428473,
 "user_tz": 420,
 "elapsed": 2,
 "user": {
 "displayName": "Ryan C",
 "userId": "13217471319505539255"
 }
 }
 },
 "outputs": [],
 "source": "# output_parser = StrOutputParser()\n\n# rag_template = \"\"\"[INST]Please answer the question below only based on the context information provided.\\n\\nHere is a context:\\n{context} \\n\\nHere is a question: \\n{question}.[/INST]\"\"\"\n# rag_prompt = ChatPromptTemplate.from_template(rag_template)\n\n# rag_chain = (\n# {\"context\": retriever | format_docs,\n# \"question\": RunnablePassthrough()}\n# | rag_prompt\n# | mistral_llm_lc\n\n# )"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "KfNT72j2mZw-"
 },
 "source": "What about the Cohere models?"
 },
 {
 "cell_type": "code",
 "execution_count": 39,
 "metadata": {
 "id": "JHye-j_MjYvL",
 "executionInfo": {
 "status": "ok",
 "timestamp": 1765128428498,
 "user_tz": 420,
 "elapsed": 1,
 "user": {
 "displayName": "Ryan C",
 "userId": "13217471319505539255"
 }
 }
 },
 "outputs": [],
 "source": "# cohere_rag_chain = (\n# {\"context\": retriever | format_docs,\n# \"question\": RunnablePassthrough()}\n# | rag_prompt\n# | cohere_chat_model\n# | output_parser\n# )"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "gtgVF-T8nN9o"
 },
 "source": "Works too! Time to build the real thing and do experimentation."
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "dM7gS9kGNOJp"
 },
 "source": "## 3. The RAG Model & Experimentation\n\nWith this we can get started. First, we need to acquire the data, chunk it, vectorize it, and store the embeddings (and in this simple case also the docs) in our Qdrant vector db.\n\n### 3.1 The Vector Database\n\nWe will start by creating our datastore, Qdrant. Usually, you would deploy the vector db as a server, but in this case let's simply put everything in memory. Also, in this case we will store not only the embeddings but the whole document in the vector store. We will seed the store with the splits from the blog post we had used before.\n\nWe will also create the retriever, which defines the way the documents are being retrieved. The retriever parameters define for example which method is used, how many docs are retrieved, etc. See [this LangChain link ](https://python.langchain.com/docs/modules/data_connection/retrievers/vectorstore)for more information."
 },
 {
 "cell_type": "code",
 "execution_count": 41,
 "metadata": {
 "id": "aXxjbq6RsKq3",
 "executionInfo": {
 "status": "ok",
 "timestamp": 1765128428503,
 "user_tz": 420,
 "elapsed": 1,
 "user": {
 "displayName": "Ryan C",
 "userId": "13217471319505539255"
 }
 }
 },
 "outputs": [],
 "source": "# qdrant_vectorstore = Qdrant.from_documents(splits,\n# base_embeddings,\n# location=\":memory:\", # Local mode with in-memory storage only\n# collection_name=\"rag_tech_db\",\n# force_recreate=True\n# )\n\n# retriever = qdrant_vectorstore.as_retriever()"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "GC9z0o5ZsLML"
 },
 "source": "### 3.2 Data Acquisition, Chunking, and Vectorization\n\nNow where we have our store we need to get the data into it. We will need to retrieve the data, create the chunks, then vectorize them, and finally store the vectors (along with the docs in this case) in the vector db.\n\nLet us first set chunk size and overlap, as well as the type of splitter. These are starting parameters and you may want to experiment with them:"
 },
 {
 "cell_type": "code",
 "execution_count": 42,
 "metadata": {
 "id": "V_BR4rgYrCGn",
 "executionInfo": {
 "status": "ok",
 "timestamp": 1765128428506,
 "user_tz": 420,
 "elapsed": 2,
 "user": {
 "displayName": "Ryan C",
 "userId": "13217471319505539255"
 }
 }
 },
 "outputs": [],
 "source": "# #Note that these defaults may or may not be ideal!\n# CHUNK_SIZE=128\n# OVERLAP=0\n\n# text_splitter = RecursiveCharacterTextSplitter(chunk_size=CHUNK_SIZE, chunk_overlap=OVERLAP)"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "b-vhTZ_tvIvD"
 },
 "source": "Now let's work with an actual document collection. We will work with four types of documents:\n\n* A few papers from the ArXiv on RAG and NLP\n* A few blogs from Lily Weng that talk about Open Domain Question Answering and related topics\n* A number of Wikipedia articles on that topic\n\nTo make testing easier we'll define a global record number so we can trace back to see which chunk came from which specific document."
 },
 {
 "cell_type": "code",
 "execution_count": 43,
 "metadata": {
 "id": "ZEmdCyqqx5kl",
 "executionInfo": {
 "status": "ok",
 "timestamp": 1765128428517,
 "user_tz": 420,
 "elapsed": 9,
 "user": {
 "displayName": "Ryan C",
 "userId": "13217471319505539255"
 }
 }
 },
 "outputs": [],
 "source": "# #assign a unique number to each document we ingest\n# global_doc_number = 1"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "t_xFjAUSx7ES"
 },
 "source": "First we'll grab some papers from ArXiv. We'll grab the pdf files and get all of the pages as separate documents."
 },
 {
 "cell_type": "code",
 "execution_count": 44,
 "metadata": {
 "id": "s6Jj2vLAyxic",
 "executionInfo": {
 "status": "ok",
 "timestamp": 1765128428520,
 "user_tz": 420,
 "elapsed": 2,
 "user": {
 "displayName": "Ryan C",
 "userId": "13217471319505539255"
 }
 }
 },
 "outputs": [],
 "source": "# arxiv_numbers = ('2005.11401', '2104.07567', '2104.09864', '2105.03011', '2106.09685', '2203.15556', '2203.02155', '2211.09260', '2211.12561',\n# '2212.09741', '2305.14314', '2305.18290', '2306.15595', '2309.08872', '2309.15217', '2310.06825', '2310.11511',\n# '2311.08377', '2312.05708', '2401.06532', '2401.17268', '2402.01306', '2402.19473', '2406.04744',\n# '2312.10997', '2410.12812', '2410.15944', '2404.00657',\n# )"
 },
 {
 "cell_type": "code",
 "execution_count": 45,
 "metadata": {
 "id": "hq8ETxzzygTj",
 "executionInfo": {
 "status": "ok",
 "timestamp": 1765128428532,
 "user_tz": 420,
 "elapsed": 11,
 "user": {
 "displayName": "Ryan C",
 "userId": "13217471319505539255"
 }
 }
 },
 "outputs": [],
 "source": "# all_arxiv_pages = []\n\n# #loop through the papers\n# for identifier in arxiv_numbers:\n# # Construct URL using the arXiv unique identifier\n# arx_url = f\"https://arxiv.org/pdf/{identifier}.pdf\"\n\n# # Extract pages from the document and add them to the list of pages\n# arx_loader = PyMuPDFLoader(arx_url)\n# arx_pages = arx_loader.load()\n# for page_num in range(len(arx_pages)):\n# page = arx_pages[page_num]\n# #CHANGED\n# page.metadata['page_num'] = page_num\n# page.metadata['doc_num'] = global_doc_number\n# page.metadata['doc_source'] = \"ArXiv\"\n# all_arxiv_pages.append(page)\n\n# global_doc_number += 1"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "iYMkGJ7igvg_"
 },
 "source": "How many docs did we get? Is that the correct number? And what is the content?"
 },
 {
 "cell_type": "code",
 "execution_count": 46,
 "metadata": {
 "id": "VXNS5MaMwrOK",
 "executionInfo": {
 "status": "ok",
 "timestamp": 1765128428544,
 "user_tz": 420,
 "elapsed": 1,
 "user": {
 "displayName": "Ryan C",
 "userId": "13217471319505539255"
 }
 }
 },
 "outputs": [],
 "source": "# num_pages = len(all_arxiv_pages)\n# num_docs = global_doc_number - 1\n\n# print(f\"{num_docs} documents in total\")\n# print(f\"{num_pages} pages in total\")"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "Czx-h_Aeg_NM"
 },
 "source": "Now we need to split the docs into chunks. LangChain provides a couple of ways to do that. We'll use for now the `RecursiveCharacterTextSplitter`."
 },
 {
 "cell_type": "code",
 "execution_count": 48,
 "metadata": {
 "id": "8Ln3nmeVvLBI",
 "executionInfo": {
 "status": "ok",
 "timestamp": 1765128428563,
 "user_tz": 420,
 "elapsed": 1,
 "user": {
 "displayName": "Ryan C",
 "userId": "13217471319505539255"
 }
 }
 },
 "outputs": [],
 "source": "# #index doc chunks\n# splits = text_splitter.split_documents(all_arxiv_pages)\n# for idx, text in enumerate(splits):\n# splits[idx].metadata['split_id'] = idx\n\n# print('Number of splits/chunks: ', len(splits))"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "u6DUxHWuozJB"
 },
 "source": "Let's add the vectors to the datastore and see whether we can retrieve a nearest neighbor to a query. Let's look at the second closest match:"
 },
 {
 "cell_type": "code",
 "execution_count": 50,
 "metadata": {
 "id": "KSJmDkj6SvQQ",
 "executionInfo": {
 "status": "ok",
 "timestamp": 1765128428618,
 "user_tz": 420,
 "elapsed": 6,
 "user": {
 "displayName": "Ryan C",
 "userId": "13217471319505539255"
 }
 }
 },
 "outputs": [],
 "source": "# %%capture\n\n# qdrant_vectorstore.add_documents(documents=splits)"
 },
 {
 "cell_type": "code",
 "execution_count": 51,
 "metadata": {
 "id": "4xOhcgKd5ckk",
 "executionInfo": {
 "status": "ok",
 "timestamp": 1765128428643,
 "user_tz": 420,
 "elapsed": 15,
 "user": {
 "displayName": "Ryan C",
 "userId": "13217471319505539255"
 }
 }
 },
 "outputs": [],
 "source": "# query = \"How can we train a model for preferences?\"\n# found_docs = qdrant_vectorstore.similarity_search_with_score(query)"
 },
 {
 "cell_type": "code",
 "execution_count": 52,
 "metadata": {
 "id": "QDelZRCF5Ite",
 "executionInfo": {
 "status": "ok",
 "timestamp": 1765128428669,
 "user_tz": 420,
 "elapsed": 19,
 "user": {
 "displayName": "Ryan C",
 "userId": "13217471319505539255"
 }
 }
 },
 "outputs": [],
 "source": "# print(found_docs[0][0].page_content)\n# print(found_docs[0][1])"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "ipT82FOghpr6"
 },
 "source": "Next, let's get some information from Wikipedia on our main topic -- Gen AI. LangChain provides a DocumentLoader that accesses the Wikipedia API."
 },
 {
 "cell_type": "code",
 "execution_count": 53,
 "metadata": {
 "id": "pfWbJmg0vKfv",
 "executionInfo": {
 "status": "ok",
 "timestamp": 1765128428707,
 "user_tz": 420,
 "elapsed": 37,
 "user": {
 "displayName": "Ryan C",
 "userId": "13217471319505539255"
 }
 }
 },
 "outputs": [],
 "source": "# wiki_docs = WikipediaLoader(query=\"Generative Artificial Intelligence\", load_max_docs=4).load()\n# for idx, text in enumerate(wiki_docs):\n# wiki_docs[idx].metadata['doc_num'] = global_doc_number\n# wiki_docs[idx].metadata['doc_source'] = \"Wikipedia\"\n\n# global_doc_number += 1\n\n# print('Number of documents: ', len(wiki_docs))\n\n# #index docs\n# wiki_splits = text_splitter.split_documents(wiki_docs)\n# for idx, text in enumerate(wiki_splits):\n# wiki_splits[idx].metadata['split_id'] = idx\n\n# print('Number of splits/chunks: ', len(wiki_splits))"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "FWD9eytoc7er"
 },
 "source": "Now we'll add these splits to the vector stores."
 },
 {
 "cell_type": "code",
 "execution_count": 54,
 "metadata": {
 "id": "VC2LSdGwIcvn",
 "executionInfo": {
 "status": "ok",
 "timestamp": 1765128428710,
 "user_tz": 420,
 "elapsed": 1,
 "user": {
 "displayName": "Ryan C",
 "userId": "13217471319505539255"
 }
 }
 },
 "outputs": [],
 "source": "# %%capture\n\n# qdrant_vectorstore.add_documents(documents=wiki_splits)"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "HsZSNcp4TWxO"
 },
 "source": "Same with a couple of other queries:"
 },
 {
 "cell_type": "code",
 "execution_count": 55,
 "metadata": {
 "id": "qlO0AYTmTaPM",
 "executionInfo": {
 "status": "ok",
 "timestamp": 1765128428712,
 "user_tz": 420,
 "elapsed": 1,
 "user": {
 "displayName": "Ryan C",
 "userId": "13217471319505539255"
 }
 }
 },
 "outputs": [],
 "source": "# wiki_docs = WikipediaLoader(query=\"Information Retrieval\", load_max_docs=4).load()\n# for idx, text in enumerate(wiki_docs):\n# wiki_docs[idx].metadata['doc_num'] = global_doc_number\n# wiki_docs[idx].metadata['doc_source'] = \"Wikipedia\"\n\n# global_doc_number += 1\n\n# print('Number of documents: ', len(wiki_docs))\n\n# #index docs\n# wiki_splits = text_splitter.split_documents(wiki_docs)\n# for idx, text in enumerate(wiki_splits):\n# wiki_splits[idx].metadata['split_id'] = idx\n\n# print('Number of splits/chunks: ', len(wiki_splits))"
 },
 {
 "cell_type": "code",
 "execution_count": 56,
 "metadata": {
 "id": "yQqX-w8tTjah",
 "executionInfo": {
 "status": "ok",
 "timestamp": 1765128428714,
 "user_tz": 420,
 "elapsed": 1,
 "user": {
 "displayName": "Ryan C",
 "userId": "13217471319505539255"
 }
 }
 },
 "outputs": [],
 "source": "# %%capture\n\n# qdrant_vectorstore.add_documents(documents=wiki_splits)"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "PtU-BeWpTkbv"
 },
 "source": "And yet another related Wikipedia article."
 },
 {
 "cell_type": "code",
 "execution_count": 57,
 "metadata": {
 "id": "J_uwyvZaTkx4",
 "executionInfo": {
 "status": "ok",
 "timestamp": 1765128428716,
 "user_tz": 420,
 "elapsed": 1,
 "user": {
 "displayName": "Ryan C",
 "userId": "13217471319505539255"
 }
 }
 },
 "outputs": [],
 "source": "# wiki_docs = WikipediaLoader(query=\"Large Language Models\", load_max_docs=4).load()\n# for idx, text in enumerate(wiki_docs):\n# wiki_docs[idx].metadata['doc_num'] = global_doc_number\n# wiki_docs[idx].metadata['doc_source'] = \"Wikipedia\"\n\n# global_doc_number += 1\n\n# print('Number of documents: ', len(wiki_docs))\n\n# #index docs\n# wiki_splits = text_splitter.split_documents(wiki_docs)\n# for idx, text in enumerate(wiki_splits):\n# wiki_splits[idx].metadata['split_id'] = idx\n\n# print('Number of splits/chunks: ', len(wiki_splits))"
 },
 {
 "cell_type": "code",
 "execution_count": 58,
 "metadata": {
 "id": "iw-FQnNZTk7I",
 "executionInfo": {
 "status": "ok",
 "timestamp": 1765128428718,
 "user_tz": 420,
 "elapsed": 1,
 "user": {
 "displayName": "Ryan C",
 "userId": "13217471319505539255"
 }
 }
 },
 "outputs": [],
 "source": "# %%capture\n\n# qdrant_vectorstore.add_documents(documents=wiki_splits)"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "8GedFRyK1p6E"
 },
 "source": "And finally another related Wikipedia article."
 },
 {
 "cell_type": "code",
 "execution_count": 59,
 "metadata": {
 "id": "IrZslvv01p6H",
 "executionInfo": {
 "status": "ok",
 "timestamp": 1765128428720,
 "user_tz": 420,
 "elapsed": 1,
 "user": {
 "displayName": "Ryan C",
 "userId": "13217471319505539255"
 }
 }
 },
 "outputs": [],
 "source": "# wiki_docs = WikipediaLoader(query=\"Retrieval Augmented Generation\", load_max_docs=4).load()\n# for idx, text in enumerate(wiki_docs):\n# wiki_docs[idx].metadata['doc_num'] = global_doc_number\n# wiki_docs[idx].metadata['doc_source'] = \"Wikipedia\"\n\n# global_doc_number += 1\n\n# print('Number of documents: ', len(wiki_docs))\n\n# #index docs\n# wiki_splits = text_splitter.split_documents(wiki_docs)\n# for idx, text in enumerate(wiki_splits):\n# wiki_splits[idx].metadata['split_id'] = idx\n\n# print('Number of splits/chunks: ', len(wiki_splits))"
 },
 {
 "cell_type": "code",
 "execution_count": 60,
 "metadata": {
 "id": "k_o3PCEa1p6I",
 "executionInfo": {
 "status": "ok",
 "timestamp": 1765128428732,
 "user_tz": 420,
 "elapsed": 11,
 "user": {
 "displayName": "Ryan C",
 "userId": "13217471319505539255"
 }
 }
 },
 "outputs": [],
 "source": "# %%capture\n\n# qdrant_vectorstore.add_documents(documents=wiki_splits)"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "Hfs8ro_Ziekd"
 },
 "source": "We'll also augment our collection with some blog entries about Open Domain Question Answering, of which RAG is an approach, and some related topics in case users want to ask how the new Search system works."
 },
 {
 "cell_type": "code",
 "execution_count": 61,
 "metadata": {
 "id": "K5LdggJw8nBF",
 "executionInfo": {
 "status": "ok",
 "timestamp": 1765128428830,
 "user_tz": 420,
 "elapsed": 97,
 "user": {
 "displayName": "Ryan C",
 "userId": "13217471319505539255"
 }
 }
 },
 "outputs": [],
 "source": "# web_loader = WebBaseLoader(\n# web_paths=(\"https://lilianweng.github.io/posts/2020-10-29-odqa/\",\n# \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",\n# \"https://lilianweng.github.io/posts/2018-06-24-attention/\",\n# \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n# \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\"),\n# \"https://lilianweng.github.io/posts/2024-07-07-hallucination/\"\n\n# bs_kwargs=dict(\n# parse_only=bs4.SoupStrainer(\n# class_=(\"post-content\", \"post-title\", \"post-header\")\n# )\n# ),\n# )\n\n# web_documents = web_loader.load()\n\n# for idx, text in enumerate(web_documents):\n# web_documents[idx].metadata['doc_num'] = global_doc_number\n# web_documents[idx].metadata['doc_source'] = \"WWW\"\n# global_doc_number += 1\n\n# print('Number of documents: ', len(web_documents))"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "wMjLDnHf8nBG"
 },
 "source": "Again, we will split the retrieved data into chunks and add the data to the vector store:"
 },
 {
 "cell_type": "code",
 "execution_count": 62,
 "metadata": {
 "id": "D29aVbKn8nBH",
 "executionInfo": {
 "status": "ok",
 "timestamp": 1765128428833,
 "user_tz": 420,
 "elapsed": 1,
 "user": {
 "displayName": "Ryan C",
 "userId": "13217471319505539255"
 }
 }
 },
 "outputs": [],
 "source": "# web_splits = text_splitter.split_documents(web_documents)\n\n# for idx, text in enumerate(web_splits):\n# web_splits[idx].metadata['split_id'] = idx\n\n# print('Number of splits: ', len(web_splits))"
 },
 {
 "cell_type": "code",
 "execution_count": 63,
 "metadata": {
 "id": "R5slq33qt6Dc",
 "executionInfo": {
 "status": "ok",
 "timestamp": 1765128428836,
 "user_tz": 420,
 "elapsed": 1,
 "user": {
 "displayName": "Ryan C",
 "userId": "13217471319505539255"
 }
 }
 },
 "outputs": [],
 "source": "# %%capture\n\n# qdrant_vectorstore.add_documents(documents=web_splits)"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "OLghy_LyXME5"
 },
 "source": "# project Start"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "4KVUbsDa3BBF"
 },
 "source": "### 3.3 The Test Data\n\nYou will want to test the system that you (will) have built. Below we give you a validation set that you could take as labeled data (imagine, your user personas would have had these questions and deemed the answers to be good). We also will give you a test set that only contains questions. (This is the set that we will use to get a feel for how well your RAG system corresponds to our Gold model).\n\nHere are is the gold validation set and the test questions. **DO NOT CHANGE OR DELETE!!**"
 },
 {
 "cell_type": "code",
 "execution_count": 64,
 "metadata": {
 "id": "mxMnyWig7R-T",
 "executionInfo": {
 "status": "ok",
 "timestamp": 1765128428839,
 "user_tz": 420,
 "elapsed": 2,
 "user": {
 "displayName": "Ryan C",
 "userId": "13217471319505539255"
 }
 }
 },
 "outputs": [],
 "source": "validation_questions_answers = {\n 0: {\n \"question\": \"What defines a large language model in the context of natural language processing tasks?\",\n \"gold_answer_research\": \"A large language model in the context of natural language processing tasks is characterized by its ability to achieve general-purpose language generation and other NLP tasks through self-supervised and semi-supervised training on large datasets. These models typically utilize feedforward neural networks and transformers, surpassing earlier models like recurrent neural networks and word n-gram language models. They are often pre-trained on vast amounts of text data from the internet and can be fine-tuned for specific downstream tasks such as summarization, machine reading comprehension, and natural language to SQL translation.\",\n \"gold_answer_marketing\": \"A large language model (LLM) is a language model notable for its ability to achieve general-purpose language generation and other natural language processing tasks such as classification. LLMs acquire these abilities by learning statistical relationships from text documents during a computationally intensive self-supervised and semi-supervised training process.\"\n },\n 1: {\n \"question\": \"How do large language models like GPT-3 become capable of text generation?\",\n \"gold_answer_research\": \"Large language models like GPT-3 become capable of text generation through a process of pre-training on vast amounts of unlabelled text data, where they learn statistical relationships and patterns in language. This pre-training process involves training the model to predict the next word in a sequence, allowing it to understand and generate human-like text. Additionally, fine-tuning on specific tasks or datasets further enhances the model's ability to generate text that is contextually relevant and coherent. By combining these pre-training and fine-tuning techniques, GPT-3 can generate novel and diverse text outputs across a wide range of natural language processing tasks.\",\n \"gold_answer_marketing\": \"Large language models like GPT-3 become capable of text generation by being pre-trained on large data sets of unlabelled text, learning statistical relationships from text documents during a computationally intensive self-supervised and semi-supervised training process. This allows them to generate novel human-like content based on the patterns and relationships they have learned.\"\n },\n 2: {\n \"question\": \"What are some of the architectures used in building artificial neural networks for LLMs?\",\n \"gold_answer_research\": \"Some common architectures used in building artificial neural networks for LLMs include decoder-only transformer-based architectures, recurrent neural network variants, and Mamba (a state space model). MRKL (Modular Reasoning, Knowledge and Language) is another neuro-symbolic architecture that utilizes neural and symbolic expert modules. Additionally, the unified architecture proposed by Ronan Collobert and Jason Weston combines deep neural networks with multitask learning for natural language processing.\",\n \"gold_answer_marketing\": \"The architectures used in building artificial neural networks for LLMs include decoder-only transformer-based architecture, recurrent neural network variants, and Mamba (a state space model).\"\n },\n 3: {\n \"question\": \"Can you name some notable large language models and their respective creators or companies?\",\n \"gold_answer_research\": \"Some notable large language models include Mistral 7B developed by a team of engineers including Albert Q. Jiang and Guillaume Lample, as well as Chinchilla developed by the research team at DeepMind. Additionally, The Pile dataset was created by Leo Gao, Stella Biderman, and others, while the Realm model was developed by Kelvin Guu and his team. These models have been recognized for their superior performance and efficiency in language generation and natural language processing tasks.\",\n \"gold_answer_marketing\": \"Some notable large language models include Mistral 7B by a team of researchers, Chinchilla by DeepMind, and GPT-3 by OpenAI.\"\n },\n 7: {\n \"question\": \"What licensing terms are associated with source-available models like Mistral AI's language models?\",\n \"gold_answer_research\": \"Source-available models like Mistral AI's language models, including Mistral 7B, are released under the Apache 2.0 license. This license allows for more permissive use and modification of the models, providing flexibility for users to adapt the models to their specific needs. Additionally, the release of Mistral 7B includes a reference implementation for easy deployment on various cloud platforms and integration with tools like the vLLM inference server and Hugging Face for streamlined usage. This licensing approach aims to facilitate widespread adoption and adaptation of the models for different tasks and applications.\",\n \"gold_answer_marketing\": \"Source-available models like Mistral AI's language models are released under the Apache 2.0 license.\"\n },\n 8: {\n \"question\": \"What are the main applications of language models?\",\n \"gold_answer_research\": \"Language models have a wide range of applications, including speech recognition, machine translation, natural language generation, optical character recognition, handwriting recognition, grammar induction, and information retrieval. Large language models, which are currently the most advanced form, are created using larger datasets, often sourced from the public internet, and feedforward neural networks. These models can be used for tasks such as text generation, question-answering, creative writing, dialogue, and classification.\",\n \"gold_answer_marketing\": \"The main applications of language models include speech recognition, machine translation, natural language generation, optical character recognition, handwriting recognition, grammar induction, and information retrieval.\"\n },\n 9: {\n \"question\": \"Who proposed the first significant statistical language model?\",\n \"gold_answer_research\": \"The first significant statistical language model was proposed in 1980. This model was a probabilistic model of natural language. IBM conducted 'Shannon-style' experiments during the 1980s to identify potential sources for language modeling improvement by observing and analyzing the performance of human subjects in predicting or correcting text.\",\n \"gold_answer_marketing\": \"The first significant statistical language model was proposed by IBM in 1980.\"\n },\n 11: {\n \"question\": \"Which components have allowed large language models to surpass their predecessors?\",\n \"gold_answer_research\": \"Large language models have surpassed their predecessors due to their use of larger datasets, feedforward neural networks, and transformers. These components have enabled these models to outperform recurrent neural network-based models and traditional statistical models like word n-gram language models. The combination of these elements has allowed for the creation of more adaptable, efficient, and high-performing language models that can be applied to a wide range of tasks in the field of natural language processing.\",\n \"gold_answer_marketing\": \"The combination of larger datasets, feedforward neural networks, and transformers.\"\n },\n 12: {\n \"question\": \"What is a common strategy used by language models to address the curse of dimensionality?\",\n \"gold_answer_research\": \"One common strategy used by language models to address the curse of dimensionality is the use of continuous space embeddings produced by recurrent neural network-based models. These embeddings represent words as non-linear combinations of weights, which helps alleviate the data sparsity issue caused by the exponential increase in possible word sequences with the size of the vocabulary. This approach allows language models to efficiently capture the relationships between words in a lower-dimensional space, reducing the impact of the curse of dimensionality.\",\n \"gold_answer_marketing\": \"Continuous space embeddings produced in recurrent neural network-based language models are a common strategy used to address the curse of dimensionality.\"\n },\n 13: {\n \"question\": \"Why might large language models not be considered plausible cognitive models?\",\n \"gold_answer_research\": \"Large language models may not be considered plausible cognitive models because they sometimes learn patterns that humans do not learn, and fail to learn patterns that humans typically do learn. This discrepancy in learning capabilities between large language models, particularly recurrent neural networks, and humans raises questions about the true cognitive abilities of these models. Additionally, the potential for misuse of these models, such as generating misinformation or abusive content, highlights the need for caution when deploying them in various applications. Furthermore, the pre-defined context window size in LLMs and the potential for hallucination of data also contribute to the concerns about their cognitive modeling capabilities.\",\n \"gold_answer_marketing\": \"Large language models sometimes learn patterns that humans do not learn and fail to learn patterns that humans typically learn, making them not considered plausible cognitive models.\"\n },\n 16: {\n \"question\": \"What is the purpose of the constitution in training AI systems?\",\n \"gold_answer_research\": \"The purpose of the constitution in training AI systems, such as in the Constitutional AI approach, is to provide a set of guiding principles for the AI model to follow during the supervised learning phase. This constitution helps the model generate responses to prompts, self-critique these responses, and revise them accordingly. Additionally, the constitution serves as a framework for the reinforcement learning phase, where the AI evaluates responses based on these principles, ultimately training the AI to be harmless and helpful without extensive human feedback.\",\n \"gold_answer_marketing\": \"The purpose of the constitution in training AI systems is to guide the model to generate responses that align with a set of guiding principles, ensuring that the AI is harmless and helpful without extensive human feedback.\"\n },\n 17: {\n \"question\": \"What is the meaning of the term alignment tax in the context of AI development?\",\n \"gold_answer_research\": \"In the context of AI development, the term alignment tax refers to the additional cost incurred in ensuring that an AI system is aligned with human intent. This cost arises from the need to mitigate performance degradations introduced by fine-tuning the model to achieve alignment. A high alignment tax could deter the adoption of alignment techniques, as it may impact usability and performance. Therefore, there is a growing need for alignment techniques that have a low alignment tax to encourage the development of highly capable AI systems aligned with human intent.\",\n \"gold_answer_marketing\": \"The term 'alignment tax' in the context of AI development refers to the additional cost incurred to ensure that an AI system is aligned with human intent and ethical considerations. This cost can arise from mitigating performance degradations introduced by fine-tuning the AI model.\"\n },\n 18: {\n \"question\": \"How does the release of successive models in a language model series typically improve functionality?\",\n \"gold_answer_research\": \"The release of successive models in a language model series typically improves functionality by incorporating larger datasets, allowing for more comprehensive training and better understanding of syntax, semantics, and ontology in human language corpora. Additionally, newer models often address inaccuracies and biases present in earlier versions, leading to more accurate and reliable results. Furthermore, advancements in technology and research techniques contribute to the development of more efficient and high-performing language models, enabling them to be used in a wider range of tasks effectively.\",\n \"gold_answer_marketing\": \"The release of successive models in a language model series typically improves functionality by enhancing adaptability, performance, and efficiency through advancements in training methods and model architecture.\"\n },\n 19: {\n \"question\": \"What is the significant enhancement in Claude 2.1 compared to its previous version?\",\n \"gold_answer_research\": \"The significant enhancement in Claude 2.1 compared to its previous version Claude 2 is the expanded context window, which has been increased from 100,000 tokens to 200,000 tokens. This allows Claude 2.1 to process and analyze larger amounts of text, enabling it to provide more comprehensive summaries and assistance with tasks. Additionally, Claude 2.1 has improved its performance in handling complex queries and requests, showcasing advancements in its capabilities for natural language processing and understanding.\",\n \"gold_answer_marketing\": \"The significant enhancement in Claude 2.1 compared to its previous version is the expansion of the context window from 9,000 tokens to 100,000 tokens, as well as the ability to upload PDFs and other documents for reading, summarizing, and task assistance.\"\n },\n 20: {\n \"question\": \"In what way can a language model demonstrate meta-cognitive reasoning capabilities?\",\n \"gold_answer_research\": \"A language model can demonstrate meta-cognitive reasoning capabilities by realizing it is being artificially tested during needle-in-a-haystack evaluations, as shown by Claude 3. This ability allows the model to understand the context of the evaluation and adjust its responses accordingly. Additionally, fine-tuning a pre-trained model to answer questions without external context, like in the study by Roberts et al. (2020), can also showcase the meta-cognitive reasoning abilities of the language model. This fine-tuning process forces the model to rely on internalized knowledge acquired during pre-training, demonstrating its capacity for self-awareness and adaptive reasoning.\",\n \"gold_answer_marketing\": \"A language model can demonstrate meta-cognitive reasoning capabilities by realizing it is being artificially tested during evaluations such as needle in a haystack tasks.\"\n },\n 22: {\n \"question\": \"How can a language model's ability to analyze images expand its range of applications?\",\n \"gold_answer_research\": \"One way a language model's ability to analyze images can expand its range of applications is by enabling it to perform tasks that require both textual and visual information, such as image captioning or visual question answering. By incorporating image features into the model's input, it can generate more contextually relevant and accurate responses. This integration of image analysis can also enhance the model's performance in tasks like content generation, recommendation systems, and sentiment analysis, where visual cues play a significant role in understanding and interpreting the data. Additionally, combining language and image processing capabilities can lead to more sophisticated and versatile AI systems that can handle a wider range of real-world applications effectively.\",\n \"gold_answer_marketing\": \"By incorporating image analysis capabilities, a language model can be used for tasks such as image captioning, visual question answering, and text-to-image generation, expanding its range of applications beyond just text-based tasks.\"\n },\n 23: {\n \"question\": \"What are some ethical considerations that come into play when refining the performance of language models?\",\n \"gold_answer_research\": \"Some ethical considerations when refining the performance of language models include preventing biased outputs, ensuring privacy of data, avoiding generation of misinformation, and not promoting harmful activities. It is important to evaluate model outputs based on criteria such as not generating abusive or offensive language, not providing bad advice or promoting illegal activities, and not causing harm to individuals or the environment. Trade-offs may need to be made between these criteria depending on the task at hand. Additionally, alignment techniques should be used as part of a broader safety ecosystem to address safety issues associated with large language models.\",\n \"gold_answer_marketing\": \"Ethical considerations when refining language models include avoiding biased outputs, protecting privacy, preventing misinformation, and ensuring outputs do not cause harm or promote illegal activity. Trade-offs may need to be made in evaluating model outputs based on these criteria.\"\n },\n 24: {\n \"question\": \"Who developed the language model family known as Chinchilla?\",\n \"gold_answer_research\": \"The Chinchilla language model family was developed by the research team at DeepMind and presented in March 2022. It is named 'Chinchilla' as an advancement over the previous Gopher model family. The Chinchilla family has been trained to investigate the scaling laws of large language models and is designed to outperform GPT-3.\",\n \"gold_answer_marketing\": \"The research team at DeepMind developed the language model family known as Chinchilla.\"\n },\n 25: {\n\t\"question\": \"What benchmark did Chinchilla achieve an average accuracy of 67.5% on?\",\n\t\"gold_answer_research\": \"Chinchilla achieved an average accuracy of 67.5% on the MMLU benchmark (Measuring Massive Multitask Language Understanding).\",\n\t\"gold_answer_marketing\": \"Chinchilla achieved an average accuracy of 67.5% on the MMLU benchmark (Measuring Massive Multitask Language Understanding).\"\n },\n 27: {\n \"question\": \"What is the significance of the accuracy percentage achieved by Chinchilla on the MMLU benchmark?\",\n \"gold_answer_research\": \"The significance of the accuracy percentage achieved by Chinchilla on the MMLU benchmark is that it is 67.5%, which is 7% higher than Gopher's performance. Chinchilla's performance on the benchmark is a positive indicator of its effectiveness in developing an effective training paradigm for large autoregressive language models with limited compute resources. The higher accuracy suggests that Chinchilla may have better performance on downstream tasks compared to Gopher. Additionally, Chinchilla was still in the testing phase as of January 12, 2023, indicating ongoing development and potential for further improvement in accuracy.\",\n \"gold_answer_marketing\": \"The significance of the accuracy percentage achieved by Chinchilla on the MMLU benchmark is 67.5%, which is 7% higher than Gopher's performance.\"\n },\n 28: {\n \"question\": \"Why is Chinchilla considered more efficient in terms of computing power for inference and fine-tuning?\",\n \"gold_answer_research\": \"Chinchilla is considered more efficient in terms of computing power for inference and fine-tuning because it has 70B parameters and four times as much data, allowing for high performance with limited compute resources. Additionally, Chinchilla's training paradigm recommends doubling the number of training tokens for every model size doubling, leading to better results on downstream tasks. Moreover, Chinchilla's smooth and differentiable model architecture contributes to its efficiency in training and inference processes.\",\n \"gold_answer_marketing\": \"Chinchilla is considered more efficient in terms of computing power for inference and fine-tuning because it has 70B parameters and four times as much data, which allows for high performance with limited compute resources.\"\n },\n 30: {\n \"question\": \"What is the recommended strategy for training large autoregressive language models with limited compute resources, as contributed by the Chinchilla team?\",\n \"gold_answer_research\": \"The Chinchilla team recommends that the number of training tokens should be doubled for every model size doubling to achieve better results on downstream tasks. They also suggest using larger, higher-quality training datasets to improve performance. Additionally, they mention the importance of balancing model size and efficiency to address computational costs and inference latency limitations. It is advised to focus on Transformer language models and consider sharing model parameters for quick task-switching when deploying as a service.\",\n \"gold_answer_marketing\": \"The Chinchilla team recommends doubling the number of training tokens for every model size doubling and using larger, higher-quality training datasets to achieve better results on downstream tasks.\"\n },\n 33: {\n \"question\": \"What assumptions must be met in order for the reparameterization of reward functions to be applied within the context of Plackett-Luce and Bradley-Terry models?\",\n \"gold_answer_research\": \"In order for the reparameterization of reward functions to be applied within the context of Plackett-Luce and Bradley-Terry models, the assumptions must include the existence of a reward function r(x, y) that induces an optimal model \u03c0r(y | x) as specified by Equation 4. Additionally, the reward function must be from an equivalence class of reward functions that differ only in an input-specific component. It is also important to ensure that the value distribution, representing human utility, is affected by input-specific changes to maximize preference. Finally, the assumptions should allow for the cancellation of the normalization constant Z(x) in the derived expressions.\",\n \"gold_answer_marketing\": \"The assumptions that must be met for the reparameterization of reward functions to be applied within the context of Plackett-Luce and Bradley-Terry models are that the reward classes consistent with these models can be represented by the reparameterization formula r(x, y) = \u03b2 log(\u03c0(y|x) / \u03c0ref(y|x)).\"\n },\n 34: {\n\t\"question\": \"What are some of the limitations of traditional position encoding methods in the architecture of pre-trained language models (PLMs), and what novel approach does the paper propose to address these issues?\",\n\t\"gold_answer_research\": \"One limitation of traditional position encoding methods in PLMs is that they may not enable length extrapolation of pre-existing models, leading to the need for substantial pre-training costs. The paper proposes a novel approach called Position Interpolation, which extends existing PLMs without deviating far from existing definitions of position encoding or attention mechanisms. This method allows for much extended context windows for text modeling, leading to significant perplexity gains and improved model performance.\",\n\t\"gold_answer_marketing\": \"Traditional position encoding methods in PLMs have limitations in enabling length extrapolation and adapting to extended context windows. The paper proposes a novel approach called Position Interpolation, which generates strong models that can effectively make use of much extended context windows. This method allows for substantial pre-training cost savings and preserves the quality of the original models, even for small context window tasks.\"\n },\n 35: {\n\t\"question\": \"How does the Rotary Position Embedding (RoPE) approach in Transformers differ from the traditional additive method of position embedding with respect to encoding position information?\",\n\t\"gold_answer_research\": \"The RoPE approach in Transformers differs from the traditional additive method of position embedding by being multiplicative instead of additive. While traditional methods add position encoding to context representations, RoPE incorporates relative position information through rotation matrix product. This means that RoPE naturally includes relative position dependency in the self-attention formulation, without altering terms in the expanded formulation like the additive method does. Additionally, RoPE's properties show that it decays as the relative distance between positions increases, providing a clear theoretical interpretation of how position information is encoded.\",\n\t\"gold_answer_marketing\": \"The RoPE approach in Transformers differs from the traditional additive method of position embedding by incorporating relative position information through rotation matrix product instead of altering terms in the expanded formulation of additive position encoding.\"\n },\n 36: {\n \"question\": \"What approaches or methods are suggested for improving the alignment of language models with human preferences?\",\n \"gold_answer_research\": \"One approach suggested for improving the alignment of language models with human preferences is Direct Preference Optimization (DPO), which optimizes language models to adhere to human preferences without explicit reward modeling or reinforcement learning. This algorithm implicitly optimizes the same objective as existing RLHF algorithms, making it simple to implement and straightforward to train. Additionally, incorporating pretraining data into RLHF fine-tuning can help mitigate the alignment tax and improve the alignment of language models with human preferences.\",\n \"gold_answer_marketing\": \"Direct Preference Optimization (DPO) is a suggested approach for improving the alignment of language models with human preferences. It optimizes the same objective as existing reinforcement learning with a KL-divergence constraint but is simpler to implement and train.\"\n },\n 38: {\n \"question\": \"What methods have been explored to improve the alignment of language models with user preferences or desired outputs?\",\n \"gold_answer_research\": \"Several methods have been explored to improve the alignment of language models with user preferences or desired outputs. These include fine-tuning language models from human preferences, using reinforcement learning from human feedback, and minimizing f-divergence to align language models with preferences. Additionally, incorporating pretraining data into reinforcement learning from human feedback (RLHF) has shown promise in mitigating alignment issues. It is important to consider factors such as the underlying model, training data, fine-tuning data, and alignment methods when working towards better alignment with user intentions.\",\n \"gold_answer_marketing\": \"Methods explored to improve the alignment of language models with user preferences or desired outputs include RLHF, DPO, and f-divergence minimization.\"\n },\n 39: {\n \"question\": \"How does the uniqueness of a reparameterized reward function within an equivalence class impact the selection of optimal policies in constrained reinforcement learning problems?\",\n \"gold_answer_research\": \"The uniqueness of a reparameterized reward function within an equivalence class ensures that different reward functions from the same class will induce the same optimal policy in constrained reinforcement learning problems. This means that despite the under-specification issue, the optimal policy remains consistent across equivalent reward functions, allowing for more stable and reliable policy learning. By reparameterizing the reward function, the selection of optimal policies becomes more straightforward and consistent, leading to better convergence and performance in constrained reinforcement learning scenarios.\",\n \"gold_answer_marketing\": \"The uniqueness of a reparameterized reward function within an equivalence class ensures that different reward functions from the same class will lead to the selection of the same optimal policy in constrained reinforcement learning problems.\"\n },\n 41: {\n\t\"question\": \"Question: When conducting demographic and technical assessments of teams or research subjects, what types of data categories are typically collected and analyzed to ensure a comprehensive understanding of the group's composition and the methods used?\",\n\t\"gold_answer_research\": \"When conducting demographic and technical assessments of teams or research subjects, it is important to collect and analyze data categories such as age, gender, education level, professional background, and expertise in specific areas. By gathering information on these categories, you can ensure a comprehensive understanding of the group's composition and the methods used in your assessments. Additionally, it may be helpful to consider factors like cultural background, language proficiency, and geographical location to capture a more nuanced picture of the group being assessed. This detailed approach to data collection and analysis can provide valuable insights for making informed decisions and recommendations based on the gathered information.\",\n\t\"gold_answer_marketing\": \"Answer: Demographic data such as age, gender, education level, and technical data related to skills and experience are typically collected and analyzed for comprehensive understanding.\"\n },\n 43: {\n \"question\": \"How does the evaluation process determine the level of alignment in the models being tested?\",\n \"gold_answer_research\": \"The evaluation process determines the level of alignment in the models by comparing the system-level judgments made by the language model (GPT-4) with those made by human annotators, using metrics such as Kendall Tau and Spearman rank correlation. At the example level, the agreement between the model and human annotators is measured using Fleiss \u03ba. These metrics provide insights into the alignment of the language model with human intentions and indicate the reliability of model-based evaluation as an alternative to human evaluation.\",\n \"gold_answer_marketing\": \"The evaluation process determines the level of alignment in the models being tested by comparing system-level judgments by the language model and human annotators, as well as examining agreement at the example level.\"\n },\n 44: {\n \"question\": \"What approaches can be used to improve the performance of an AI model on various datasets, and how do they compare in terms of effectiveness?\",\n \"gold_answer_research\": \"To improve the performance of an AI model on various datasets, you can consider combining your method with ways to filter the pretraining data or training the initial pretrained models using human data. Additionally, you could explore methods that improve models' truthfulness, such as WebGPT. Comparing the effectiveness of these approaches, it may be beneficial to experiment with different adaptation approaches in the low-data regime and evaluate their performance on tasks like MNLI-n. Fine-tuning models using different training datasets and varying the number of fine-tuning steps can also impact performance on public NLP datasets.\",\n \"gold_answer_marketing\": \"Approaches such as fine-tuning with different datasets, filtering pretraining data, and combining methods to improve model truthfulness can be used to enhance the performance of an AI model on various datasets. However, the effectiveness of these approaches may vary depending on the specific task and dataset.\"\n },\n 46: {\n \"question\": \"What are some common strategies for addressing the ethical and social risks associated with the deployment of language models, according to recent research findings?\",\n \"gold_answer_research\": \"Recent research findings suggest that common strategies for addressing the ethical and social risks associated with the deployment of language models include implementing alignment techniques to ensure that the models are helpful, honest, and harmless. These alignment techniques involve fine-tuning approaches such as reinforcement learning from human feedback to train the models to follow a broad class of written instructions. Additionally, it is important to consider the potential harms of language models in real-world applications and to evaluate how their outputs are used, especially in safety-critical situations. Furthermore, regulating access to large language models and incorporating diverse values-targeted datasets can help mitigate the risks of biased outputs, private data leaks, and misinformation generation.\",\n \"gold_answer_marketing\": \"Recent research findings suggest that common strategies for addressing ethical and social risks associated with the deployment of language models include mitigating biases, protecting private data, preventing the generation of misinformation, and ensuring alignment with human intentions to be helpful, honest, and harmless. These strategies aim to minimize the potential harms that language models may cause in various domains, such as medical diagnoses, employment, housing, and law enforcement.\"\n },\n 47: {\n \"question\": \"What categories are the listed companies classified into, and what are some examples of tasks that can be performed in relation to text extraction, generation, rewriting, and chat services based on the given content?\",\n \"gold_answer_research\": \"The listed companies are classified as follows: Apple falls under the category of Technology, Facebook is categorized as Social Media, and Fedex's category is not specified. Some examples of tasks that can be performed in relation to text extraction include salient span masking, named entity identification, and adding an empty null document. For text generation, tasks can include natural language generation, speech recognition, and machine translation. For rewriting, tasks can involve summarization, paraphrasing, and producing rap lyrics based on a given article. Chat services can include customer assistance, complaint resolution, and information retrieval.\",\n \"gold_answer_marketing\": \"Apple is classified into the Technology category, Facebook is classified into the Social Media category, and Fedex is not classified. Tasks that can be performed in relation to text extraction include salient span masking and adding an empty null document. Tasks related to text generation include speech recognition, machine translation, and natural language generation. For rewriting, tasks can include summarization and rewriting rap lyrics. Chat services can involve customer assistance and complaints.\"\n },\n 48: {\n \"question\": \"What criteria were used to select labelers to ensure they can effectively detect and rate sensitive content?\",\n \"gold_answer_research\": \"The criteria used to select labelers included agreement on sensitive speech flagging, performance on a screening test measuring the ability to detect and respond to sensitive content, comparison of outputs, and a demonstration score of 6/7. Labelers were chosen subjectively based on these criteria, with soft cutoffs at 75% agreement on sensitive speech flagging and comparisons. Additionally, labelers were trained on the project and provided with detailed instructions for each task to ensure consistency and accuracy in their ratings.\",\n \"gold_answer_marketing\": \"The criteria used to select labelers included agreement on sensitive speech flagging, performance on a screening test measuring sensitivity to different demographic groups, and ability to identify potentially harmful outputs. Soft cutoffs were set at 75% agreement on sensitive speech flagging and comparisons, and a 6/7 demonstration score.\"\n },\n 50: {\n \"question\": \"How do the evaluation metrics used in a model contribute to the overall quality and reliability of the generated responses?\",\n \"gold_answer_research\": \"The evaluation metrics used in a model, such as percentage of true responses and informative responses, play a crucial role in assessing the quality and reliability of the generated responses. By comparing the model's outputs to ground-truth summaries and baselines, these metrics help determine the performance of the model in generating accurate and relevant information. Additionally, subjective evaluations by labelers on a Likert scale further contribute to understanding the overall quality of the responses. Future work should focus on mitigating subjective preferences and biases in evaluation systems to ensure more reliable and consistent results.\",\n \"gold_answer_marketing\": \"The evaluation metrics used in a model help determine how often the model's outputs are preferred to a baseline policy, as well as the overall quality of each response. This contributes to assessing the performance and reliability of the generated responses in comparison to other models and benchmarks.\"\n },\n 51: {\n \"question\": \"How do the datasets encourage the application of common-sense reasoning and entailment recognition in natural language processing tasks?\",\n \"gold_answer_research\": \"The datasets mentioned in the context focus on evaluating natural language models on tasks that require common-sense reasoning and entailment recognition. By incorporating prompts that require understanding of context and relevant information, the models are challenged to go beyond surface-level processing. This encourages the development of models that can infer implicit information, draw logical conclusions, and make accurate predictions based on contextual clues. Through these datasets, researchers aim to improve the ability of natural language processing systems to perform more complex and nuanced tasks that mimic human-like reasoning.\",\n \"gold_answer_marketing\": \"The datasets encourage common-sense reasoning and entailment recognition by evaluating model performance on tasks like question answering, reading comprehension, and summarization, which require understanding context and relationships between different pieces of information in natural language.\"\n },\n 52: {\n \"question\": \"What metrics are used to evaluate the quality of translations and summaries in the dataset examples provided?\",\n \"gold_answer_research\": \"Translations in the dataset examples are evaluated using the BLEU metric, while summaries are judged based on their ROUGE-L scores with respect to a set of reference summaries. Additionally, the evaluation metric for the summaries is the f1 score from the sample to the target completion. These metrics help assess the quality and accuracy of the translations and summaries in the datasets.\",\n \"gold_answer_marketing\": \"Translations are evaluated using the BLEU metric, while summaries are judged via their ROUGE-L scores with respect to a set of reference summaries. The evaluation metric for translations is the f1 score from the sample to the target completion.\"\n },\n 53: {\n \"question\": \"What are the implications of adding updates on the pretraining data during the fine-tuning phase of model development based on the observed performance of the models?\",\n \"gold_answer_research\": \"Adding updates on the pretraining data during the fine-tuning phase of model development can help mitigate performance regressions observed in the models. This approach allows for reducing performance regressions on specific datasets without compromising labeler preference scores. Additionally, it enables the models to generalize to the preferences of 'held-out' labelers that did not provide training data, improving overall model performance. This method can be particularly useful for addressing performance issues and enhancing model robustness during the fine-tuning process.\",\n \"gold_answer_marketing\": \"Adding updates on the pretraining data during the fine-tuning phase can help mitigate performance regressions on certain tasks without compromising labeler preference scores. This approach can improve the generalization of models to new data and tasks.\"\n },\n 54: {\n \"question\": \"How do the capabilities of different AI models compare in terms of following explicit constraints and minimizing hallucinations, as evidenced by metadata ratings?\",\n \"gold_answer_research\": \"Based on the metadata ratings, it appears that the capabilities of different AI models vary in terms of following explicit constraints and minimizing hallucinations. The results show that extending LLaMA 7B and 13B models with a longer context window size leads to significant reductions in perplexity, indicating improved performance in minimizing hallucinations. However, it is important to note that the specific term 'AI hallucination' may anthropomorphize computers and the concept of hallucinations in AI is associated with unjustified responses or beliefs. Further research and analysis are needed to fully understand and compare the capabilities of different AI models in this context.\",\n \"gold_answer_marketing\": \"The capabilities of different AI models in terms of following explicit constraints and minimizing hallucinations can be compared based on metadata ratings.\"\n },\n 55: {\n \"question\": \"How do human likert scores compare when evaluating PPO with different initial models based on the pretraining fraction?\",\n \"gold_answer_research\": \"Based on the data provided, it appears that human likert scores for PPO with different initial models do not show significant sensitivity to the pretraining fraction choice, as indicated by the performance seeming not sensitive to the particular choice of 0%, 0.1%, or 0.5% pretraining data mix. Additionally, the likert scores seem to be consistent across different pretraining fractions. Further investigation into the impact of pretraining fraction on likert scores may be necessary to determine if there are any subtle differences in performance.\",\n \"gold_answer_marketing\": \"Human likert scores for PPO with different initial models show that there is not a significant difference in performance based on the pretraining fraction used.\"\n },\n 59: {\n \"question\": \"What methodologies can be employed to test the reliability and accuracy of AI-generated responses to prompts with varying levels of obscurity?\",\n \"gold_answer_research\": \"To test the reliability and accuracy of AI-generated responses to prompts with varying levels of obscurity, methodologies such as measuring truthfulness by comparing the model's actual output to its believed correct output can be employed. Additionally, sensitivity speech flagging can be used to identify and label prompts or completions that may elicit strong negative feelings. Agreement on rankings of prompts runted to the API can also help assess the model's performance on novel questions. Finally, human evaluation through random sampling of prompts and generated outputs can provide valuable insights into the quality of model responses.\",\n \"gold_answer_marketing\": \"Use automated and human evaluations, measure truthfulness, analyze model responses against user intentions, and conduct closed-book QA testing with various prompts.\"\n },\n 60: {\n \"question\": \"How does the subjective nature of human preferences influence the evaluation of chatbot task performance, and what are potential methods for addressing this challenge?\",\n \"gold_answer_research\": \"The subjective nature of human preferences can introduce variability in the evaluation of chatbot task performance, as different human annotators may have differing opinions on what constitutes a preferred response. This can lead to disagreements among evaluators, as seen in the case of comparing generations from different chatbot systems. The text suggests that drawing from disciplines such as Human-Computer Interaction and Psychology may offer insights into mitigating these challenges posed by subjective preferences. Additionally, the proposed Direct Preference Optimization (DPO) algorithm aims to optimize language models based on human preferences without the need for explicit reward modeling or reinforcement learning, offering a potential method to address this challenge.\",\n \"gold_answer_marketing\": \"The subjective nature of human preferences can impact the evaluation of chatbot task performance by introducing disagreements among evaluators. Potential methods for addressing this challenge mentioned in the text include investigating approaches from disciplines like Human-Computer Interaction and Psychology to mitigate subjective preferences.\"\n },\n 61: {\n \"question\": \"What are some potential societal impacts of the widespread use of the QLORA finetuning method for Language Learning Models (LLMs)?\",\n \"gold_answer_research\": \"The authors suggest that the QLORA finetuning method could help close the resource gap between large corporations and small teams, making state-of-the-art NLP technology more accessible. They believe that QLORA could lead to more independent analysis and auditing of LLMs, which could have a positive impact by ensuring models align with societal values and consensus. The method may also enable further investigations into the tradeoffs between simple cross-entropy loss and reinforcement learning from human feedback, potentially leading to more efficient training methods for LLMs.\",\n \"gold_answer_marketing\": \"The potential societal impacts of the widespread use of the QLORA finetuning method for LLMs include making state-of-the-art NLP technology more accessible to researchers with limited resources, closing the resource gap between large corporations and small teams, and enabling independent analysis of LLMs.\"\n },\n 62: {\n \"question\": \"What methods or approaches are being investigated or utilized to optimize or adapt machine learning models?\",\n \"gold_answer_research\": \"Some of the methods or approaches being investigated or utilized to optimize or adapt machine learning models according to the listed references include large-scale pre-training on general domain data followed by adaptation to specific tasks or domains, adding adapter layers, optimizing certain forms of adaptations, and exploring the use of large language models for reference-free text quality evaluation. Additionally, strategies like fine-tuning continuous prompts for generation and learning overparameterized neural networks via stochastic gradient descent on structured data are being explored for model adaptation. These approaches aim to make model adaptation more parameter- and compute-efficient.\",\n \"gold_answer_marketing\": \"The methods or approaches being investigated or utilized include large-scale pre-training on general domain data and adaptation to particular tasks or domains, adding adapter layers, and optimizing some forms of adaptations.\"\n },\n 63: {\n \"question\": \"What are the advantages of applying LoRA to transformer models in terms of computational efficiency during training and deployment?\",\n \"gold_answer_research\": \"Applying LoRA to transformer models offers several advantages in terms of computational efficiency. By using LoRA, a pre-trained model can be shared and used for multiple tasks, reducing the storage requirements and task-switching overhead significantly. Additionally, LoRA makes training more efficient and lowers the hardware barrier to entry by up to 3 times when using adaptive optimizers, as gradients do not need to be calculated for all parameters. This can lead to faster model training and deployment times, improving overall computational efficiency.\",\n \"gold_answer_marketing\": \"Applying LoRA to transformer models can reduce the number of trainable parameters, making training more efficient and lowering the hardware barrier to entry. It also allows for quick task-switching during deployment by sharing the majority of model parameters.\"\n },\n 64: {\n \"question\": \"How does the training cost impact the evaluation of performance metrics across the different methods of adaptation for large Transformer models like GPT-3?\",\n \"gold_answer_research\": \"The training cost has a significant impact on the evaluation of performance metrics across different adaptation methods for large Transformer models like GPT-3. The cost of training our 175B SFT model is 4.9 petaflops/s-days, while training our 175B PPO-ptx model requires 60 petaflops/s-days, compared to 3,640 petaflops/s-days for GPT-3. This difference in training cost affects the efficiency and effectiveness of adaptation methods, as it determines the resources required for training and evaluating the models. Additionally, the training cost influences the scalability and feasibility of implementing these adaptation methods on a larger scale. It is essential to consider the trade-offs between training cost and performance metrics when evaluating and selecting adaptation methods for large Transformer models like GPT-3.\",\n \"gold_answer_marketing\": \"The training cost has a significant impact on the evaluation of performance metrics for large Transformer models like GPT-3. By achieving the largest reduction of trainable parameters, we can adapt the model while maintaining task performances, but the training cost for different adaptation methods varies. This cost can affect the overall efficiency and effectiveness of the adaptation process, as seen in the empirical studies conducted.\"\n },\n 65: {\n \"question\": \"What are the advantages of using low-rank adaptations during the fine-tuning process of pre-trained transformer models?\",\n \"gold_answer_research\": \"Using low-rank adaptations during the fine-tuning process of pre-trained transformer models can potentially amplify important features for specific downstream tasks that were not emphasized in the general pre-training model. This approach allows for a reduction in the number of trainable parameters, leading to a more efficient use of computational resources without sacrificing task performance. Additionally, low-rank adaptations can help in identifying the trade-off between performance and the number of trainable parameters, aiding in better optimization strategies for adaptation in low-data regimes.\",\n \"gold_answer_marketing\": \"Low-rank adaptations potentially amplify important features for specific downstream tasks that were not emphasized in the general pre-training model.\"\n },\n 67: {\n \"question\": \"What are the implications of fine-tuning large language models on datasets of varying sizes in contrast to using few-shot learning techniques?\",\n \"gold_answer_research\": \"Fine-tuning large language models on datasets of varying sizes compared to using few-shot learning techniques can significantly improve model performance across different tasks. Fine-tuning with DPO has shown to exceed PPO-based RLHF in controlling sentiment of generations and matching or improving response quality in summarization and single-turn dialogue. Additionally, fine-tuning is simpler to implement and train, making it a more practical approach for performance-sensitive applications with curated training datasets. The ability to fine-tune models for specific tasks on larger datasets can lead to substantial performance improvements compared to few-shot learning methods.\",\n \"gold_answer_marketing\": \"Fine-tuning with DPO surpasses PPO-based RLHF in sentiment control across generations and improves response quality in summarization and single-turn dialogue while being simpler to implement and train.\"\n },\n 69: {\n \"question\": \"How does the RoFormer model perform on semantic text matching tasks, such as those found in the CAIL2019-SCM dataset?\",\n \"gold_answer_research\": \"Based on the results from the experiments conducted on the CAIL2019-SCM dataset, RoFormer consistently outperforms its alternatives, including BERT and WoBERT, in dealing with long texts and semantic text matching tasks. When compared with BERT and WoBERT on the same pre-training data with short text cut-offs, RoFormer shows comparable performance to WoBERT and slightly better performance than BERT. However, when the maximum input text length is increased to 1024, RoFormer significantly outperforms WoBERT by an absolute improvement of 1.5%, showcasing its effectiveness in semantic text matching tasks.\",\n \"gold_answer_marketing\": \"The RoFormer model outperforms other pre-trained models on semantic text matching tasks, such as those in the CAIL2019-SCM dataset, especially when the maximum input text length is increased to 1024.\"\n },\n 73: {\n \"question\": \"What is the purpose of using a regularization term (like the epsilon multiplied by the identity matrix) in the linear regression model described in the code?\",\n \"gold_answer_research\": \"The purpose of using a regularization term (epsilon multiplied by the identity matrix) in the linear regression model described in the code is to prevent overfitting by adding a penalty term to the loss function. This regularization term helps to control the complexity of the model by discouraging large coefficients, leading to a more stable and generalizable solution. By incorporating this regularization, the model can better handle noise and outliers in the data, ultimately improving its performance on unseen data points.\",\n \"gold_answer_marketing\": \"The purpose of using a regularization term in the linear regression model is to mitigate or resolve catastrophic extrapolation error, making the extrapolated values comparable to those within the original range.\"\n },\n 74: {\n \"question\": \"What novel attention mechanisms does the language model discussed employ to improve performance and efficiency in processing long sequences?\",\n \"gold_answer_research\": \"The language model discussed employs sliding window attention to improve performance and efficiency in processing long sequences. This mechanism helps reduce the number of operations and memory usage in comparison to vanilla attention, leading to lower latency and increased throughput during inference. Additionally, the model incorporates linear attention as an alternative mechanism to avoid quadratic computation costs associated with input sequence length, further enhancing its ability to handle longer sequences effectively.\",\n \"gold_answer_marketing\": \"The language model discussed employs sliding window attention and linear attention mechanisms to improve performance and efficiency in processing long sequences.\"\n },\n 75: {\n \"question\": \"How does the performance of the Mistral 7B model in mathematical benchmarks compare to that of larger pretrained models?\",\n \"gold_answer_research\": \"Based on the provided information, the Mistral 7B model outperforms the Llama 34B model in mathematics and code generation benchmarks. Additionally, Mistral 7B approaches the performance of the larger Code-Llama 7B model in coding tasks. This indicates that Mistral 7B demonstrates high performance in mathematical benchmarks compared to larger pretrained models.\",\n \"gold_answer_marketing\": \"The Mistral 7B model outperforms larger pretrained models in mathematics benchmarks.\"\n },\n 76: {\n \"question\": \"What are the key differences between HALO and non-HALO loss functions, and what impact do these differences have on language model alignment?\",\n \"gold_answer_research\": \"HALO loss functions, such as DPO and KTO, explicitly model human biases like loss aversion, while non-HALO loss functions, like CSFT and SLiC, do not. These differences can impact language model alignment by influencing how well the model captures human preferences and perceptions in text generation tasks. HALOs may be more effective in aligning language models to human expectations and behaviors, potentially leading to more accurate and natural language generation outputs. Further research is needed to fully understand the implications of using HALO versus non-HALO loss functions in language model alignment.\",\n \"gold_answer_marketing\": \"HALO loss functions implicitly model human biases, such as loss aversion, while non-HALO loss functions do not. These biases can impact language model alignment by influencing the generation of text outputs that better match human perceptions of language quality.\"\n },\n 78: {\n \"question\": \"What implications does the lemma from Rafailov et al. (2023) have on the relationship between optimal policies and reward functions within the same equivalence class?\",\n \"gold_answer_research\": \"The lemma from Rafailov et al. (2023) implies that reward functions within the same equivalence class will induce the same optimal policy under the constrained RL problem. This means that even though the reward functions may differ by an input-specific component, they will lead to the same decision-making strategy. This has important implications for understanding the relationship between reward functions and optimal policies, showing that certain variations in the reward functions do not impact the final decision-making process. Understanding this relationship can help in designing more efficient and effective RL algorithms.\",\n \"gold_answer_marketing\": \"The lemma from Rafailov et al. (2023) implies that optimal policies and reward functions within the same equivalence class induce the same optimal policy under the constrained RL problem.\"\n },\n 80: {\n \"question\": \"What is the significance of the term open-domain in the context of question answering?\",\n \"gold_answer_research\": \"Answer: The term 'open-domain' in the context of question answering refers to the lack of specific context or relevant information provided to the model when answering a factual question. This means that the model must rely solely on the question itself without additional articles or background knowledge. In open-domain question answering, the model's task is to produce answers to factoid questions based solely on the input question without any accompanying context or information. Additionally, open-domain question answering systems typically aim to provide objective answers to factual questions, making it easier to evaluate the model's performance.\",\n \"gold_answer_marketing\": \"Answer: The term 'open-domain' in question answering refers to the lack of specific context or relevant information provided for any arbitrarily asked factual question. The model must generate answers solely based on the question itself.\"\n },\n 81: {\n \"question\": \"How do various models handle the retrieval of relevant context for question answering from external knowledge sources?\",\n \"gold_answer_research\": \"Various models handle the retrieval of relevant context for question answering from external knowledge sources in different ways. Some models, like DenSPI, encode all text in the knowledge corpus at the phrase level and rely on a retriever to identify the most relevant phrase as the predicted answer. Others, like ORQA, jointly learn a retriever and reader QA model to optimize correct answers without needing ground-truth context passages during training. Additionally, methods like Self-Ask and IRCoT combine iterative prompting and queries to external sources to construct the thought process iteratively and retrieve relevant content. The effectiveness of these models depends on the retrieval quality and the integration of retrieved content into the prompt.\",\n \"gold_answer_marketing\": \"Various models handle the retrieval of relevant context for question answering from external knowledge sources by using retrieval models to surface context based on relevance, recency, and importance. The dense representations of a question and context passage are extracted using language models, and the dot-product of these representations is used as the retrieval score to rank and select the most relevant passages. Additionally, some models use bi-directional LSTM and reader models to extract answers from context documents.\"\n },\n 82: {\n \"question\": \"What are the differences between open-book and closed-book question answering?\",\n \"gold_answer_research\": \"In open-book question answering, the retriever-generator approach involves two stages where the second stage is to generate free text directly to answer the question. This is also known as generative question answering. On the other hand, closed-book question answering involves the use of generative language models that are pre-trained on a large collection of textual data and can answer questions without explicit context, similar to a closed-book exam. These models produce free text responses to questions without requiring explicit reading comprehension. Additionally, swapping the question with the evidence in closed-book QA demonstrations has been found to consistently yield lower results across all datasets.\",\n \"gold_answer_marketing\": \"Open-book question answering involves generating free text directly to answer a question, while closed-book question answering uses pre-trained language models to produce free text responses without explicit context.\"\n },\n 84: {\n \"question\": \"What are some of the concerns related to fine-tuning QA models with common datasets?\",\n \"gold_answer_research\": \"One concern related to fine-tuning QA models with common datasets is the significant overlap between questions in the train and test sets of public QA datasets. This overlap can potentially lead to biased performance evaluations and limit the generalizability of the model. Additionally, using datasets that primarily focus on tasks like classification and question answering may not fully capture the diverse range of tasks that language models are used for in practice. Furthermore, efforts to modify language models to mitigate harms, such as reducing toxicity, can inadvertently impact their ability to model text from under-represented groups.\",\n \"gold_answer_marketing\": \"One concern of fine-tuning QA models with common datasets is the significant overlap between questions in the training and test sets, which can impact the model's performance.\"\n },\n 85: {\n \"question\": \"How does the performance of question answering models tend to vary with the size of the model?\",\n \"gold_answer_research\": \"The performance of question answering models tends to improve with the size of the model, with larger models like T5 with 11B parameters outperforming smaller models like DPR with 3 BERT-base models. However, increasing the size of the model may negatively affect its performance in certain cases. Research has shown that larger models have the capability to memorize answers seen during training and perform well on novel questions at test time, but dataset suitability is also important for optimal performance. Additionally, extensive analysis of chatbot performance using both human raters and AI evaluation has been conducted to determine the effectiveness of different models.\",\n \"gold_answer_marketing\": \"The performance of question answering models tends to improve with larger model sizes.\"\n },\n 86: {\n \"question\": \"What are the advantages and limitations of using generative language models for closed-book question answering?\",\n \"gold_answer_research\": \"The advantages of using generative language models for closed-book question answering include the ability to memorize factual knowledge within parameter weights and produce free text responses without explicit context. Additionally, models like RAG can be fine-tuned on various tasks for better performance. However, limitations may arise from the reliance on supervised learning, which can be costly and time-consuming, and may restrict the use on datasets that are not well-annotated. Furthermore, the performance of generative models may vary based on the model size and the quality of the retrieved relevant context.\",\n \"gold_answer_marketing\": \"Advantages: Generative language models can memorize factual knowledge and answer questions without explicit context, similar to a closed-book exam. Limitations: Reliance on supervised learning limits use on datasets that are not well-annotated and can be expensive and time-consuming to train extremely large models.\"\n },\n 87: {\n \"question\": \"What are the main components that complement the central controller in an autonomous agent system?\",\n \"gold_answer_research\": \"In an autonomous agent system powered by LLM, the main components that complement the central controller include Planning, Memory, and Task Decomposition. Planning involves breaking down tasks into subgoals and reflecting on past actions for refinement. Memory allows the agent to store and retrieve information for decision-making. Task Decomposition focuses on the reliability of the natural language interface and the use of expert modules for routing inquiries. Together, these components enhance the overall functionality and efficiency of the autonomous agent system.\",\n \"gold_answer_marketing\": \"The main components that complement the central controller in an autonomous agent system are Planning and Memory.\"\n },\n 88: {\n \"question\": \"How can subgoals and task decomposition improve the handling of complex tasks in autonomous systems?\",\n \"gold_answer_research\": \"Subgoals and task decomposition can improve the handling of complex tasks in autonomous systems by breaking down large tasks into smaller, more manageable subtasks. This enables the agent to efficiently navigate through the various steps required to complete the overall task. By reflecting on past actions and refining their approach, the agent can learn from mistakes and continuously improve the quality of their final results. This iterative process of breaking down tasks and refining actions ultimately leads to enhanced safety and reliability in autonomous systems.\",\n \"gold_answer_marketing\": \"Subgoals and task decomposition can break down large tasks into smaller, manageable steps, enabling efficient handling of complex tasks in autonomous systems. This allows the agent to plan ahead, reflect on past actions, learn from mistakes, and refine strategies for future steps, ultimately improving the quality of final results.\"\n },\n 89: {\n \"question\": \"What types of memory are leveraged in autonomous agents, and how do they differ?\",\n \"gold_answer_research\": \"In autonomous agents, two main types of memory are leveraged: short-term memory and long-term memory. Short-term memory is utilized for in-context learning, while long-term memory allows for the retention and recall of information over extended periods. The agents can interact with other agents and retain past experiences using these memory mechanisms. Short-term memory is essential for immediate learning, while long-term memory enables agents to store and retrieve a vast amount of information over time. These memory systems enable agents to make informed decisions based on past experiences and interactions.\",\n \"gold_answer_marketing\": \"Short-term memory and long-term memory are utilized in autonomous agents. Short-term memory is used for in-context learning, while long-term memory allows agents to retain and recall information over extended periods. Long-term memory often leverages external storage for fast retrieval.\"\n },\n 91: {\n \"question\": \"How does planning and reflection contribute to the iterative improvement of autonomous agents?\",\n \"gold_answer_research\": \"Planning and reflection play a crucial role in the iterative improvement of autonomous agents by enabling them to break down tasks into manageable subgoals, learn from past actions, and refine their strategies for future steps. Through task decomposition, agents can efficiently handle complex tasks, while self-criticism and self-reflection allow them to identify mistakes and make necessary adjustments, ultimately improving the quality of their final results. By synthesizing memories and guiding future behavior based on past events, the reflection mechanism helps agents make higher-level inferences over time, leading to continuous learning and enhancement in performance.\",\n \"gold_answer_marketing\": \"Planning and reflection contribute to the iterative improvement of autonomous agents by allowing them to break down tasks into manageable subgoals, learn from past actions, correct mistakes, and refine their approach for future steps. This process leads to better decision-making, higher-quality results, and overall improvement in performance over time.\"\n },\n 92: {\n \"question\": \"In what ways can an agent's performance be evaluated and refined over time?\",\n \"gold_answer_research\": \"An agent's performance can be evaluated and refined over time by continuously reviewing and analyzing actions, constructively self-criticizing behavior, reflecting on past decisions and strategies, and aiming to complete tasks in the least number of steps. Additionally, the agent can evaluate the results of API calls and refine inputs if necessary, as well as break down large tasks into smaller subgoals for more efficient handling. Self-reflection, learning from mistakes, and refining actions for future steps can also improve the quality of final results.\",\n \"gold_answer_marketing\": \"The speaker of the dialogue is 'agent'. An agent's performance can be evaluated and refined over time by continuously reviewing and analyzing actions, constructively self-criticizing big-picture behavior, reflecting on past decisions and strategies, being smart and efficient in completing tasks, breaking down tasks into smaller subgoals, self-reflection and refinement of actions, learning from mistakes, calling external APIs for additional information, and using memory, planning, and reflection mechanisms to improve behavior based on past experiences.\"\n },\n 93: {\n \"question\": \"What challenges are faced in enhancing the long-term planning capabilities of autonomous agents?\",\n \"gold_answer_research\": \"Enhancing long-term planning capabilities of autonomous agents faces challenges such as effectively exploring the solution space over a lengthy history and adjusting plans when unexpected errors occur. LLMs struggle to adapt plans in response to errors, making them less robust compared to human learning from trial and error. To address this, agents need to improve task decomposition by breaking down large tasks into manageable subgoals and incorporate reflection mechanisms for self-criticism and learning from past actions. Additionally, the integration of external classical planners like PDDL can aid in long-horizon planning for complex tasks.\",\n \"gold_answer_marketing\": \"Challenges in long-term planning for autonomous agents include difficulty in task decomposition, adjusting plans when faced with unexpected errors, and lack of robustness compared to human learning from trial and error.\"\n },\n 94: {\n \"question\": \"How does the use of tools and external resources extend the capabilities of large language models in practical applications?\",\n \"gold_answer_research\": \"The use of tools and external resources extends the capabilities of large language models by fine-tuning them to leverage external tool APIs. This process involves expanding the dataset based on the potential improvement in model outputs from newly added API call annotations. By incorporating external resources, such as APIs, large language models can enhance their performance and adaptability for a wide range of practical applications, including speech recognition, machine translation, and natural language generation. This approach helps create more efficient and high-performing language models that can effectively handle diverse tasks and scenarios.\",\n \"gold_answer_marketing\": \"The use of tools and external resources allows large language models to access additional information and functions, improving their performance in various tasks such as speech recognition, machine translation, and natural language generation. By fine-tuning the models to utilize external APIs, the dataset is expanded and enriched, leading to more accurate and efficient model outputs. This approach enhances the adaptability and effectiveness of large language models in practical applications.\"\n },\n 95: {\n \"question\": \"What are the primary techniques involved in steering the behavior of language models without modifying their underlying architectures?\",\n \"gold_answer_research\": \"The primary techniques involved in steering the behavior of language models without modifying their underlying architectures include using language models to generate toxic outputs as part of a data augmentation pipeline, making models refuse certain user instructions, and combining Reinforcement Learning from Human Feedback (RLHF) with architectures that heavily use self-attention. Additionally, techniques such as supervised fine-tuning (SFT) and few-shot learning, or prompt engineering, are utilized to achieve precise control over the behavior of language models. These techniques focus on aligning language models with human intentions through iterative improvements and alignment research.\",\n \"gold_answer_marketing\": \"The primary techniques involved in steering the behavior of language models without modifying their underlying architectures include using language models to generate toxic outputs as part of a data augmentation pipeline, making models refuse certain user instructions, and combining reinforcement learning with architecture that heavily uses self-attention.\"\n },\n 96: {\n \"question\": \"How can the alignment and steerability of language models be influenced through specific engineering methods?\",\n \"gold_answer_research\": \"Specific engineering methods that can influence the alignment and steerability of language models include fine-tuning data selection, alignment method choice, and incorporating pretraining data into reinforcement learning from human feedback (RLHF) processes. Additionally, designing interfaces for human labelers to provide feedback to language models can play a crucial role in improving alignment. It is also important to consider factors like model response editing, generating critiques, and using diverse feedback mechanisms to enhance alignment and steerability in language models.\",\n \"gold_answer_marketing\": \"Promote alignment and steerability of language models through specific engineering methods like prompt engineering, fine-tuning data, and incorporating pretraining data into reinforcement learning from human feedback (RLHF).\"\n },\n 101: {\n \"question\": \"What innovative approaches are being explored to enhance language model's reasoning capabilities, specifically for complex reasoning tasks?\",\n \"gold_answer_research\": \"Some innovative approaches being explored to enhance language model's reasoning capabilities for complex tasks include ReAct by Yao et al. (2023), which synergizes reasoning and acting in language models, Complexity-based prompting for multi-step reasoning by Fu et al. (2022), Rationale-augmented ensembles in language models by Wang et al. (2022), and Automatic chain of thought prompting in large language models by Zhang et al. (2022). These approaches focus on improving the model's ability to handle complex reasoning tasks by incorporating advanced techniques such as prompt-based learning, rationale augmentation, and automatic chaining of thoughts. Additionally, models like PAL and PoT offload complex computation and reasoning tasks by generating programming language statements to resolve natural language problems, enhancing the efficiency and performance of the language model.\",\n \"gold_answer_marketing\": \"Researchers are exploring approaches like ReAct, complexity-based prompting, rationale-augmented ensembles, and automatic chain of thought prompting to enhance language model's reasoning capabilities for complex tasks.\"\n },\n 102: {\n \"question\": \"How can external tools and APIs be integrated with language models to extend their capabilities and applications?\",\n \"gold_answer_research\": \"External tools and APIs can be integrated with language models by fine-tuning the models to learn how to use these external resources effectively. This involves expanding the dataset to determine if adding new API call annotations can enhance the model's output quality. By calling external APIs for additional information that may be missing from the model weights, such as current information or access to proprietary sources, the language model can be prompted further to generate more accurate and insightful responses. Additionally, incorporating tool use capabilities, like those seen in ChatGPT Plugins and OpenAI API function calling, can provide practical examples of how language models can benefit from external resources.\",\n \"gold_answer_marketing\": \"External tools and APIs can be integrated with language models through techniques like fine-tuning the models to use the APIs for extra information, accessing external data sources, executing code, and enhancing the model's capabilities for tasks like speech recognition, machine translation, and natural language generation. This integration helps extend the applications of language models by providing access to additional resources and functionalities.\"\n },\n 103: {\n \"question\": \"What kind of learning challenges does the attention mechanism address in neural machine translation?\",\n \"gold_answer_research\": \"The attention mechanism in neural machine translation addresses the challenge of memorizing long source sentences by creating shortcuts between the context vector and the entire source input. This allows for more efficient learning of dependencies between source and target sequences, regardless of the in-between distance. Additionally, the attention mechanism enables the model to learn correlations between current words and previous parts of the sentence, improving the overall translation quality. By adapting different types of attention weight matrices, the model can further enhance its learning capabilities.\",\n \"gold_answer_marketing\": \"The attention mechanism helps address the challenge of memorizing long source sentences and creating shortcuts between context vectors and entire source inputs in neural machine translation.\"\n },\n 104: {\n \"question\": \"How is the encoder-decoder architecture in seq2seq models affected by long input sequences?\",\n \"gold_answer_research\": \"In seq2seq models, the encoder-decoder architecture can be affected by long input sequences. This can lead to challenges in attending to the relevant parts of the input during decoding, potentially resulting in erroneous generation. Additionally, the design of the decoding strategy itself, such as using top-k sampling for improved generation diversity, can also contribute to issues like hallucinations. Therefore, careful consideration of handling long input sequences and refining decoding strategies is crucial for effective seq2seq model performance.\",\n \"gold_answer_marketing\": \"The encoder-decoder architecture in seq2seq models may struggle with long input sequences, leading to difficulties in generating accurate target sequences.\"\n },\n 105: {\n \"question\": \"What are the differences between soft and hard attention in the context of image caption generation?\",\n \"gold_answer_research\": \"Soft attention in image caption generation allows the model to distribute its focus over all patches in the source image, similar to the attention mechanism proposed by Bahdanau et al. in 2015. On the other hand, hard attention restricts the model to only attend to a specific patch of the image when generating a word. This distinction between soft and hard attention impacts how the model learns to align image features with generated words, influencing the quality and interpretability of the generated captions. Additionally, the choice between soft and hard attention can affect the model's ability to capture global dependencies within the image.\",\n \"gold_answer_marketing\": \"Soft attention in image caption generation refers to alignment weights that are learned and placed 'softly' over all patches in the source image. This allows the model to focus on different regions of the image simultaneously. On the other hand, hard attention involves selecting a single aligned position for the current target word, making it more like a spotlight that focuses on specific areas of the image at a time.\"\n },\n 106: {\n \"question\": \"Can you describe the multi-head self-attention mechanism in the transformer model?\",\n \"gold_answer_research\": \"In the transformer model, the multi-head self-attention mechanism runs the scaled dot-product attention multiple times in parallel instead of just once. This allows the model to jointly attend to information from different representation subspaces at different positions. The independent attention outputs are then concatenated and linearly transformed to the expected dimensions. This approach enhances the model's ability to capture relationships and dependencies across different parts of the input sequence, leading to improved performance in tasks such as machine reading and summarization.\",\n \"gold_answer_marketing\": \"The multi-head self-attention mechanism in the transformer model runs scaled dot-product attention multiple times in parallel, allowing the model to jointly attend to information from different representation subspaces at different positions. The independent attention outputs are concatenated and linearly transformed into the expected dimensions.\"\n },\n 107: {\n \"question\": \"In what ways does SNAIL address the issue of positioning in the transformer model?\",\n \"gold_answer_research\": \"SNAIL addresses the issue of weakly incorporating sequential order in the transformer model by using self-attention to enhance performance. It formulates relative position using vector production and encodes absolute position information through a rotation matrix. Additionally, SNAIL mathematically illustrates the advantageous properties of this method when applied to the Transformer. This approach improves the model's ability to handle positional dependencies, particularly important for tasks like reinforcement learning.\",\n \"gold_answer_marketing\": \"SNAIL addresses the issue of positioning in the transformer model by using self-attention to enhance the performance and incorporating relative position naturally through vector production and absolute position information through a rotation matrix.\"\n },\n 108: {\n \"question\": \"How does the Pointer Network differ from standard seq2seq models in dealing with sequential data?\",\n \"gold_answer_research\": \"The Pointer Network differs from standard seq2seq models in that it is specifically designed to handle problems where the output elements correspond to positions in an input sequence. Instead of using attention to blend hidden units like in seq2seq models, the Pointer Network directly points to the elements in the input sequence. This allows for greater flexibility in determining the output elements, which is particularly useful in tasks like sorting or the traveling salesman problem where the output categories are not predetermined. The architecture of a Pointer Network model includes an encoder-decoder setup with a focus on predicting or inferring specific elements in the input sequence based on their importance weights.\",\n \"gold_answer_marketing\": \"The Pointer Network differs from standard seq2seq models by being able to handle problems where the output elements correspond to positions in an input sequence, rather than using attention to blend hidden units.\"\n },\n 110: {\n \"question\": \"How does Neural Turing Machine (NTM) simulate the infinite memory characteristic of Turing machines?\",\n \"gold_answer_research\": \"Neural Turing Machine (NTM) simulates the infinite memory characteristic of Turing machines by coupling a neural network with external memory storage. The memory in NTM mimics the Turing machine tape, allowing the neural network to control operation heads to read from or write to the tape. However, the memory in NTM is finite, resembling more of a 'Neural von Neumann Machine' due to practical limitations in real modern computers. This limitation is addressed by various works that add memory capabilities to Transformers through recurrence, enhancing the model's ability to handle very long sequences while still maintaining attention mechanisms for efficient memory access.\",\n \"gold_answer_marketing\": \"Neural Turing Machine (NTM) uses external memory storage that mimics the Turing machine tape, allowing the neural network to control operation heads to read from or write to the tape. However, the memory in NTM is finite, making it more similar to a 'Neural von Neumann Machine' rather than having infinite memory like a Turing machine.\"\n }\n}\n\ntest_questions = {\n 4: {\"question\": \"What significant advancements did the transformer architecture introduce in natural language processing?\"},\n 5: {\"question\": \"How has the public accessibility of GPT models evolved since their inception?\"},\n 6: {\"question\": \"What benchmark is used to compare the performance of different language models, as mentioned in the text?\"},\n 10: {\"question\": \"What type of experiments did IBM perform in the 1980s regarding language models?\"},\n 14: {\"question\": \"How is the evaluation of language models typically conducted?\"},\n 15: {\"question\": \"Name some benchmarks or data sets used for evaluating language processing systems.\"},\n 21: {\"question\": \"For what purpose do language models like Claude undergo fine-tuning after their initial pre-training?\"},\n 26: {\"question\": \"How does Chinchilla's training approach differ with respect to the number of training tokens compared to the model size?\"},\n 29: {\"question\": \"What is the role of optimizers in the training of Chinchilla and how does it differ from its predecessor?\"},\n 31: {\"question\": \"In what context is LaMDA mentioned in relation to the Gopher and Chinchilla models?\"},\n 32: {\"question\": \"How does the DPO method influence the generation of language completions in terms of preferred and dispreferred outcomes?\"},\n 37: {\"question\": \"What are the key advantages of InstructGPT models over GPT-3 models according to the findings in the research?\"},\n 40: {\"question\": \"When evaluating summaries or chatbot responses, what criteria should be considered to determine their effectiveness and helpfulness?\"},\n 42: {\"question\": \"What are the potential limitations and drawbacks of large language models that have not been fine-tuned with human feedback?\"},\n 49: {\"question\": \"What methods are used to evaluate and measure bias and toxicity in the provided datasets?\"},\n 56: {\"question\": \"What are the benchmarks used to evaluate the performance of the Deep Policy Optimization (DPO) method compared to other preference learning algorithms in the document provided?\"},\n 57: {\"question\": \"What innovations does QLORA introduce to enhance the efficiency of language model fine-tuning, and what are the benefits achieved by these innovations?\"},\n 58: {\"question\": \"How does the use of 4-bit QLORA with NF4 data type compare to 16-bit LoRA finetuning in terms of performance on benchmark datasets?\"},\n 66: {\"question\": \"What approaches have been proposed to enhance the effectiveness of fine-tuning in language models, according to recent research?\"},\n 68: {\"question\": \"What performance improvement can be observed when using fine-tuning over few-shot learning according to the study's results outlined in Table 8?\"},\n 71: {\"question\": \"What metric is used to evaluate the effectiveness of a language model in the context of processing long sequences of text, as reported in the given document?\"},\n 72: {\"question\": \"How does Position Interpolation contribute to the adaptability of language models for processing texts with varying lengths?\"},\n 77: {\"question\": \"What are some of the research areas and methodologies referenced in the literature cited within the field of artificial intelligence and machine learning?\"},\n 79: {\"question\": \"What are the common approaches for building open-domain question answering systems?\"},\n 83: {\"question\": \"In what ways do neural networks, particularly Transformer-based models, contribute to question answering tasks?\"},\n 90: {\"question\": \"What potential benefits does integrating external APIs provide for autonomous agents?\"},\n 98: {\"question\": \"Can you elaborate on some biases that may affect the performance of language models during tasks and the strategies employed to mitigate these biases?\"},\n 99: {\"question\": \"What is the significance of example diversity and order during language model prompting, and what are some methods to optimize these factors?\"},\n 100: {\"question\": \"In the context of task instruction, how can the communication cost be reduced while maintaining effective interaction with language models?\"},\n 109: {\"question\": \"What actor plays the role of Thanos in the Marvel Universe and the role of Cable in Deadpool 2?\"}\n}"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "jC86ggPEuipg"
 },
 "source": "### 3.3 Running the RAG System\n\nLet's have a quick look at the validation and test data:"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "2FQbNGECSq1g"
 },
 "source": "Let's now use the data to ask questions against it. So we need to define our prompt templates, the RAG Chain, etc.\n\nWe have two types of User Personas we need to support:\n\n1. The engineers, who require pretty detailed information when they ask questions \n2. The marketing team and supporting staff who also will ask questions around GenAI in order to better understand the products and the field as a whole, but a lot more high level answers would likely be in order\n\n**Below, please build your RAG pipeline including the relevant prompts. This is free form so you will need to create your own cells, text documentation as you need, etc.**"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "T5kZk7tL2tu0"
 },
 "source": "## Class Config"
 },
 {
 "cell_type": "code",
 "execution_count": 67,
 "metadata": {
 "id": "LoGMg6cu2ver",
 "executionInfo": {
 "status": "ok",
 "timestamp": 1765128428859,
 "user_tz": 420,
 "elapsed": 1,
 "user": {
 "displayName": "Ryan C",
 "userId": "13217471319505539255"
 }
 }
 },
 "outputs": [],
 "source": "from dataclasses import dataclass\nfrom typing import Optional\n\n@dataclass\nclass VectorStoreConfig:\n MODEL_ABBREV = {\n 'all-mpnet-base-v2':'mpnet',\n 'all-MiniLM-L12-v2':'minilm',\n 'multi-qa-mpnet-base-dot-v1': 'multiqampnet',\n 'all-distilroberta-v1':'distilroberta',\n 'multi-qa-distilbert-cos-v1': 'multiqadistilbert'\n }\n\n embedding_model: str = \"multi-qa-mpnet-base-dot-v1\"\n splitter_type: str = \"text\"\n chunk_size: Optional[int] = None\n chunk_overlap: Optional[int] = None\n\n def get_id(self):\n abbrev = VectorStoreConfig.MODEL_ABBREV[self.embedding_model]\n parts = [abbrev, self.splitter_type]\n if self.chunk_size is not None:\n parts.append(str(self.chunk_size))\n if self.chunk_overlap is not None:\n parts.append(str(self.chunk_overlap))\n return '_'.join(parts)"
 },
 {
 "cell_type": "code",
 "execution_count": 68,
 "metadata": {
 "id": "SkCjuBxc7h3B",
 "executionInfo": {
 "status": "ok",
 "timestamp": 1765128428861,
 "user_tz": 420,
 "elapsed": 1,
 "user": {
 "displayName": "Ryan C",
 "userId": "13217471319505539255"
 }
 }
 },
 "outputs": [],
 "source": "config = VectorStoreConfig(\n embedding_model=\"all-distilroberta-v1\",\n splitter_type=\"text\",\n chunk_size=1200,\n chunk_overlap=200\n)"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "ZE9Aj-cqW6l1"
 },
 "source": "## Step 1: Embedding Model"
 },
 {
 "cell_type": "code",
 "execution_count": 69,
 "metadata": {
 "id": "FaWqSWErzUEX",
 "executionInfo": {
 "status": "ok",
 "timestamp": 1765128436769,
 "user_tz": 420,
 "elapsed": 7907,
 "user": {
 "displayName": "Ryan C",
 "userId": "13217471319505539255"
 }
 }
 },
 "outputs": [],
 "source": "# Step 1: Embedding Model\n# The embedding models you can use are: 'all-mpnet-base-v2', 'all-MiniLM-L12-v2', 'multi-qa-mpnet-base-dot-v1', 'all-distilroberta-v1', and 'multi-qa-distilbert-cos-v1 '\n%%capture\nbase_embeddings = HuggingFaceEmbeddings(model_name=config.embedding_model)"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "LXaFb_0aW6l1"
 },
 "source": "## Step 2: Build Vectorstore"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "0xNLhZTBW6l1"
 },
 "source": "### 2.1: Archive X"
 },
 {
 "cell_type": "code",
 "execution_count": 70,
 "metadata": {
 "id": "phyfAoE7SUyt",
 "executionInfo": {
 "status": "ok",
 "timestamp": 1765128436793,
 "user_tz": 420,
 "elapsed": 25,
 "user": {
 "displayName": "Ryan C",
 "userId": "13217471319505539255"
 }
 }
 },
 "outputs": [],
 "source": "# Step 2: Load Chunks\n\ntext_splitter = RecursiveCharacterTextSplitter(chunk_size=config.chunk_size, chunk_overlap=config.chunk_overlap)"
 },
 {
 "cell_type": "code",
 "execution_count": 71,
 "metadata": {
 "colab": {
 "base_uri": "https://localhost:8080/"
 },
 "executionInfo": {
 "elapsed": 18037,
 "status": "ok",
 "timestamp": 1765128454831,
 "user": {
 "displayName": "Ryan C",
 "userId": "13217471319505539255"
 },
 "user_tz": 420
 },
 "id": "vfqxmmPiW6l2",
 "outputId": "0e5a953a-d8c1-4c9c-ee45-38cdc4d51e33"
 },
 "outputs": [
 {
 "output_type": "stream",
 "name": "stdout",
 "text": [
 "Number of splits/chunks: 2291\n",
 "28 documents in total\n",
 "598 pages in total\n"
 ]
 }
 ],
 "source": "# 2.1: Archive X\n#assign a unique number to each document we ingest\nglobal_doc_number = 1\n\narxiv_numbers = ('2005.11401', '2104.07567', '2104.09864', '2105.03011', '2106.09685', '2203.15556', '2203.02155', '2211.09260', '2211.12561',\n '2212.09741', '2305.14314', '2305.18290', '2306.15595', '2309.08872', '2309.15217', '2310.06825', '2310.11511',\n '2311.08377', '2312.05708', '2401.06532', '2401.17268', '2402.01306', '2402.19473', '2406.04744',\n '2312.10997', '2410.12812', '2410.15944', '2404.00657',\n )\n\nall_arxiv_pages = []\n\n#loop through the papers\nfor identifier in arxiv_numbers:\n # Construct URL using the arXiv unique identifier\n arx_url = f\"https://arxiv.org/pdf/{identifier}.pdf\"\n\n # Extract pages from the document and add them to the list of pages\n arx_loader = PyMuPDFLoader(arx_url)\n arx_pages = arx_loader.load()\n for page_num in range(len(arx_pages)):\n page = arx_pages[page_num]\n #CHANGED\n page.metadata['page_num'] = page_num\n page.metadata['doc_num'] = global_doc_number\n page.metadata['doc_source'] = \"ArXiv\"\n all_arxiv_pages.append(page)\n\n global_doc_number += 1\n\n# Document chunking\n#index doc chunks\nsplits = text_splitter.split_documents(all_arxiv_pages)\nfor idx, text in enumerate(splits):\n splits[idx].metadata['split_id'] = idx\n\nprint('Number of splits/chunks: ', len(splits))\n\nnum_pages = len(all_arxiv_pages)\nnum_docs = global_doc_number - 1\n\nprint(f\"{num_docs} documents in total\")\nprint(f\"{num_pages} pages in total\")"
 },
 {
 "cell_type": "code",
 "execution_count": 72,
 "metadata": {
 "id": "aNCbQV9nn2Xy",
 "executionInfo": {
 "status": "ok",
 "timestamp": 1765128489408,
 "user_tz": 420,
 "elapsed": 34576,
 "user": {
 "displayName": "Ryan C",
 "userId": "13217471319505539255"
 }
 }
 },
 "outputs": [],
 "source": "qdrant_vectorstore = Qdrant.from_documents(splits,\n base_embeddings,\n location=\":memory:\", # Local mode with in-memory storage only\n collection_name=\"rag_tech_db\",\n force_recreate=True\n)\n\nretriever = qdrant_vectorstore.as_retriever(search_kwargs={\"k\": 8})"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "NHxJSgInW6l2"
 },
 "source": "## 2.2: Wikipedia"
 },
 {
 "cell_type": "code",
 "execution_count": 73,
 "metadata": {
 "colab": {
 "base_uri": "https://localhost:8080/"
 },
 "executionInfo": {
 "elapsed": 10639,
 "status": "ok",
 "timestamp": 1765128500049,
 "user": {
 "displayName": "Ryan C",
 "userId": "13217471319505539255"
 },
 "user_tz": 420
 },
 "id": "rEHRxNyRzVxf",
 "outputId": "7c962c47-2ed9-4b95-d142-d959e78dbd16"
 },
 "outputs": [
 {
 "output_type": "stream",
 "name": "stdout",
 "text": [
 "Number of documents: 4\n",
 "Number of splits/chunks: 21\n",
 "Number of documents: 4\n",
 "Number of splits/chunks: 22\n",
 "Number of documents: 4\n",
 "Number of splits/chunks: 17\n",
 "Number of documents: 4\n",
 "Number of splits/chunks: 19\n"
 ]
 },
 {
 "output_type": "execute_result",
 "data": {
 "text/plain": [
 "['2c3c71f09f3246f2b4fe5ce1a36bbfb0',\n",
 " '3c3acbdb84394b0197065472165bf37b',\n",
 " '533f17692aa94e2eb4978fd7d88a892a',\n",
 " 'f39c4321f0ec4e309ae3a74e2d8da4d2',\n",
 " '15280c0e015f4cb49c32c15170529060',\n",
 " 'a099cc724dbc4be7a31af76fa2a1b7d8',\n",
 " 'fcb79298d20b442dbe135640c66f4623',\n",
 " '141a849406bf472893414249016b6881',\n",
 " '288fbb014c3449cf8b85cd16444de5ef',\n",
 " 'e373aaf0908c400ca902cde7be581334',\n",
 " 'ddba3f9de272410eb88e0ef084f575b9',\n",
 " '427455834255424d923095f3e5826391',\n",
 " '17d9f6abe91d47b994eb4ffc30242432',\n",
 " 'a9cbfd0bbd5f4d82979b6e0748b36f2e',\n",
 " 'cdc3649f2a97489aa81a40f9a20188e0',\n",
 " 'f6b92690af274475b660fa6b7a00dda1',\n",
 " '9bb52f2527aa43ce8bffdf03189e7cff',\n",
 " 'cbf10e84defb4905a712c975967a4153',\n",
 " '3a182a2840814c11a9036360c0da499a']"
 ]
 },
 "metadata": {},
 "execution_count": 73
 }
 ],
 "source": "wiki_docs = WikipediaLoader(query=\"Generative Artificial Intelligence\", load_max_docs=4).load()\nfor idx, text in enumerate(wiki_docs):\n wiki_docs[idx].metadata['doc_num'] = global_doc_number\n wiki_docs[idx].metadata['doc_source'] = \"Wikipedia\"\n\n global_doc_number += 1\n\nprint('Number of documents: ', len(wiki_docs))\n\n#index docs\nwiki_splits = text_splitter.split_documents(wiki_docs)\nfor idx, text in enumerate(wiki_splits):\n wiki_splits[idx].metadata['split_id'] = idx\n\nprint('Number of splits/chunks: ', len(wiki_splits))\n\nqdrant_vectorstore.add_documents(documents=wiki_splits)\n\nwiki_docs = WikipediaLoader(query=\"Information Retrieval\", load_max_docs=4).load()\nfor idx, text in enumerate(wiki_docs):\n wiki_docs[idx].metadata['doc_num'] = global_doc_number\n wiki_docs[idx].metadata['doc_source'] = \"Wikipedia\"\n\n global_doc_number += 1\n\nprint('Number of documents: ', len(wiki_docs))\n\n#index docs\nwiki_splits = text_splitter.split_documents(wiki_docs)\nfor idx, text in enumerate(wiki_splits):\n wiki_splits[idx].metadata['split_id'] = idx\n\nprint('Number of splits/chunks: ', len(wiki_splits))\n\nqdrant_vectorstore.add_documents(documents=wiki_splits)\n\nwiki_docs = WikipediaLoader(query=\"Large Language Models\", load_max_docs=4).load()\nfor idx, text in enumerate(wiki_docs):\n wiki_docs[idx].metadata['doc_num'] = global_doc_number\n wiki_docs[idx].metadata['doc_source'] = \"Wikipedia\"\n\n global_doc_number += 1\n\nprint('Number of documents: ', len(wiki_docs))\n\n#index docs\nwiki_splits = text_splitter.split_documents(wiki_docs)\nfor idx, text in enumerate(wiki_splits):\n wiki_splits[idx].metadata['split_id'] = idx\n\nprint('Number of splits/chunks: ', len(wiki_splits))\n\nqdrant_vectorstore.add_documents(documents=wiki_splits)\n\nwiki_docs = WikipediaLoader(query=\"Retrieval Augmented Generation\", load_max_docs=4).load()\nfor idx, text in enumerate(wiki_docs):\n wiki_docs[idx].metadata['doc_num'] = global_doc_number\n wiki_docs[idx].metadata['doc_source'] = \"Wikipedia\"\n\n global_doc_number += 1\n\nprint('Number of documents: ', len(wiki_docs))\n\n#index docs\nwiki_splits = text_splitter.split_documents(wiki_docs)\nfor idx, text in enumerate(wiki_splits):\n wiki_splits[idx].metadata['split_id'] = idx\n\nprint('Number of splits/chunks: ', len(wiki_splits))\n\nqdrant_vectorstore.add_documents(documents=wiki_splits)"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "246iMRplW6l2"
 },
 "source": "## 2.3: Blogs"
 },
 {
 "cell_type": "code",
 "execution_count": 74,
 "metadata": {
 "colab": {
 "base_uri": "https://localhost:8080/"
 },
 "executionInfo": {
 "elapsed": 4705,
 "status": "ok",
 "timestamp": 1765128504773,
 "user": {
 "displayName": "Ryan C",
 "userId": "13217471319505539255"
 },
 "user_tz": 420
 },
 "id": "KIpDo4nCW6l3",
 "outputId": "e1beba58-5457-4f61-e289-324d79f81977"
 },
 "outputs": [
 {
 "output_type": "stream",
 "name": "stdout",
 "text": [
 "Number of documents: 6\n",
 "Number of splits: 283\n"
 ]
 },
 {
 "output_type": "execute_result",
 "data": {
 "text/plain": [
 "['76ebbe1b8c2742e79f1611b247fa70d2',\n",
 " 'd2cfd072a0214acf8e1e4e015ef5bc9b',\n",
 " '8e74141c52814e418d4b51f5e2495155',\n",
 " '233c937357f14c74996103f8e7de38ab',\n",
 " 'e70475372df6454fb2e930ea9067847a',\n",
 " '365302d984974544820dfe58382e0810',\n",
 " 'dd2ddb6009dc4936b53a63fb1511c9b7',\n",
 " '4dd0453f41b340fdb541afd8a04556dc',\n",
 " 'bf4c06e09ee542079a3e52ae1a04a0e0',\n",
 " '5500345c7b254133935cbd035cc320fc',\n",
 " '58f4442e9a4d4e348123d051e59a8e97',\n",
 " '11fd08aa8d1b4735bfcbe80e271d8e54',\n",
 " 'd3d0e7ba611e402397f9ef52a48305f0',\n",
 " '5a78b9f4c32640efb6d50986d8466c48',\n",
 " '0cf804251b264a92817ae90786df0ebd',\n",
 " '5bbb3b92cda64227af307a912de112d0',\n",
 " 'bc642885b87a41c08be8eaf1bac99443',\n",
 " '3256c953a82d49fb9ed536a72bd506e1',\n",
 " 'bac2b887ff444b47973ac53e6a46a750',\n",
 " '6a3491bfec654805b75447c3933aa1e8',\n",
 " '64fa0a67bd02417a8fefa01af663f5c6',\n",
 " '169dab0a802f41caaf9158bf29bddbca',\n",
 " 'a9017b55105c49d89d58f9d644a294d3',\n",
 " '5f6e98141b924da9a9ac2be497378d4b',\n",
 " '63513d3019eb4a459236710fa30ad9d3',\n",
 " '321bfd0681eb4b9d9e569c1e0bcb76c7',\n",
 " '458256c652524233a0a4f382dec04d18',\n",
 " 'f07eea12f88a4be89f37c2abd8fdd93d',\n",
 " '563982b3019543f6a1c82af3827c7108',\n",
 " 'dde1d6e8c6a54eb5b78902aba8032e79',\n",
 " '2248397bde784f4ead80f6dcaba65a16',\n",
 " '73ba404203034f82b2da97e893bb55c4',\n",
 " '5926228e25be4097b9cbda46712940f8',\n",
 " 'd20fad4428754128a0c66eab804f8e4d',\n",
 " '3d161c16542b403eb3f7c112980e06b2',\n",
 " '376cb78b757a4d58875a0672cddad8d9',\n",
 " '57793fdc24a7471ba77259549b864a7a',\n",
 " '31b69943f2334a149b6b5dfd1e6acf56',\n",
 " '8684945d8a5048d1a7fb2494d1693dc9',\n",
 " '6bf8c135a2f74902a85ea9b4f98c60d6',\n",
 " 'c3349590c0d841c9a517597e4693768c',\n",
 " '6df153e78f704205929e52c58f1bb254',\n",
 " '34fdaff42b6b45018beebed642f103df',\n",
 " '6f311af0ff08402a80fa4d71df8f831e',\n",
 " 'b5d868f8d1e0453fae516dd16f280070',\n",
 " 'db619f55108146eca4a8629b763336fe',\n",
 " '75d0b3b7670b43958500eb99ef9ae351',\n",
 " '1eb88518d1e445dcafb2b0e880407886',\n",
 " '8a5e275031bb4c1a9e7c788e28cf8ef5',\n",
 " '4467af7750de4de5896c91e1d2180527',\n",
 " '83653f4a264f45098f37fef917aa2b79',\n",
 " '8987e58a32e54faa9bc6c3354482717a',\n",
 " '0d6c01a86c3e4d59995293e0fee79b0d',\n",
 " 'ae486325be174e9e9589f2c642ee52f5',\n",
 " '491799f171f648a6bb8626b436a56010',\n",
 " '56c96bd5ecdb4297b4c2ddb8506b5e94',\n",
 " '896144ff30e94d3b828d9b9659cba646',\n",
 " '3f10e76c6cac4a4390c551b41811363d',\n",
 " 'aa86e69c51324577985b838f1ccdaa93',\n",
 " 'bf99188c016c438392b743b95ea32552',\n",
 " 'c28b5bf54f714b2abe16d7c13b177b95',\n",
 " '79db9407198246d0b8ccef8e16b44e81',\n",
 " '3d785984473243fb9c80a2acdbc1c602',\n",
 " '6bf6bed9f00e4adc8cd16960671d8f73',\n",
 " '6bb79d67273d4c6a9ad67d68148d9d30',\n",
 " 'e88315245e2848cf990de686895d86bd',\n",
 " 'fce3bb2c7d7447e7bf9ea18b6e41ac25',\n",
 " 'e78f8b4f824042edb3baf9af365f73b0',\n",
 " '28d96c8549b2430ca0ddab815970912e',\n",
 " '746cfa0587124fb69eee7fb5a321a9eb',\n",
 " 'bef3b9c4e97c4d3b98ad26a3bc825cfb',\n",
 " '74e1105f9b654f42bcc36948bcc20b4e',\n",
 " '3f3dfe1f1edb44f2b7ab67447dadd6ff',\n",
 " '6c4e1f5bc18f406c94df344daffac7d4',\n",
 " '614fbfcb940a411f95290949c068182e',\n",
 " '140e1dc9a447cb1585bd87357c38f',\n",
 " 'fc0c3ad671aa45989cd59aa91c0f0669',\n",
 " '5a3557f481634de2ac2819878a607760',\n",
 " 'bf8cf6cf0f1e42ff9dfdff1f34ddde14',\n",
 " 'c0897bbe573d436ba9740d7ceb55795a',\n",
 " 'fc54073389c649cbb741a8f54f7f0b7f',\n",
 " 'a7d12c579b1241fc89c312b1dc3dd9d7',\n",
 " 'eff76951d50d45ba921f8e832a2f3dba',\n",
 " 'dbf8cc9ed59a449c896ab2ac01157cb9',\n",
 " '3cef4c09046f4d66831bb00f064d6f98',\n",
 " 'e5674a2689cd4ccbba53fc41ea4d0816',\n",
 " '61c956d0169c4f92a0d6cc6855e1fb8f',\n",
 " '1718106b1a79431fb71fbc9b28ae82ec',\n",
 " '094949e0858544f2a18efb8776e34670',\n",
 " '556507807977438d94277c5a3245f3a6',\n",
 " '420db01d6e714e62a65731176649e86d',\n",
 " 'f28947b003934c2c89184b2f97912b92',\n",
 " '0478e28b13794aa4afeaf6f4783cbbf4',\n",
 " '22e270bd82824e2db7e9fac902231760',\n",
 " '764cf37f34d94a93b17631e58f63dc6f',\n",
 " '0abf067d32cf4e7081eac83e792dcb66',\n",
 " '15e636ff29874c43ad096faf3623531a',\n",
 " 'b98a414f05614d1e80ac0ef1b9c4b65e',\n",
 " 'd0f5b536ebdc4b81920015d9e85b9ac1',\n",
 " '36bb368d3d9f4481aa1b9efca2c91ea8',\n",
 " 'a9af55aa2f4948bb9829876e4b4316dc',\n",
 " 'b0465b54694e49e9933e12579b1b68d1',\n",
 " 'a4f9c390847744ebb9c58cc34e001b44',\n",
 " 'fc1ff87bb5c84c508df2babb4c0ac296',\n",
 " 'e9f6221db3fa4a7f87eb4293e93776a8',\n",
 " '04e42aef6ff948b1a5924d0d1b3f0fd0',\n",
 " '1db2a397db6a4ad0a9478694dc1b6420',\n",
 " 'd3cf85a87b4b491abcef1af170eea848',\n",
 " '4b2c5074f71f468b91c85548b992b35a',\n",
 " '1c6b6951e5a94a1799e3e465efc60340',\n",
 " 'f17313a7fa0d4c48a3fbf11b78a82ae2',\n",
 " '569b5c15083e4a81a9b63270017da4ca',\n",
 " 'fc9d21520bda4c6e8cfe6621d200d132',\n",
 " '97d2025be0814d9085e40d9d8ec45fe5',\n",
 " '20826f12bb114e39942a48814414a7c9',\n",
 " '7d2dfba5cbf64df9b335617c8b7501df',\n",
 " '84831e29922642a0b5b06c945a173614',\n",
 " '7703c5ee05a2426f9a3523095cbde1d3',\n",
 " '5e180d3f95ef4a4cbf7c17689e7ab712',\n",
 " '4dc14fd3ebb44b68894c00dbe7a47f7c',\n",
 " '98d0944a455c488f9a5c723e8e643325',\n",
 " '2fd3e4920ffa475f978dc1183144b040',\n",
 " '514a1db579ce451f8902efa9dc814413',\n",
 " 'e864c03e96d2444b8e4e4f69d4db1078',\n",
 " 'aaecbbdb92ad47cd9669de1ebb1d74ea',\n",
 " 'bd2ea575ff3e4650bfb13690220a2769',\n",
 " '61be9c2fb5454dcebcbdb65a57d08fe1',\n",
 " 'f8b1c083d9dd4e75a4a510c6d212f80c',\n",
 " '8832683044ca427a9588f3478877032b',\n",
 " '97f04f676225440ca39b9f8f14144a01',\n",
 " 'ad645b894ba64ed49a906950d448cc91',\n",
 " '8269510d3f7b479c9bef20e7113e8e0f',\n",
 " '4a5813c965144aa4a6c8cc72aa66ed7e',\n",
 " '029df2c3098147f092667fc567c13e1d',\n",
 " '8408777302e9470f95ce392f9f680639',\n",
 " '17ad8f5402b147b29307524a623fbc3d',\n",
 " '64e03a2dfb684f8a8f12141462eeb95e',\n",
 " '98bc6f331863478da89e4c4e2a01bb01',\n",
 " 'a7ecd9a1e8d347cb8fd3a75b17d4312a',\n",
 " 'e49667c6bdd04c38970000ffac351cfc',\n",
 " '55b401fc88834fe79c92f56624909459',\n",
 " '01123ee8758f4f5b80f002040944ea0d',\n",
 " '62110168b1254548bb6a1b65027a0323',\n",
 " 'caf080e62c694424a268d9d6d3d1979a',\n",
 " '802568daa29148258b905848945e3238',\n",
 " '0744a1492ee74d2b8f6c538a0cb318ac',\n",
 " '4a459a4cbad34934a1a388a340b536ed',\n",
 " 'a597e8a942414506b6ee587fb07ef2e3',\n",
 " 'a0b072b2837342e3b90ef1536b091ab9',\n",
 " '98bc6402e9534fd7a19d713147339c25',\n",
 " '2cffc808009d45a886d702d9a193db34',\n",
 " '8ca148484be047d7a5df0e40bf3bf6e9',\n",
 " 'b03c01ae4dc34729b4340909d4dbbad1',\n",
 " '0370684596004024a73928f19435271a',\n",
 " '6e300db2cb0f4b0b9ada5346e1112561',\n",
 " '447d66c4b8c74a6396336d6d9e5ff2ec',\n",
 " '2b75907329504ccc824504f29bf34259',\n",
 " 'e2ec24830dc641b3b916a4a0d2124fb1',\n",
 " '3d8fa196ef0544248ab1f21ba258ac26',\n",
 " '4167e5d6003c43139b3083cff7c7e847',\n",
 " '94a18a3d667b457fa999de47ad1d2ece',\n",
 " '2f1d921dd7cf4110b2a0712ef7b3a488',\n",
 " 'ec97c458896f46b18cbd5c8e27868258',\n",
 " '2196799e39a943ee8c6f2eb20c8d2eab',\n",
 " '13ff1d8f823549ebbf6918d56dadb70a',\n",
 " '5187394740fa441f87d361f5c2f7df4b',\n",
 " '5bc3530d94924953b00e736a9f1a7a24',\n",
 " '6c4b8fdbe42e4194948097d4f3eb5c39',\n",
 " 'dc8b2b0c511b4d4a98b9ef8719417ce8',\n",
 " '23a45f0b2f7d47e4800a504ca4cb2d2b',\n",
 " 'f82a6b63fba14e248ff4ea7cc43be442',\n",
 " 'e34a657805d249a682f6c0364f4c6206',\n",
 " 'ed781324dddd456fb40a4368dba87e7d',\n",
 " '22bd1c718acf49e78c3471f7b58df35d',\n",
 " 'c94281e0a57e4db7ba0c3a387e89aeaa',\n",
 " '03e3326bf56f4e638743aec093b8579d',\n",
 " '194ea44b818b4d619a7535c37ac347b6',\n",
 " 'a0dca5c51c6648619c920687bb1247ff',\n",
 " '3e585ad4ccc14d98bdec9ac93c18bf2a',\n",
 " 'b939c60573224c4096a4f8d91f2ab001',\n",
 " '193a0a6004604592bb55476a69c2a75a',\n",
 " 'c56c7edddb8c45c3ab70f95e198e60df',\n",
 " '765cde0990074349a8a404bb0cc3ba2d',\n",
 " 'a79154ae12cb4467abf74ebaf51a6604',\n",
 " '10d4a60ed6ca4c34996189c7161e592b',\n",
 " 'b40805dbf3364af4b61187eedbc971ac',\n",
 " '643dc00dc8f7442a8d9a2415e6499bc9',\n",
 " '3106487ba04d494082a2613b36be7565',\n",
 " '60a8308663d34ad6ac863988c5162a9f',\n",
 " '7ce1ea25ac354ee3a7c38a59d73e4309',\n",
 " '4e17930c79f84b07804a89eeec6dc416',\n",
 " '90785fcdc956476a9db846226baf017d',\n",
 " '7c173f27a19741e2aec5654599a328f6',\n",
 " 'd52645e810564ce19b393c0bf55a9a54',\n",
 " '40999a59a307462fb8fe5cf6f32bb192',\n",
 " 'ae7295a65e2f46b18fe587cc6a8ea9ac',\n",
 " '410f58333f3547d99cd7faffe2d4782c',\n",
 " 'e5c7a3d9b7b240a7b2c8f7628abfcc8f',\n",
 " 'a0cf9ac4ce784c0da76b211d3a1b223a',\n",
 " '39ecf2a806674ed9bc44733e183e86e2',\n",
 " 'b098bd54a8be4e82a95d25bc9a2457e5',\n",
 " '60be126392724d1d8ff7c9c2b25ce3eb',\n",
 " '5eb8959f52ec4e46bc47a86c8bb728c2',\n",
 " 'ff6264da93c949c3b11579a71ab446e6',\n",
 " '8091b4386bae432089cff8c43bbac541',\n",
 " 'e06364d5aec244268e36cc7ee9f66680',\n",
 " '866f72d086e44a8cb332b70b22365b26',\n",
 " 'cacf4f41654e443999bdd742c27b59ba',\n",
 " '11a5d0a0a4d8446bbb6dc3ced6891450',\n",
 " '4025306406cd4d5a96d16400debd02f8',\n",
 " '077da7fd83c84376918f737a1995fd94',\n",
 " 'e82a36f9a4b94b489c59bfd153379214',\n",
 " '6071ce3d85c14402807dbca5554f4fcd',\n",
 " '059baa0db81143caa0db35a5478dd70a',\n",
 " '27a0c0bc2a7e419dafeeb4c0b6dba31a',\n",
 " 'bb6d041358b143a7b4b0066a7491d86e',\n",
 " '7f213a6d0c204fe7a69e9728ed1eb4ad',\n",
 " 'fb4db788e90b4bf8b3c957966bbb60b2',\n",
 " '3f1549ac317a4cf99295663684caf4a2',\n",
 " '9393197b53804fa2b8366e4ba849c863',\n",
 " '43d4172dfb6e49aeae816136597d40c9',\n",
 " 'f605a2ec349a461ba735bb0928c47654',\n",
 " 'ff72a1052f7b469787ceae536383c9d7',\n",
 " '6ef615ccc1f34715880c9851e20eda62',\n",
 " '0b46f6309618413ebd9858991f9206e0',\n",
 " '2b2e72363d0147c79c946763b8018878',\n",
 " '14322a4521084ffb8513dec17ba8009e',\n",
 " '9cc6c74d0cfc4205be7d4cdb392f2902',\n",
 " '7bf7aeaf6a994a9b8d4ecfac4e5e5697',\n",
 " '04c5574656d84fa3b3d55dbef9b4603a',\n",
 " '4d43de99c685467ba276076b55be768e',\n",
 " '75e9658a2f97440791f511fe524fed5d',\n",
 " 'a1b87320124f4e49892bffe2620613fe',\n",
 " '13329284abca437880a311aa1e8df82d',\n",
 " '70252bc2bdb94b27b1e594af42347aec',\n",
 " 'e214b78ae82c45e6a8bab0fd247401cd',\n",
 " 'd8928dca6b834cd5928fed90c9bce54e',\n",
 " '0fa1185202cf49afa3396531d03bc9ac',\n",
 " 'bcef7030583248f8ab6df3eb103aece3',\n",
 " '7fe0cbf573fc44a598bc40c9c7182f27',\n",
 " '237b1f6122b14ceeb6e36fbef397f398',\n",
 " '68336e118d314540a59ed9483e543d24',\n",
 " '49d09e03cbae497aa7bbc0dc2b8052b3',\n",
 " '308fbefa2e514c8d8a914b6f0f72734d',\n",
 " 'ccfba187f68c4eb8a8c1a8280540912c',\n",
 " '4ea7126d27fe4599baf908ed4616f8e8',\n",
 " 'beae9839d01d49df926fb58b5b63bf26',\n",
 " '5af73970d1af466f872efa121d552927',\n",
 " '9aa8fe3055f54e61aa71ed6c5ff43fd6',\n",
 " 'c5b3517e87134d7b9eec7e39e5ba67d6',\n",
 " 'ab04454b43d94081a78219283b8899a9',\n",
 " '3d798d5dcd8f4d8f9a7b87a61ccefbca',\n",
 " '85fc786718d34f3b85cb96a7b6f3c5f7',\n",
 " 'e566db7af9cf4e3c98b765d0b0b6f53d',\n",
 " '82c22d883abc4f62851eda6ea3e820ae',\n",
 " '04124ea5262d436e8b287d46cadf3f74',\n",
 " 'e65fc25c34444366835b988fc373c9ae',\n",
 " 'c3225549659e4c9a8a4b18c4f1e1f6a2',\n",
 " '8cfce9815d65429aa417445104671220',\n",
 " 'e56854ae312f44bf8a492ef70c7e8274',\n",
 " '7c4e887ff81145e3991346a8a1747b24',\n",
 " 'd50c46633a8c4b3db50c3c43f761928d',\n",
 " '7b029453a1fa4ad9a8ec159af9b17607',\n",
 " 'fa984c2db4df49c0bffb3ec6ba40f4d7',\n",
 " 'f368f2b53197463585f28f7517097b56',\n",
 " '5433a49942304b1393f807cdd00aa114',\n",
 " '5a232272640349a2c9429c91b53f7',\n",
 " '7c537d413bf34c3984c70874b393842d',\n",
 " '630f625e5661471b9f7f8b9f2308a0af',\n",
 " '1b2393be14eb42f9a5101ff6f1f5cb4f',\n",
 " 'e0df4bef8aeb467884f16874a44f24b8',\n",
 " '17dc7192e2af479a96d6507c3b5cbad6',\n",
 " 'c314ec19687448ac82855951bbf3fc7f',\n",
 " '8a137808da654acf9451d50f47a9e44f',\n",
 " '2cb3c8fa34cb4958a06353dc7b0302fd',\n",
 " 'a74c32b0317f4f3893a44bdcb5f30466',\n",
 " '35e117c951314a9a858882169cf937d4',\n",
 " '638a3de3dcec4eb9987c98550e8d5337',\n",
 " 'd06ddac8b9f549f4a3aed2539bf40a41',\n",
 " '7b7daa11264140de8140cad00263dc1a',\n",
 " '54c6a5f43aa043c4897351510e354523',\n",
 " '7eb5c97e492d4d408d99171b09f0786d',\n",
 " 'e7f92c41a80e4c7390aec23f082f2554']"
 ]
 },
 "metadata": {},
 "execution_count": 74
 }
 ],
 "source": "web_loader = WebBaseLoader(\n web_paths=(\"https://lilianweng.github.io/posts/2020-10-29-odqa/\",\n \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",\n \"https://lilianweng.github.io/posts/2018-06-24-attention/\",\n \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\",\n \"https://lilianweng.github.io/posts/2024-07-07-hallucination/\"),\n\n bs_kwargs=dict(\n parse_only=bs4.SoupStrainer(\n class_=(\"post-content\", \"post-title\", \"post-header\")\n )\n ),\n)\n\nweb_documents = web_loader.load()\n\nfor idx, text in enumerate(web_documents):\n web_documents[idx].metadata['doc_num'] = global_doc_number\n web_documents[idx].metadata['doc_source'] = \"WWW\"\n global_doc_number += 1\n\nprint('Number of documents: ', len(web_documents))\n\nweb_splits = text_splitter.split_documents(web_documents)\n\nfor idx, text in enumerate(web_splits):\n web_splits[idx].metadata['split_id'] = idx\n\nprint('Number of splits: ', len(web_splits))\n\nqdrant_vectorstore.add_documents(documents=web_splits)"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "D2Y_wjQGW6l3"
 },
 "source": "## Step 3: LLM"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "V5JtdsIAW6l3"
 },
 "source": "### 3.1: Mistral"
 },
 {
 "cell_type": "code",
 "execution_count": 75,
 "metadata": {
 "colab": {
 "base_uri": "https://localhost:8080/",
 "height": 0,
 "referenced_widgets": [
 "ad4f6a07ecd248b79cacfc7ef83a3d1c",
 "e32200249e7c4d3e85ef2653456aa560",
 "4bd2d1d86e8844d391c91ea02d96cde7",
 "cfa9d46a31734cffbc488a15203fc2ae",
 "c282f0f2c67145d5bfac5bc0c4b21c84",
 "6985c0ecd8c643af89be0a849c849db3",
 "861900cf7c724daa810e64ab286750b6",
 "25dbb4477693457e90d52cbe694b3f26",
 "9e5109aa85f848a9a8fde9fbd829a1ea",
 "69b212b297ae4dc1b7198f0bd687f7e7",
 "5ca6bf272a9b4fe7ab539cee14265ff6",
 "aee68dd9066e48e5b5642c2bb1aee228",
 "28c27d29978e4478b1c9a6b97275a936",
 "3370563daf9847bba12608d30c3bd826",
 "066e509d01894a61bbd9828290876811",
 "9a24014fbf8f467d8fec856855d70088",
 "d9068e76323d490389d8161496f19326",
 "24e5884ef9eb4207843d55c5f53ab45d",
 "34a8f8da7d01480f92cf5d2758161d60",
 "98db90ab654847bd81c929df5c9cc620",
 "9573427506e0455fa1534eca766320a8",
 "1dc69664a4844e71baa9c7e0e3c7195f",
 "ee3a715fd0a04f8196a634d8952fee13",
 "211050f71cc24786a13cbbee3042074f",
 "58c9ebc16374407d8b4a5e92f3590faf",
 "0fd37791e2c24960b3ec87db6d1b5654",
 "7b59aed4e88f42dab5fdef583b01fd7d",
 "b196199206a34657ae972ce1a0f6f8d2",
 "3b04b6185c4e41f1b11b80892c6dc3a0",
 "c0343fb543954a5c948ce30e72c057fb",
 "f5651379546d4d9da64111938b7d9449",
 "e3cb1dda1c3f4de4b752e5a4608a18e9",
 "0a4ada2df78d4770b5e373e2bd194c08",
 "e32d7fc81a1b4abda7c687c244d9e601",
 "f61d4fed82014f429f3379f11cb8c733",
 "f7ed551041754535896aca95679ed170",
 "a8d6691ad2de4c7493b95f3e6e77b50d",
 "ca9d1a9b532642c0b89805b8c3086c81",
 "2e22177f31b14107a0eaa336709b2791",
 "9f2a021a76084da2b10120e700dc76e7",
 "0eb59101835a4b1da4bee3fa1a7bb0da",
 "76c685ca1b9c4ca8b53fcd8fddbbaef0",
 "3d8376b6671e4530be0b88c28ef5939a",
 "c5c04afbcfef42c68cf4cfa88836a000"
 ]
 },
 "executionInfo": {
 "elapsed": 92373,
 "status": "ok",
 "timestamp": 1765128597162,
 "user": {
 "displayName": "Ryan C",
 "userId": "13217471319505539255"
 },
 "user_tz": 420
 },
 "id": "iQPFwAVXW6l3",
 "outputId": "21180638-bb75-4cf2-8ff2-6db9c89cdf63"
 },
 "outputs": [
 {
 "output_type": "stream",
 "name": "stdout",
 "text": [
 "Loading model from Google Drive...\n"
 ]
 },
 {
 "output_type": "stream",
 "name": "stderr",
 "text": [
 "/usr/local/lib/python3.12/dist-packages/transformers/quantizers/auto.py:239: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.\n",
 " warnings.warn(warning_msg)\n"
 ]
 },
 {
 "output_type": "display_data",
 "data": {
 "text/plain": [
 "tokenizer_config.json: 0.00B [00:00, ?B/s]"
 ],
 "application/vnd.jupyter.widget-view+json": {
 "version_major": 2,
 "version_minor": 0,
 "model_id": "ad4f6a07ecd248b79cacfc7ef83a3d1c"
 }
 },
 "metadata": {}
 },
 {
 "output_type": "display_data",
 "data": {
 "text/plain": [
 "tokenizer.model: 0%| | 0.00/587k [00:00<?, ?B/s]"
 ],
 "application/vnd.jupyter.widget-view+json": {
 "version_major": 2,
 "version_minor": 0,
 "model_id": "aee68dd9066e48e5b5642c2bb1aee228"
 }
 },
 "metadata": {}
 },
 {
 "output_type": "display_data",
 "data": {
 "text/plain": [
 "tokenizer.json: 0.00B [00:00, ?B/s]"
 ],
 "application/vnd.jupyter.widget-view+json": {
 "version_major": 2,
 "version_minor": 0,
 "model_id": "ee3a715fd0a04f8196a634d8952fee13"
 }
 },
 "metadata": {}
 },
 {
 "output_type": "display_data",
 "data": {
 "text/plain": [
 "special_tokens_map.json: 0%| | 0.00/414 [00:00<?, ?B/s]"
 ],
 "application/vnd.jupyter.widget-view+json": {
 "version_major": 2,
 "version_minor": 0,
 "model_id": "e32d7fc81a1b4abda7c687c244d9e601"
 }
 },
 "metadata": {}
 },
 {
 "output_type": "stream",
 "name": "stderr",
 "text": [
 "Device set to use cuda\n"
 ]
 }
 ],
 "source": "#Quantization config\n\nquantization_config = BitsAndBytesConfig(\n load_in_4bit=True,\n bnb_4bit_quant_type=\"nf4\",\n bnb_4bit_use_double_quant=True,\n bnb_4bit_compute_dtype=torch.bfloat16\n)\n\n# RC - Modified to store and download\n# Download and save to Google Drive (If running from Colab)\n\nmodel_save_path = \"/content/drive/MyDrive/models/mistral-7b-instruct\"\n\n# Check if already saved\nif os.path.exists(model_save_path):\n print(\"Loading model from Google Drive...\")\n model_source = model_save_path\nelse:\n print(\"Downloading model from Hugging Face (this will take ~15 min)...\")\n model_source = \"mistralai/Mistral-7B-Instruct-v0.3\"\n\n# Load from appropriate source\nllm_mistral_model = AutoModelForCausalLM.from_pretrained(\n model_source,\n dtype=torch.float32,\n device_map='cuda',\n quantization_config=quantization_config\n)\n\nllm_mistral_tokenizer = AutoTokenizer.from_pretrained(model_source)\n\n# If we downloaded, save it for next time\nif model_source != model_save_path:\n print(\"Saving model to Google Drive for future use...\")\n llm_mistral_model.save_pretrained(model_save_path)\n llm_mistral_tokenizer.save_pretrained(model_save_path)\n print(\"Saved!\")\n\nllm_mistral_tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.3\")\n\nmistral_pipe = pipeline(\n \"text-generation\",\n model=llm_mistral_model,\n tokenizer=llm_mistral_tokenizer,\n max_new_tokens=1000,\n temperature=0.6,\n top_p=0.95,\n do_sample=True,\n repetition_penalty=1.2\n)\nmistral_pipe.model.config.pad_token_id = mistral_pipe.model.config.eos_token_id\n\nmistral_llm_lc = HuggingFacePipeline(pipeline=mistral_pipe)"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "X0nsNRiYW6l3"
 },
 "source": "### 3.2: Cohere"
 },
 {
 "cell_type": "code",
 "execution_count": 76,
 "metadata": {
 "id": "EbBM0XGNW6l3",
 "executionInfo": {
 "status": "ok",
 "timestamp": 1765128597863,
 "user_tz": 420,
 "elapsed": 699,
 "user": {
 "displayName": "Ryan C",
 "userId": "13217471319505539255"
 }
 }
 },
 "outputs": [],
 "source": "cohere_chat_model = ChatCohere(cohere_api_key=COHERE_API_KEY)"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "tAIl7l-pW6l3"
 },
 "source": "## 4: Retrieval Chain"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "WdqLYjDFW6l4"
 },
 "source": "### 4.1: Prompt"
 },
 {
 "cell_type": "code",
 "execution_count": 77,
 "metadata": {
 "id": "LCeNyVkdW6l4",
 "executionInfo": {
 "status": "ok",
 "timestamp": 1765128597866,
 "user_tz": 420,
 "elapsed": 1,
 "user": {
 "displayName": "Ryan C",
 "userId": "13217471319505539255"
 }
 }
 },
 "outputs": [],
 "source": "# persona = 'marketing'\n# engineering_template = \"\"\"[INST]You are a technical AI assistant helping engineers understand complex AI and NLP concepts.\n\n# Based on the context below, provide a detailed, technical answer to the question. Include specific details, technical terms, and explanations.\n\n# Context:\n# {context}\n\n# Question: {question}\n\n# Provide a comprehensive technical answer:[/INST]\"\"\"\n\n# marketing_template = \"\"\"[INST]You are an AI assistant helping marketing professionals understand AI concepts for business communication.\n\n# Based on the context below, provide a clear, high-level answer to the question. Keep it accessible and focus on practical applications.\n\n# Context:\n# {context}\n\n# Question: {question}\n\n# Provide a clear, business-focused answer:[/INST]\"\"\"\n\n# # Select template based on persona\n# template = engineering_template if persona == \"engineering\" else marketing_template\n# rag_prompt = ChatPromptTemplate.from_template(template)\n\n# print(f\"Using {persona.upper()} persona prompt\")"
 },
 {
 "cell_type": "code",
 "execution_count": 78,
 "metadata": {
 "id": "7q9kuj5yVZJQ",
 "executionInfo": {
 "status": "ok",
 "timestamp": 1765128597868,
 "user_tz": 420,
 "elapsed": 1,
 "user": {
 "displayName": "Ryan C",
 "userId": "13217471319505539255"
 }
 }
 },
 "outputs": [],
 "source": "# Prompt version 1\n# persona = 'marketing'\n# engineering_template = \"\"\"[INST]You are a technical AI assistant helping engineers understand complex AI and NLP concepts.\n\n# Based on the context below, provide a detailed, technical answer to the question. Include specific details, technical terms, and explanations.\n\n# Context:\n# {context}\n\n# Question: {question}\n\n# Provide a comprehensive technical answer:[/INST]\"\"\"\n\n# marketing_template = \"\"\"[INST]You are an AI assistant helping marketing professionals understand AI concepts for business communication.\n\n# Based on the context below, provide a clear, high-level answer to the question. Keep it accessible and focus on practical applications.\n\n# Context:\n# {context}\n\n# Question: {question}\n\n# Provide a clear, business-focused answer:[/INST]\"\"\"\n\n# engineering_rag_prompt = ChatPromptTemplate.from_template(engineering_template)\n# marketing_rag_prompt = ChatPromptTemplate.from_template(marketing_template)"
 },
 {
 "cell_type": "code",
 "execution_count": 79,
 "metadata": {
 "id": "71yqTE4P7GE3",
 "executionInfo": {
 "status": "ok",
 "timestamp": 1765128597884,
 "user_tz": 420,
 "elapsed": 2,
 "user": {
 "displayName": "Ryan C",
 "userId": "13217471319505539255"
 }
 }
 },
 "outputs": [],
 "source": "#Prompt version 2\npersona = 'marketing'\n\nengineering_template = \"\"\"[INST]You are a technical AI assistant helping engineers understand AI and NLP concepts.\n\nUsing ONLY the context provided below, answer the question with technical precision. Your answer must:\n- Be 4-5 sentences (approximately 500 characters)\n- Use precise technical terminology\n- Focus on implementation details and architectures\n- Stay grounded in the provided context\n\nIf the context doesn't contain sufficient information to answer the question, state what information is missing.\n\nContext:\n{context}\n\nQuestion: {question}\n\nProvide a concise, technical answer (4-5 sentences):[/INST]\"\"\"\n\nmarketing_template = \"\"\"[INST]You are an AI assistant helping marketing professionals understand AI concepts for business communication.\n\nUsing ONLY the context provided below, answer the question in accessible language. Your answer must:\n- Be 2-3 sentences (approximately 250 characters)\n- Avoid technical jargon\n- Focus on business value and practical applications\n- Stay grounded in the provided context\n\nIf the context doesn't contain sufficient information to answer the question, state what information is missing.\n\nContext:\n{context}\n\nQuestion: {question}\n\nProvide a brief, business-focused answer (2-3 sentences):[/INST]\"\"\"\n\nengineering_rag_prompt = ChatPromptTemplate.from_template(engineering_template)\nmarketing_rag_prompt = ChatPromptTemplate.from_template(marketing_template)"
 },
 {
 "cell_type": "code",
 "execution_count": 80,
 "metadata": {
 "id": "LRX_A1gWd7YM",
 "executionInfo": {
 "status": "ok",
 "timestamp": 1765128597886,
 "user_tz": 420,
 "elapsed": 1,
 "user": {
 "displayName": "Ryan C",
 "userId": "13217471319505539255"
 }
 }
 },
 "outputs": [],
 "source": "# #Prompt version 3 with few-shot examples\n\n# persona = 'marketing'\n\n# engineering_template = \"\"\"[INST]You are a technical AI assistant helping engineers understand AI and NLP concepts.\n\n# Using ONLY the context provided below, answer the question with technical precision. Your answer must:\n\n# - Be 4-5 sentences (approximately 500 characters)\n# - Use precise technical terminology\n# - Focus on implementation details and architectures\n# - Stay grounded in the provided context\n\n# If the context doesn't contain sufficient information to answer the question, state what information is missing.\n\n# Examples of good engineering answers:\n\n# Example 1:\n# Question: How can a language model's ability to analyze images expand its range of applications?\n# Answer: One way a language model's ability to analyze images can expand its range of applications is by enabling it to perform tasks that require both textual and visual information, such as image captioning or visual question answering. By incorporating image features into the model's input, it can generate more contextually relevant and accurate responses. This integration of image analysis can also enhance the model's performance in tasks like content generation, recommendation systems, and sentiment analysis, where visual cues play a significant role in understanding and interpreting the data. Additionally, combining language and image processing capabilities can lead to more sophisticated and versatile AI systems that can handle a wider range of real-world applications effectively.\n\n# Example 2:\n# Question: What is the purpose of using a regularization term (like the epsilon multiplied by the identity matrix) in the linear regression model described in the code?\n# Answer: The purpose of using a regularization term (epsilon multiplied by the identity matrix) in the linear regression model described in the code is to prevent overfitting by adding a penalty term to the loss function. This regularization term helps to control the complexity of the model by discouraging large coefficients, leading to a more stable and generalizable solution. By incorporating this regularization, the model can better handle noise and outliers in the data, ultimately improving its performance on unseen data points.\n\n# Example 3:\n# Question: How does the Pointer Network differ from standard seq2seq models in dealing with sequential data?\n# Answer: The Pointer Network differs from standard seq2seq models in that it is specifically designed to handle problems where the output elements correspond to positions in an input sequence. Instead of using attention to blend hidden units like in seq2seq models, the Pointer Network directly points to the elements in the input sequence. This allows for greater flexibility in determining the output elements, which is particularly useful in tasks like sorting or the traveling salesman problem where the output categories are not predetermined. The architecture of a Pointer Network model includes an encoder-decoder setup with a focus on predicting or inferring specific elements in the input sequence based on their importance weights.\n\n# Context:\n# {context}\n\n# Question: {question}\n\n# Provide a concise, technical answer (4-5 sentences):[/INST]\"\"\"\n\n# marketing_template = \"\"\"[INST]You are an AI assistant helping marketing professionals understand AI concepts for business communication.\n\n# Using ONLY the context provided below, answer the question in accessible language. Your answer must:\n\n# - Be 2-3 sentences (approximately 250 characters)\n# - Avoid technical jargon\n# - Focus on business value and practical applications\n# - Stay grounded in the provided context\n\n# If the context doesn't contain sufficient information to answer the question, state what information is missing.\n\n# Examples of good marketing answers:\n\n# Example 1:\n# Question: How can a language model's ability to analyze images expand its range of applications?\n# Answer: By incorporating image analysis capabilities, a language model can be used for tasks such as image captioning, visual question answering, and text-to-image generation, expanding its range of applications beyond just text-based tasks.\n\n# Example 2:\n# Question: What is the purpose of using a regularization term (like the epsilon multiplied by the identity matrix) in the linear regression model described in the code?\n# Answer: The purpose of using a regularization term in the linear regression model is to mitigate or resolve catastrophic extrapolation error, making the extrapolated values comparable to those within the original range.\n\n# Example 3:\n# Question: How does the Pointer Network differ from standard seq2seq models in dealing with sequential data?\n# Answer: The Pointer Network differs from standard seq2seq models by being able to handle problems where the output elements correspond to positions in an input sequence, rather than using attention to blend hidden units.\n\n# Context:\n# {context}\n\n# Question: {question}\n\n# Provide a brief, business-focused answer (2-3 sentences):[/INST]\"\"\"\n\n# engineering_rag_prompt = ChatPromptTemplate.from_template(engineering_template)\n# marketing_rag_prompt = ChatPromptTemplate.from_template(marketing_template)"
 },
 {
 "cell_type": "code",
 "execution_count": 81,
 "metadata": {
 "id": "56oBhDA-nAXK",
 "executionInfo": {
 "status": "ok",
 "timestamp": 1765128597888,
 "user_tz": 420,
 "elapsed": 1,
 "user": {
 "displayName": "Ryan C",
 "userId": "13217471319505539255"
 }
 }
 },
 "outputs": [],
 "source": "def format_docs(docs):\n return \"\\n\\n\".join(doc.page_content for doc in docs)\n\noutput_parser = StrOutputParser()"
 },
 {
 "cell_type": "code",
 "execution_count": 82,
 "metadata": {
 "id": "81Iq7YRLVtnA",
 "executionInfo": {
 "status": "ok",
 "timestamp": 1765128597897,
 "user_tz": 420,
 "elapsed": 2,
 "user": {
 "displayName": "Ryan C",
 "userId": "13217471319505539255"
 }
 }
 },
 "outputs": [],
 "source": "# Build RAG chain\nmistral_marketing_rag_chain = (\n {\"context\": retriever | format_docs,\n \"question\": RunnablePassthrough()}\n | marketing_rag_prompt\n | mistral_llm_lc\n | output_parser\n)\n\nmistral_engineering_rag_chain = (\n {\"context\": retriever | format_docs,\n \"question\": RunnablePassthrough()}\n | engineering_rag_prompt\n | mistral_llm_lc\n | output_parser\n)\n\ncohere_marketing_rag_chain = (\n {\"context\": retriever | format_docs,\n \"question\": RunnablePassthrough()}\n | marketing_rag_prompt\n | cohere_chat_model\n | output_parser\n)\n\ncohere_engineering_rag_chain = (\n {\"context\": retriever | format_docs,\n \"question\": RunnablePassthrough()}\n | engineering_rag_prompt\n | cohere_chat_model\n | output_parser\n)\n\nchains = {\n 'marketing_mistral': mistral_marketing_rag_chain,\n 'marketing_cohere': cohere_marketing_rag_chain,\n 'engineering_mistral': mistral_engineering_rag_chain,\n 'engineering_cohere': cohere_engineering_rag_chain\n}"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "jJE1wexvV7ay"
 },
 "source": "## 4. Tests & Evaluations\n\nHere you should evaluate the results. First, you should implement your evaluation metrics and then you should run evaluation tests. This is really your area, but key results to show are:\n\n1) Your metrics of choice \n2) How your various models compare to the labeled validation data.\n\nMake sure you look at the results for the marketing team and the research team separately.\n\n**Note:** You do not need to run all models against all labeled questions, as that may take some time. Just do that for a few models/configs, and test a larger set with a smaller subset. But if you use a subset you must justify why you are using that specific subset of questions.\n\n**This is free form so you will need to create your own cells, text documentation as you need, etc.**\n\nAfter you have implemented you evaluation strategy please answer the questions below in sections 4.1 and 4.2.\n\nPlease feel free to add more text and code cells as needed."
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "Ygbu2D330obV"
 },
 "source": "### RAG Function Setup"
 },
 {
 "cell_type": "code",
 "execution_count": 84,
 "metadata": {
 "id": "gTl1-kpn45Xg",
 "executionInfo": {
 "status": "ok",
 "timestamp": 1765128610642,
 "user_tz": 420,
 "elapsed": 109,
 "user": {
 "displayName": "Ryan C",
 "userId": "13217471319505539255"
 }
 }
 },
 "outputs": [],
 "source": "from bert_score import score"
 },
 {
 "cell_type": "code",
 "execution_count": 85,
 "metadata": {
 "id": "ovZS06PqEXDJ",
 "executionInfo": {
 "status": "ok",
 "timestamp": 1765128610671,
 "user_tz": 420,
 "elapsed": 25,
 "user": {
 "displayName": "Ryan C",
 "userId": "13217471319505539255"
 }
 }
 },
 "outputs": [],
 "source": "def ask_rag_question(question_id, chain_type='marketing_mistral', retrieved_contexts=None):\n question = validation_questions_answers[question_id]['question']\n\n if retrieved_contexts is None:\n retrieved_docs = retriever.invoke(question)\n retrieved_contexts = [doc.page_content for doc in retrieved_docs]\n\n response = chains[chain_type].invoke(question)\n\n if 'cohere' in chain_type:\n rag_answer = response.strip()\n else:\n if \"[/INST]\" in response:\n rag_answer = response.split(\"[/INST]\", 1)[1].strip()\n else:\n rag_answer = response.strip()\n\n persona = chain_type.split('_')[0]\n\n if persona == 'marketing':\n gold_answer = validation_questions_answers[question_id]['gold_answer_marketing']\n else:\n gold_answer = validation_questions_answers[question_id]['gold_answer_research']\n\n return {\n 'question_id': question_id,\n 'question': question,\n 'response': response,\n 'rag_answer': rag_answer,\n 'gold_answer': gold_answer,\n 'retrieved_contexts': retrieved_contexts\n }"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "1h6YR5uWvP3Y"
 },
 "source": "### Evaluation with Bertscore"
 },
 {
 "cell_type": "code",
 "execution_count": 86,
 "metadata": {
 "id": "oJCpXmzk08Qs",
 "executionInfo": {
 "status": "ok",
 "timestamp": 1765128610700,
 "user_tz": 420,
 "elapsed": 14,
 "user": {
 "displayName": "Ryan C",
 "userId": "13217471319505539255"
 }
 }
 },
 "outputs": [],
 "source": "def evaluate_with_bertscore(rag_response, scorer=None):\n \"\"\"\n Evaluates RAG-generated answer against gold standard using BERTScore\n scorer: Optional pre-loaded BERTScorer instance for better performance\n \"\"\"\n rag_answer = rag_response['rag_answer']\n gold_answer = rag_response['gold_answer']\n\n if scorer is not None:\n P, R, F1 = scorer.score([rag_answer], [gold_answer])\n precision, recall, f1 = P.item(), R.item(), F1.item()\n else:\n from bert_score import score\n bert_score = score([rag_answer], [gold_answer], lang='en', rescale_with_baseline=True)\n P, R, F1 = bert_score\n precision, recall, f1 = P.item(), R.item(), F1.item()\n\n return {'bert_precision':precision, 'bert_recall':recall,'bert_f1':f1}"
 },
 {
 "cell_type": "code",
 "execution_count": 87,
 "metadata": {
 "colab": {
 "base_uri": "https://localhost:8080/",
 "height": 1000
 },
 "executionInfo": {
 "elapsed": 35246,
 "status": "ok",
 "timestamp": 1765128645949,
 "user": {
 "displayName": "Ryan C",
 "userId": "13217471319505539255"
 },
 "user_tz": 420
 },
 "id": "GjOCg-BHWBvY",
 "outputId": "c122947b-db43-4b66-8fa8-8b3cc18f9114"
 },
 "outputs": [
 {
 "output_type": "stream",
 "name": "stdout",
 "text": [
 "Collecting ragas\n",
 " Downloading ragas-0.4.0-py3-none-any.whl.metadata (22 kB)\n",
 "Requirement already satisfied: numpy<3.0.0,>=1.21.0 in /usr/local/lib/python3.12/dist-packages (from ragas) (2.0.2)\n",
 "Requirement already satisfied: datasets>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from ragas) (4.4.1)\n",
 "Requirement already satisfied: tiktoken in /usr/local/lib/python3.12/dist-packages (from ragas) (0.12.0)\n",
 "Requirement already satisfied: pydantic>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from ragas) (2.12.3)\n",
 "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.12/dist-packages (from ragas) (1.6.0)\n",
 "Collecting appdirs (from ragas)\n",
 " Downloading appdirs-1.4.4-py2.py3-none-any.whl.metadata (9.0 kB)\n",
 "Collecting diskcache>=5.6.3 (from ragas)\n",
 " Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
 "Requirement already satisfied: typer in /usr/local/lib/python3.12/dist-packages (from ragas) (0.20.0)\n",
 "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from ragas) (13.9.4)\n",
 "Requirement already satisfied: openai>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from ragas) (2.8.1)\n",
 "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from ragas) (4.67.1)\n",
 "Collecting instructor (from ragas)\n",
 " Downloading instructor-1.13.0-py3-none-any.whl.metadata (11 kB)\n",
 "Requirement already satisfied: pillow>=10.4.0 in /usr/local/lib/python3.12/dist-packages (from ragas) (11.3.0)\n",
 "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from ragas) (3.6)\n",
 "Collecting scikit-network (from ragas)\n",
 " Downloading scikit_network-0.33.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.5 kB)\n",
 "Requirement already satisfied: langchain in /usr/local/lib/python3.12/dist-packages (from ragas) (1.1.0)\n",
 "Requirement already satisfied: langchain-core in /usr/local/lib/python3.12/dist-packages (from ragas) (1.1.0)\n",
 "Requirement already satisfied: langchain-community in /usr/local/lib/python3.12/dist-packages (from ragas) (0.4.1)\n",
 "Collecting langchain_openai (from ragas)\n",
 " Downloading langchain_openai-1.1.0-py3-none-any.whl.metadata (2.6 kB)\n",
 "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets>=4.0.0->ragas) (3.20.0)\n",
 "Requirement already satisfied: pyarrow>=21.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=4.0.0->ragas) (22.0.0)\n",
 "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=4.0.0->ragas) (0.3.8)\n",
 "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets>=4.0.0->ragas) (2.2.2)\n",
 "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets>=4.0.0->ragas) (2.32.5)\n",
 "Requirement already satisfied: httpx<1.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=4.0.0->ragas) (0.28.1)\n",
 "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets>=4.0.0->ragas) (3.6.0)\n",
 "Requirement already satisfied: multiprocess<0.70.19 in /usr/local/lib/python3.12/dist-packages (from datasets>=4.0.0->ragas) (0.70.16)\n",
 "Requirement already satisfied: fsspec<=2025.10.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=4.0.0->ragas) (2025.3.0)\n",
 "Requirement already satisfied: huggingface-hub<2.0,>=0.25.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=4.0.0->ragas) (0.36.0)\n",
 "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from datasets>=4.0.0->ragas) (24.2)\n",
 "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets>=4.0.0->ragas) (6.0.3)\n",
 "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai>=1.0.0->ragas) (4.11.0)\n",
 "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai>=1.0.0->ragas) (1.9.0)\n",
 "Requirement already satisfied: jiter<1,>=0.10.0 in /usr/local/lib/python3.12/dist-packages (from openai>=1.0.0->ragas) (0.12.0)\n",
 "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai>=1.0.0->ragas) (1.3.1)\n",
 "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.12/dist-packages (from openai>=1.0.0->ragas) (4.15.0)\n",
 "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.0.0->ragas) (0.7.0)\n",
 "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.0.0->ragas) (2.41.4)\n",
 "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.0.0->ragas) (0.4.2)\n",
 "Requirement already satisfied: aiohttp<4.0.0,>=3.9.1 in /usr/local/lib/python3.12/dist-packages (from instructor->ragas) (3.13.2)\n",
 "Requirement already satisfied: docstring-parser<1.0,>=0.16 in /usr/local/lib/python3.12/dist-packages (from instructor->ragas) (0.17.0)\n",
 "Requirement already satisfied: jinja2<4.0.0,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from instructor->ragas) (3.1.6)\n",
 "Collecting jiter<1,>=0.10.0 (from openai>=1.0.0->ragas)\n",
 " Downloading jiter-0.11.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\n",
 "Collecting pre-commit>=4.3.0 (from instructor->ragas)\n",
 " Downloading pre_commit-4.5.0-py2.py3-none-any.whl.metadata (1.2 kB)\n",
 "Requirement already satisfied: tenacity<10.0.0,>=8.2.3 in /usr/local/lib/python3.12/dist-packages (from instructor->ragas) (9.1.2)\n",
 "Collecting ty>=0.0.1a23 (from instructor->ragas)\n",
 " Downloading ty-0.0.1a32-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.5 kB)\n",
 "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->ragas) (4.0.0)\n",
 "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->ragas) (2.19.2)\n",
 "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer->ragas) (8.3.1)\n",
 "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer->ragas) (1.5.4)\n",
 "Requirement already satisfied: langgraph<1.1.0,>=1.0.2 in /usr/local/lib/python3.12/dist-packages (from langchain->ragas) (1.0.3)\n",
 "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core->ragas) (1.33)\n",
 "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core->ragas) (0.4.47)\n",
 "Requirement already satisfied: langchain-classic<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community->ragas) (1.0.0)\n",
 "Requirement already satisfied: SQLAlchemy<3.0.0,>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community->ragas) (2.0.44)\n",
 "Requirement already satisfied: dataclasses-json<0.7.0,>=0.6.7 in /usr/local/lib/python3.12/dist-packages (from langchain-community->ragas) (0.6.7)\n",
 "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in /usr/local/lib/python3.12/dist-packages (from langchain-community->ragas) (2.12.0)\n",
 "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community->ragas) (0.4.0)\n",
 "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken->ragas) (2025.11.3)\n",
 "Requirement already satisfied: scipy>=1.7.3 in /usr/local/lib/python3.12/dist-packages (from scikit-network->ragas) (1.16.3)\n",
 "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.9.1->instructor->ragas) (2.6.1)\n",
 "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.9.1->instructor->ragas) (1.4.0)\n",
 "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.9.1->instructor->ragas) (25.4.0)\n",
 "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.9.1->instructor->ragas) (1.8.0)\n",
 "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.9.1->instructor->ragas) (6.7.0)\n",
 "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.9.1->instructor->ragas) (0.4.1)\n",
 "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.9.1->instructor->ragas) (1.22.0)\n",
 "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai>=1.0.0->ragas) (3.11)\n",
 "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community->ragas) (3.26.1)\n",
 "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community->ragas) (0.9.0)\n",
 "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets>=4.0.0->ragas) (2025.11.12)\n",
 "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets>=4.0.0->ragas) (1.0.9)\n",
 "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0.0->datasets>=4.0.0->ragas) (0.16.0)\n",
 "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.25.0->datasets>=4.0.0->ragas) (1.2.0)\n",
 "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2<4.0.0,>=3.1.4->instructor->ragas) (3.0.3)\n",
 "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core->ragas) (3.0.0)\n",
 "Requirement already satisfied: langchain-text-splitters<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langchain-classic<2.0.0,>=1.0.0->langchain-community->ragas) (1.0.0)\n",
 "Requirement already satisfied: langgraph-checkpoint<4.0.0,>=2.1.0 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.2->langchain->ragas) (3.0.1)\n",
 "Requirement already satisfied: langgraph-prebuilt<1.1.0,>=1.0.2 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.2->langchain->ragas) (1.0.5)\n",
 "Requirement already satisfied: langgraph-sdk<0.3.0,>=0.2.2 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.2->langchain->ragas) (0.2.10)\n",
 "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core->ragas) (3.11.4)\n",
 "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core->ragas) (1.0.0)\n",
 "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core->ragas) (0.25.0)\n",
 "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->ragas) (0.1.2)\n",
 "Collecting cfgv>=2.0.0 (from pre-commit>=4.3.0->instructor->ragas)\n",
 " Downloading cfgv-3.5.0-py2.py3-none-any.whl.metadata (8.9 kB)\n",
 "Collecting identify>=1.0.0 (from pre-commit>=4.3.0->instructor->ragas)\n",
 " Downloading identify-2.6.15-py2.py3-none-any.whl.metadata (4.4 kB)\n",
 "Collecting nodeenv>=0.11.1 (from pre-commit>=4.3.0->instructor->ragas)\n",
 " Downloading nodeenv-1.9.1-py2.py3-none-any.whl.metadata (21 kB)\n",
 "Collecting virtualenv>=20.10.0 (from pre-commit>=4.3.0->instructor->ragas)\n",
 " Downloading virtualenv-20.35.4-py3-none-any.whl.metadata (4.6 kB)\n",
 "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community->ragas) (1.2.1)\n",
 "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets>=4.0.0->ragas) (3.4.4)\n",
 "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets>=4.0.0->ragas) (2.3.0)\n",
 "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3.0.0,>=1.4.0->langchain-community->ragas) (3.2.4)\n",
 "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets>=4.0.0->ragas) (2.9.0.post0)\n",
 "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets>=4.0.0->ragas) (2025.2)\n",
 "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets>=4.0.0->ragas) (2025.2)\n",
 "Requirement already satisfied: ormsgpack>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from langgraph-checkpoint<4.0.0,>=2.1.0->langgraph<1.1.0,>=1.0.2->langchain->ragas) (1.12.0)\n",
 "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets>=4.0.0->ragas) (1.17.0)\n",
 "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain-community->ragas) (1.1.0)\n",
 "Collecting distlib<1,>=0.3.7 (from virtualenv>=20.10.0->pre-commit>=4.3.0->instructor->ragas)\n",
 " Downloading distlib-0.4.0-py2.py3-none-any.whl.metadata (5.2 kB)\n",
 "Requirement already satisfied: platformdirs<5,>=3.9.1 in /usr/local/lib/python3.12/dist-packages (from virtualenv>=20.10.0->pre-commit>=4.3.0->instructor->ragas) (4.5.0)\n",
 "Downloading ragas-0.4.0-py3-none-any.whl (397 kB)\n",
 "\u001b[2K \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m397.1/397.1 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
 "\u001b[?25hDownloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
 "\u001b[2K \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
 "\u001b[?25hDownloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
 "Downloading instructor-1.13.0-py3-none-any.whl (160 kB)\n",
 "\u001b[2K \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m160.9/160.9 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
 "\u001b[?25hDownloading langchain_openai-1.1.0-py3-none-any.whl (84 kB)\n",
 "\u001b[2K \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m84.3/84.3 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
 "\u001b[?25hDownloading scikit_network-0.33.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.0 MB)\n",
 "\u001b[2K \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m8.0/8.0 MB\u001b[0m \u001b[31m70.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
 "\u001b[?25hDownloading jiter-0.11.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (358 kB)\n",
 "\u001b[2K \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m358.8/358.8 kB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
 "\u001b[?25hDownloading pre_commit-4.5.0-py2.py3-none-any.whl (226 kB)\n",
 "\u001b[2K \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m226.4/226.4 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
 "\u001b[?25hDownloading ty-0.0.1a32-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.8 MB)\n",
 "\u001b[2K \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m9.8/9.8 MB\u001b[0m \u001b[31m56.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
 "\u001b[?25hDownloading cfgv-3.5.0-py2.py3-none-any.whl (7.4 kB)\n",
 "Downloading identify-2.6.15-py2.py3-none-any.whl (99 kB)\n",
 "\u001b[2K \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m99.2/99.2 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
 "\u001b[?25hDownloading nodeenv-1.9.1-py2.py3-none-any.whl (22 kB)\n",
 "Downloading virtualenv-20.35.4-py3-none-any.whl (6.0 MB)\n",
 "\u001b[2K \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m32.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
 "\u001b[?25hDownloading distlib-0.4.0-py2.py3-none-any.whl (469 kB)\n",
 "\u001b[2K \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m469.0/469.0 kB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
 "\u001b[?25hInstalling collected packages: distlib, appdirs, virtualenv, ty, nodeenv, jiter, identify, diskcache, cfgv, scikit-network, pre-commit, instructor, langchain_openai, ragas\n",
 " Attempting uninstall: jiter\n",
 " Found existing installation: jiter 0.12.0\n",
 " Uninstalling jiter-0.12.0:\n",
 " Successfully uninstalled jiter-0.12.0\n",
 "Successfully installed appdirs-1.4.4 cfgv-3.5.0 diskcache-5.6.3 distlib-0.4.0 identify-2.6.15 instructor-1.13.0 jiter-0.11.1 langchain_openai-1.1.0 nodeenv-1.9.1 pre-commit-4.5.0 ragas-0.4.0 scikit-network-0.33.5 ty-0.0.1a32 virtualenv-20.35.4\n",
 "Requirement already satisfied: langchain-openai in /usr/local/lib/python3.12/dist-packages (1.1.0)\n",
 "Requirement already satisfied: langchain-core in /usr/local/lib/python3.12/dist-packages (1.1.0)\n",
 "Collecting langchain-core\n",
 " Downloading langchain_core-1.1.1-py3-none-any.whl.metadata (3.7 kB)\n",
 "Requirement already satisfied: openai<3.0.0,>=1.109.1 in /usr/local/lib/python3.12/dist-packages (from langchain-openai) (2.8.1)\n",
 "Requirement already satisfied: tiktoken<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-openai) (0.12.0)\n",
 "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (1.33)\n",
 "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (0.4.47)\n",
 "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (24.2)\n",
 "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (2.12.3)\n",
 "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (6.0.3)\n",
 "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (9.1.2)\n",
 "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (4.15.0)\n",
 "Collecting uuid-utils<1.0,>=0.12.0 (from langchain-core)\n",
 " Downloading uuid_utils-0.12.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.1 kB)\n",
 "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core) (3.0.0)\n",
 "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core) (0.28.1)\n",
 "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core) (3.11.4)\n",
 "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core) (1.0.0)\n",
 "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core) (2.32.5)\n",
 "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core) (0.25.0)\n",
 "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (4.11.0)\n",
 "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (1.9.0)\n",
 "Requirement already satisfied: jiter<1,>=0.10.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (0.11.1)\n",
 "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (1.3.1)\n",
 "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (4.67.1)\n",
 "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core) (0.7.0)\n",
 "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core) (2.41.4)\n",
 "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core) (0.4.2)\n",
 "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken<1.0.0,>=0.7.0->langchain-openai) (2025.11.3)\n",
 "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai<3.0.0,>=1.109.1->langchain-openai) (3.11)\n",
 "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core) (2025.11.12)\n",
 "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core) (1.0.9)\n",
 "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core) (0.16.0)\n",
 "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core) (3.4.4)\n",
 "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core) (2.3.0)\n",
 "Downloading langchain_core-1.1.1-py3-none-any.whl (475 kB)\n",
 "\u001b[2K \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m475.0/475.0 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
 "\u001b[?25hDownloading uuid_utils-0.12.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (343 kB)\n",
 "\u001b[2K \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m343.7/343.7 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
 "\u001b[?25hInstalling collected packages: uuid-utils, langchain-core\n",
 " Attempting uninstall: langchain-core\n",
 " Found existing installation: langchain-core 1.1.0\n",
 " Uninstalling langchain-core-1.1.0:\n",
 " Successfully uninstalled langchain-core-1.1.0\n",
 "Successfully installed langchain-core-1.1.1 uuid-utils-0.12.0\n"
 ]
 },
 {
 "output_type": "display_data",
 "data": {
 "application/vnd.colab-display-data+json": {
 "pip_warning": {
 "packages": [
 "langchain_core"
 ]
 },
 "id": "84def13e8ed14e4aaa5c9a4b7e396f19"
 }
 },
 "metadata": {}
 }
 ],
 "source": "# Upgrade to latest RAGAS version\n!pip install --upgrade ragas\n\n# Also upgrade langchain components\n!pip install --upgrade langchain-openai langchain-core"
 },
 {
 "cell_type": "code",
 "execution_count": 88,
 "metadata": {
 "id": "1pURDoZ5uqAc",
 "executionInfo": {
 "status": "ok",
 "timestamp": 1765128645964,
 "user_tz": 420,
 "elapsed": 11,
 "user": {
 "displayName": "Ryan C",
 "userId": "13217471319505539255"
 }
 }
 },
 "outputs": [],
 "source": "def evaluate_with_ragas(rag_response):\n \"\"\"\n Evaluates RAG system using RAGAS metrics\n \"\"\"\n from ragas import EvaluationDataset, evaluate\n from ragas.metrics import LLMContextRecall, Faithfulness, FactualCorrectness\n from ragas.llms import LangchainLLMWrapper\n from langchain_openai import ChatOpenAI\n\n evaluator_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4o-mini\"))\n\n test_question = rag_response['question']\n gold_answer = rag_response['gold_answer']\n generated_answer = rag_response['rag_answer']\n retrieved_docs_content = rag_response['retrieved_contexts']\n\n dataset_dict = {\n \"user_input\": test_question,\n \"retrieved_contexts\": retrieved_docs_content,\n \"response\": generated_answer,\n \"reference\": gold_answer\n }\n\n evaluation_dataset = EvaluationDataset.from_list([dataset_dict])\n\n result = evaluate(\n dataset=evaluation_dataset,\n metrics=[\n LLMContextRecall(),\n Faithfulness(),\n FactualCorrectness()\n ],\n llm=evaluator_llm\n )\n\n return result"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "dxZ0qynovYS7"
 },
 "source": "### Combine evaluation"
 },
 {
 "cell_type": "code",
 "execution_count": 89,
 "metadata": {
 "id": "0Wtkd1aHt8yU",
 "executionInfo": {
 "status": "ok",
 "timestamp": 1765128645988,
 "user_tz": 420,
 "elapsed": 19,
 "user": {
 "displayName": "Ryan C",
 "userId": "13217471319505539255"
 }
 }
 },
 "outputs": [],
 "source": "def evaluate_complete(question_id, chain_type='marketing_mistral', retrieved_contexts=None, scorer=None):\n rag_response = ask_rag_question(question_id, chain_type, retrieved_contexts=retrieved_contexts)\n\n bert_scores = evaluate_with_bertscore(rag_response, scorer=scorer)\n\n ragas_result = evaluate_with_ragas(rag_response)\n\n all_scores = {\n 'question_id': question_id,\n 'chain_type': chain_type,\n 'bert_precision': bert_scores['bert_precision'],\n 'bert_recall': bert_scores['bert_recall'],\n 'bert_f1': bert_scores['bert_f1'],\n 'ragas_context_recall': ragas_result['context_recall'],\n 'ragas_faithfulness': ragas_result['faithfulness'],\n 'ragas_factual_correctness': ragas_result['factual_correctness(mode=f1)']\n }\n\n return rag_response, all_scores"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "PHM6udAyvd9Y"
 },
 "source": "### Run Testing"
 },
 {
 "cell_type": "code",
 "execution_count": 90,
 "metadata": {
 "colab": {
 "base_uri": "https://localhost:8080/",
 "height": 244,
 "referenced_widgets": [
 "ac986c9b87424e8c935a348c83ea02ca",
 "87d929364fd845f28603bf8dad4a2f32",
 "81e96dbf342b4d85b6770b201cc22dbd",
 "3f3263e023c64a14aad67561540f7b5c",
 "8476640fe3bd462da979d54ec9d65818",
 "3390fab8a87f467abd136f58b78f4f20",
 "e1d86a65e99b4690b484de16ecb162e9",
 "30e06035b5ad4219a7c7dbfc9c75e9af",
 "94e7accc3f1d4ae3b990e64e9fe3a3a1",
 "7383d151f62c4d0cae7bd2fd60c654e6",
 "44b572a936ec4c46a4c318d5279f88c3",
 "d1af6efb53e34b1a9fc35fac50bebf76",
 "5be1bc95feb54774a9245426923fd335",
 "3b7f3a3a86d34f9d90d0900e523c4dfe",
 "542a891afa694295b6d260d4deaa0f56",
 "e292432837774a89b79cf8bd7e4fd6c8",
 "4b2405d391384bf2a56fc8c0cc041964",
 "00ba8be551204bf7a40a8ec48ae4447b",
 "ba351316e08542adb67110ad79366744",
 "32ad1bdf1ac042e58b3a7aaaeedc6f87",
 "fb33497c962643069813b0586fa25d18",
 "0d71168e4d404f719aac3f509e95fb6f",
 "c71360e64cde441bbb6f9a43e50b6275",
 "4e53df3bd14848b1a45653667e863d34",
 "ab50f14c81bc4ac4b95349ea0cba4f44",
 "1fa1db49248a4a4b8ce9b0c7e0bebd3a",
 "f32492742e7b435488d6627105c0b7ba",
 "028ef42b972f4786a1ed93ee3a06a483",
 "55c0b8a49bfb4a7087de0873dd40a4a2",
 "e7dd126e72dc4de6974eef82b2416075",
 "6dca418abd0f49e5a3dc9e859aff916f",
 "50728ced30684790b76a4a136789a265",
 "ed0bcfbf927c49e19b9d473596be91f8",
 "3672ddcdc58e4c57852f4e057a7d2f6a",
 "0e8b713ae3eb4c21b079ad04b941381a",
 "bfb566ef9baa49b087f294391a38b21c",
 "ac45ef00b991402aa3eb2bfa744b4705",
 "5542e1edfddc4854ba707ce9b15c5c40",
 "dd0abe2a38f2446daef5ac79dbd6f438",
 "898525e3faa1496a84d6235aa5f4d31f",
 "b2f4a9019d7e4a3bb38dea50169b7d96",
 "4d2c36de32f04a8c9221e5f91d1556b4",
 "93db32c6ab384c539889db92eb42b9fa",
 "85dc5a5d6da04665b338103c6951454a",
 "ea383e14b0eb4c6c8f824d2a0dda442a",
 "f8825d591ab04586a194cf93a2a1e36e",
 "ba5a38cb9ebe43e6b0c81f8c0c98c074",
 "a86c969dd7d94d68a38ace129013e23b",
 "165557436e894cb5a140cd858704d286",
 "5f9937c0ac414db4bbb5670668916fce",
 "e81b36e85a4946debb783f49f35d9ca0",
 "841d86e95b38470bb43e7e4c0184a13e",
 "962e24113fa644698504a6c30f120e0a",
 "be30a8507de5499cbb1ad968e9421b4f",
 "b4de77e7c36b4cf0ab097a4fe83b89da",
 "0027ace2e7614c3ab6eb6db33f3efac0",
 "7b65f36ac816481b903b2e53d068761b",
 "e3506725163842068c7dbb1475038712",
 "8fe64231480c459c83d54c4784e75299",
 "6f517a1a6b5b4f06b85442bf9beb4f52",
 "3f8819125d00462e9e1503ccfe26ffed",
 "e16cb0de1e88409f8c9ef35336bb609e",
 "55677e2a7a2c4dad8c0e672895474407",
 "3b4638840c2f47279e285d7ea758b1c0",
 "7e093d96dbdc4b709b1d797c41f6b72f",
 "a042b02b7d5a4f43959a1cf4dfa7177b"
 ]
 },
 "executionInfo": {
 "elapsed": 14820,
 "status": "ok",
 "timestamp": 1765128660811,
 "user": {
 "displayName": "Ryan C",
 "userId": "13217471319505539255"
 },
 "user_tz": 420
 },
 "id": "SZ967k74VUK9",
 "outputId": "506dde03-9fb7-4c64-9b89-16386d030fb2"
 },
 "outputs": [
 {
 "output_type": "display_data",
 "data": {
 "text/plain": [
 "tokenizer_config.json: 0%| | 0.00/25.0 [00:00<?, ?B/s]"
 ],
 "application/vnd.jupyter.widget-view+json": {
 "version_major": 2,
 "version_minor": 0,
 "model_id": "ac986c9b87424e8c935a348c83ea02ca"
 }
 },
 "metadata": {}
 },
 {
 "output_type": "display_data",
 "data": {
 "text/plain": [
 "config.json: 0%| | 0.00/482 [00:00<?, ?B/s]"
 ],
 "application/vnd.jupyter.widget-view+json": {
 "version_major": 2,
 "version_minor": 0,
 "model_id": "d1af6efb53e34b1a9fc35fac50bebf76"
 }
 },
 "metadata": {}
 },
 {
 "output_type": "display_data",
 "data": {
 "text/plain": [
 "vocab.json: 0%| | 0.00/899k [00:00<?, ?B/s]"
 ],
 "application/vnd.jupyter.widget-view+json": {
 "version_major": 2,
 "version_minor": 0,
 "model_id": "c71360e64cde441bbb6f9a43e50b6275"
 }
 },
 "metadata": {}
 },
 {
 "output_type": "display_data",
 "data": {
 "text/plain": [
 "merges.txt: 0%| | 0.00/456k [00:00<?, ?B/s]"
 ],
 "application/vnd.jupyter.widget-view+json": {
 "version_major": 2,
 "version_minor": 0,
 "model_id": "3672ddcdc58e4c57852f4e057a7d2f6a"
 }
 },
 "metadata": {}
 },
 {
 "output_type": "display_data",
 "data": {
 "text/plain": [
 "tokenizer.json: 0%| | 0.00/1.36M [00:00<?, ?B/s]"
 ],
 "application/vnd.jupyter.widget-view+json": {
 "version_major": 2,
 "version_minor": 0,
 "model_id": "ea383e14b0eb4c6c8f824d2a0dda442a"
 }
 },
 "metadata": {}
 },
 {
 "output_type": "display_data",
 "data": {
 "text/plain": [
 "model.safetensors: 0%| | 0.00/1.42G [00:00<?, ?B/s]"
 ],
 "application/vnd.jupyter.widget-view+json": {
 "version_major": 2,
 "version_minor": 0,
 "model_id": "0027ace2e7614c3ab6eb6db33f3efac0"
 }
 },
 "metadata": {}
 },
 {
 "output_type": "stream",
 "name": "stderr",
 "text": [
 "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
 "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
 ]
 }
 ],
 "source": "from bert_score import BERTScorer\n\nbert_scorer = BERTScorer(lang=\"en\", rescale_with_baseline=True)"
 },
 {
 "cell_type": "code",
 "execution_count": 91,
 "metadata": {
 "executionInfo": {
 "elapsed": 10,
 "status": "ok",
 "timestamp": 1765128660824,
 "user": {
 "displayName": "Ryan C",
 "userId": "13217471319505539255"
 },
 "user_tz": 420
 },
 "id": "ym2iGmkJct2c"
 },
 "outputs": [],
 "source": "# import os\n\n# # Delete the checkpoint file to start fresh\n# checkpoint_file = '/content/drive/MyDrive/evaluation_results_checkpoint.pkl'\n\n# if os.path.exists(checkpoint_file):\n# os.remove(checkpoint_file)"
 },
 {
 "cell_type": "code",
 "execution_count": 92,
 "metadata": {
 "colab": {
 "base_uri": "https://localhost:8080/"
 },
 "executionInfo": {
 "elapsed": 675,
 "status": "ok",
 "timestamp": 1765128661503,
 "user": {
 "displayName": "Ryan C",
 "userId": "13217471319505539255"
 },
 "user_tz": 420
 },
 "id": "sQm7XYG4fPr4",
 "outputId": "b3855d00-df2d-449d-a24c-2144c85b8f25"
 },
 "outputs": [
 {
 "output_type": "stream",
 "name": "stdout",
 "text": [
 "Total available questions: 78\n",
 "First 20 question IDs: [0, 1, 2, 3, 7, 8, 9, 11, 12, 13, 16, 17, 18, 19, 20, 22, 23, 24, 25, 27]\n",
 "Testing with questions: [0, 1, 2]\n",
 "Loaded 156 existing generated responses\n",
 "Progress: 156/6 generations complete\n",
 "PHASE 1: Generating RAG responses...\n"
 ]
 },
 {
 "output_type": "stream",
 "name": "stderr",
 "text": [
 "Questions: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [00:00<00:00, 15.12it/s]"
 ]
 },
 {
 "output_type": "stream",
 "name": "stdout",
 "text": [
 "PHASE 1 Complete: 156 responses generated\n",
 "PHASE 2: Batch Evaluation on 156 items...\n",
 " BERTScore already computed, skipping...\n",
 " RAGAS already computed, skipping...\n",
 "\n",
 "======================================================================\n",
 "EVALUATION COMPLETE!\n",
 "======================================================================\n",
 "Total items evaluated: 156\n",
 "Questions tested: [np.int64(0), np.int64(1), np.int64(2), np.int64(3), np.int64(7), np.int64(8), np.int64(9), np.int64(11), np.int64(12), np.int64(13), np.int64(16), np.int64(17), np.int64(18), np.int64(19), np.int64(20), np.int64(22), np.int64(23), np.int64(24), np.int64(25), np.int64(27), np.int64(28), np.int64(30), np.int64(33), np.int64(34), np.int64(35), np.int64(36), np.int64(38), np.int64(39), np.int64(41), np.int64(43), np.int64(44), np.int64(46), np.int64(47), np.int64(48), np.int64(50), np.int64(51), np.int64(52), np.int64(53), np.int64(54), np.int64(55), np.int64(59), np.int64(60), np.int64(61), np.int64(62), np.int64(63), np.int64(64), np.int64(65), np.int64(67), np.int64(69), np.int64(73), np.int64(74), np.int64(75), np.int64(76), np.int64(78), np.int64(80), np.int64(81), np.int64(82), np.int64(84), np.int64(85), np.int64(86), np.int64(87), np.int64(88), np.int64(89), np.int64(91), np.int64(92), np.int64(93), np.int64(94), np.int64(95), np.int64(96), np.int64(101), np.int64(102), np.int64(103), np.int64(104), np.int64(105), np.int64(106), np.int64(107), np.int64(108), np.int64(110)]\n",
 "Configs tested: ['engineering_cohere', 'marketing_cohere']\n",
 "Results saved to: /content/drive/MyDrive/evaluation_results_checkpoint.pkl\n",
 "\n",
 "Quick preview:\n",
 " bert_f1 ragas_context_recall ragas_context_precision \\\n",
 "chain_type \n",
 "engineering_cohere 0.240939 0.568590 0.631942 \n",
 "marketing_cohere 0.310688 0.695513 0.575615 \n",
 "\n",
 " ragas_faithfulness ragas_factual_correctness \\\n",
 "chain_type \n",
 "engineering_cohere 0.884755 0.270000 \n",
 "marketing_cohere 0.654070 0.249231 \n",
 "\n",
 " ragas_answer_relevancy \n",
 "chain_type \n",
 "engineering_cohere 0.836686 \n",
 "marketing_cohere 0.833979 \n",
 "\n",
 "======================================================================\n",
 "RETRIEVED DOCUMENT SOURCES\n",
 "======================================================================\n",
 "\n",
 "Q0: What defines a large language model in the context of natura...\n",
 "Sources: ['https://arxiv.org/pdf/2203.15556.pdf', 'https://en.wikipedia.org/wiki/Llama_(language_model)', 'https://en.wikipedia.org/wiki/List_of_large_language_models', 'https://en.wikipedia.org/wiki/Large_language_model', 'https://arxiv.org/pdf/2203.15556.pdf', 'https://en.wikipedia.org/wiki/Large_language_model', 'https://arxiv.org/pdf/2310.06825.pdf', 'https://lilianweng.github.io/posts/2020-10-29-odqa/']\n",
 "\n",
 "Q1: How do large language models like GPT-3 become capable of te...\n",
 "Sources: ['https://en.wikipedia.org/wiki/Llama_(language_model)', 'https://en.wikipedia.org/wiki/Generative_pre-trained_transformer', 'https://en.wikipedia.org/wiki/Generative_pre-trained_transformer', 'https://arxiv.org/pdf/2203.15556.pdf', 'https://en.wikipedia.org/wiki/Large_language_model', 'https://arxiv.org/pdf/2312.10997.pdf', 'https://arxiv.org/pdf/2203.02155.pdf', 'https://lilianweng.github.io/posts/2020-10-29-odqa/']\n",
 "\n",
 "Q2: What are some of the architectures used in building artifici...\n",
 "Sources: ['https://lilianweng.github.io/posts/2023-06-23-agent/', 'https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/', 'https://en.wikipedia.org/wiki/Llama_(language_model)', 'https://en.wikipedia.org/wiki/Large_language_model', 'https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/', 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'https://en.wikipedia.org/wiki/Generative_artificial_intelligence', 'https://lilianweng.github.io/posts/2018-06-24-attention/']\n"
 ]
 },
 {
 "output_type": "stream",
 "name": "stderr",
 "text": [
 "\n"
 ]
 }
 ],
 "source": "import pandas as pd\nfrom tqdm import tqdm\nimport time\nimport pickle\nimport os\nfrom bert_score import score\nfrom datasets import Dataset\n\n# Configuration\n# configs = ['marketing_mistral', 'marketing_cohere', 'engineering_mistral', 'engineering_cohere']\nconfigs = ['marketing_cohere', 'engineering_cohere']\nresults_file = '/content/drive/MyDrive/evaluation_results_checkpoint.pkl'\n\navailable_question_ids = sorted(validation_questions_answers.keys())\nprint(f\"Total available questions: {len(available_question_ids)}\")\nprint(f\"First 20 question IDs: {available_question_ids[:20]}\")\n\nquestion_ids_to_test = available_question_ids[:3]\nprint(f\"Testing with questions: {question_ids_to_test}\")\n\nif os.path.exists(results_file):\n with open(results_file, 'rb') as f:\n rag_outputs = pickle.load(f)\n print(f\"Loaded {len(rag_outputs)} existing generated responses\")\nelse:\n rag_outputs = []\n print(\"Starting fresh generation\")\n\ncompleted_keys = {(r['question_id'], r['chain_type']) for r in rag_outputs}\ntotal_needed = len(question_ids_to_test) * len(configs)\nprint(f\"Progress: {len(completed_keys)}/{total_needed} generations complete\")\n\nprint(\"PHASE 1: Generating RAG responses...\")\nfor question_id in tqdm(question_ids_to_test, desc=\"Questions\"):\n\n question = validation_questions_answers[question_id]['question']\n retrieved_docs = retriever.invoke(question)\n retrieved_contexts = [doc.page_content for doc in retrieved_docs]\n\n doc_sources = []\n for doc in retrieved_docs:\n if hasattr(doc, 'metadata') and 'source' in doc.metadata:\n doc_sources.append(doc.metadata['source'])\n else:\n doc_sources.append('unknown')\n\n for chain_type in configs:\n if (question_id, chain_type) in completed_keys:\n continue\n\n try:\n start = time.time()\n\n rag_response = ask_rag_question(\n question_id,\n chain_type\n )\n\n entry = {\n 'question_id': question_id,\n 'chain_type': chain_type,\n 'question': rag_response['question'],\n 'rag_answer': rag_response['rag_answer'],\n 'gold_answer': rag_response['gold_answer'],\n 'retrieved_contexts': rag_response['retrieved_contexts'],\n 'retrieved_sources': doc_sources\n }\n rag_outputs.append(entry)\n\n with open(results_file, 'wb') as f:\n pickle.dump(rag_outputs, f)\n\n elapsed = time.time() - start\n print(f\" Generated Q{question_id}-{chain_type} ({elapsed:.1f}s)\")\n\n except Exception as e:\n print(f\" Error Q{question_id}-{chain_type}: {e}\")\n\nprint(f\"PHASE 1 Complete: {len(rag_outputs)} responses generated\")\n\nprint(f\"PHASE 2: Batch Evaluation on {len(rag_outputs)} items...\")\n\nif rag_outputs and 'bert_f1' not in rag_outputs[0]:\n print(\" Running BERTScore in batch mode...\")\n start = time.time()\n\n rag_answers = [x['rag_answer'] for x in rag_outputs]\n gold_answers = [x['gold_answer'] for x in rag_outputs]\n\n P, R, F1 = score(rag_answers, gold_answers, lang='en', rescale_with_baseline=True)\n\n for i, entry in enumerate(rag_outputs):\n entry['bert_precision'] = P[i].item()\n entry['bert_recall'] = R[i].item()\n entry['bert_f1'] = F1[i].item()\n\n elapsed = time.time() - start\n print(f\" BERTScore complete ({elapsed:.1f}s for {len(rag_outputs)} items)\")\n\n with open(results_file, 'wb') as f:\n pickle.dump(rag_outputs, f)\nelse:\n print(\" BERTScore already computed, skipping...\")\n\nif rag_outputs and 'ragas_faithfulness' not in rag_outputs[0]:\n print(\" Running RAGAS in batch mode...\")\n start = time.time()\n\n data_dict = {\n \"user_input\": [x['question'] for x in rag_outputs],\n \"retrieved_contexts\": [x['retrieved_contexts'] for x in rag_outputs],\n \"response\": [x['rag_answer'] for x in rag_outputs],\n \"reference\": [x['gold_answer'] for x in rag_outputs]\n }\n\n dataset = Dataset.from_dict(data_dict)\n\n from ragas import evaluate\n from ragas.metrics import LLMContextRecall, ContextPrecision, Faithfulness, FactualCorrectness, AnswerRelevancy\n from ragas.llms import LangchainLLMWrapper\n from langchain_openai import ChatOpenAI\n\n evaluator_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4o-mini\"))\n metrics = [LLMContextRecall(), ContextPrecision(), Faithfulness(), FactualCorrectness(), AnswerRelevancy()]\n\n ragas_results = evaluate(\n dataset=dataset,\n metrics=metrics,\n llm=evaluator_llm\n )\n\n for i, entry in enumerate(rag_outputs):\n entry['ragas_context_recall'] = ragas_results['context_recall'][i]\n entry['ragas_context_precision'] = ragas_results['context_precision'][i]\n entry['ragas_faithfulness'] = ragas_results['faithfulness'][i]\n entry['ragas_factual_correctness'] = ragas_results['factual_correctness(mode=f1)'][i]\n entry['ragas_answer_relevancy'] = ragas_results['answer_relevancy'][i]\n\n elapsed = time.time() - start\n print(f\" RAGAS complete ({elapsed:.1f}s for {len(rag_outputs)} items)\")\n\n with open(results_file, 'wb') as f:\n pickle.dump(rag_outputs, f)\nelse:\n print(\" RAGAS already computed, skipping...\")\n\nresults_df = pd.DataFrame(rag_outputs)\nprint(f\"\\n{'='*70}\")\nprint(f\"EVALUATION COMPLETE!\")\nprint(f\"{'='*70}\")\nprint(f\"Total items evaluated: {len(results_df)}\")\nprint(f\"Questions tested: {sorted(results_df['question_id'].unique())}\")\nprint(f\"Configs tested: {sorted(results_df['chain_type'].unique())}\")\nprint(f\"Results saved to: {results_file}\")\nprint(f\"\\nQuick preview:\")\nprint(results_df.groupby('chain_type')[['bert_f1', 'ragas_context_recall',\n 'ragas_context_precision',\n 'ragas_faithfulness',\n 'ragas_factual_correctness',\n 'ragas_answer_relevancy']].mean())\n\nprint(f\"\\n{'='*70}\")\nprint(\"RETRIEVED DOCUMENT SOURCES\")\nprint(f\"{'='*70}\")\nfor q_id in question_ids_to_test:\n q_data = results_df[results_df['question_id'] == q_id].iloc[0]\n print(f\"\\nQ{q_id}: {q_data['question'][:60]}...\")\n print(f\"Sources: {q_data['retrieved_sources']}\")"
 },
 {
 "cell_type": "code",
 "execution_count": 93,
 "metadata": {
 "id": "MmHVT0ouokAv",
 "executionInfo": {
 "status": "ok",
 "timestamp": 1765128661516,
 "user_tz": 420,
 "elapsed": 9,
 "user": {
 "displayName": "Ryan C",
 "userId": "13217471319505539255"
 }
 }
 },
 "outputs": [],
 "source": "results_df['rag_answer_length'] = results_df['rag_answer'].str.len()\nresults_df['gold_answer_length'] = results_df['gold_answer'].str.len()"
 },
 {
 "cell_type": "code",
 "execution_count": 94,
 "metadata": {
 "id": "FCMS8EC22uOZ",
 "executionInfo": {
 "status": "ok",
 "timestamp": 1765128661522,
 "user_tz": 420,
 "elapsed": 3,
 "user": {
 "displayName": "Ryan C",
 "userId": "13217471319505539255"
 }
 }
 },
 "outputs": [],
 "source": "# results_df.to_csv(r'/content/drive/MyDrive/Gen AI/phase_1_results.csv',index=False)"
 },
 {
 "cell_type": "code",
 "execution_count": 95,
 "metadata": {
 "id": "NKMhvAlw83ab",
 "executionInfo": {
 "status": "ok",
 "timestamp": 1765128661527,
 "user_tz": 420,
 "elapsed": 2,
 "user": {
 "displayName": "Ryan C",
 "userId": "13217471319505539255"
 }
 }
 },
 "outputs": [],
 "source": "# results_df.to_csv(r'/content/drive/MyDrive/Gen AI/phase_2a_results.csv',index=False)"
 },
 {
 "cell_type": "code",
 "execution_count": 96,
 "metadata": {
 "id": "eOXAYhVccp3q",
 "executionInfo": {
 "status": "ok",
 "timestamp": 1765128661571,
 "user_tz": 420,
 "elapsed": 41,
 "user": {
 "displayName": "Ryan C",
 "userId": "13217471319505539255"
 }
 }
 },
 "outputs": [],
 "source": "# results_df.to_csv(r'/content/drive/MyDrive/Gen AI/phase_2b_results.csv',index=False)"
 },
 {
 "cell_type": "code",
 "execution_count": 97,
 "metadata": {
 "id": "ItNDq6jhfldr",
 "executionInfo": {
 "status": "ok",
 "timestamp": 1765128661576,
 "user_tz": 420,
 "elapsed": 6,
 "user": {
 "displayName": "Ryan C",
 "userId": "13217471319505539255"
 }
 }
 },
 "outputs": [],
 "source": "# results_df.to_csv(r'/content/drive/MyDrive/Gen AI/phase_2c_results.csv',index=False)"
 },
 {
 "cell_type": "code",
 "execution_count": 98,
 "metadata": {
 "id": "MQ0m4qUcqdMZ",
 "executionInfo": {
 "status": "ok",
 "timestamp": 1765128661869,
 "user_tz": 420,
 "elapsed": 293,
 "user": {
 "displayName": "Ryan C",
 "userId": "13217471319505539255"
 }
 }
 },
 "outputs": [],
 "source": "results_df.to_csv(r'/content/drive/MyDrive/Gen AI/phase_2d_results.csv',index=False)"
 },
 {
 "cell_type": "code",
 "execution_count": 100,
 "metadata": {
 "id": "B7ePc_ofvzgy",
 "colab": {
 "base_uri": "https://localhost:8080/"
 },
 "executionInfo": {
 "status": "ok",
 "timestamp": 1765128661928,
 "user_tz": 420,
 "elapsed": 24,
 "user": {
 "displayName": "Ryan C",
 "userId": "13217471319505539255"
 }
 },
 "outputId": "a970f12d-755d-4b59-bd5c-e3de1fe045a1"
 },
 "outputs": [
 {
 "output_type": "stream",
 "name": "stdout",
 "text": [
 "Average Gold Answer Lengths:\n",
 "Research: 613 chars\n",
 "Marketing: 253 chars\n",
 "Ratio: 2.42x\n"
 ]
 }
 ],
 "source": "gold_lengths = []\n\nfor q_id, q_data in validation_questions_answers.items():\n gold_lengths.append({\n 'question_id': q_id,\n 'research': len(q_data['gold_answer_research']),\n 'marketing': len(q_data['gold_answer_marketing'])\n })\n\ngold_df = pd.DataFrame(gold_lengths)\n\nprint(\"Average Gold Answer Lengths:\")\nprint(f\"Research: {gold_df['research'].mean():.0f} chars\")\nprint(f\"Marketing: {gold_df['marketing'].mean():.0f} chars\")\nprint(f\"Ratio: {(gold_df['research'].mean() / gold_df['marketing'].mean()):.2f}x\")"
 },
 {
 "cell_type": "code",
 "execution_count": 101,
 "metadata": {
 "id": "8WRCfXS9yWBM",
 "colab": {
 "base_uri": "https://localhost:8080/"
 },
 "executionInfo": {
 "status": "ok",
 "timestamp": 1765128661959,
 "user_tz": 420,
 "elapsed": 29,
 "user": {
 "displayName": "Ryan C",
 "userId": "13217471319505539255"
 }
 },
 "outputId": "2390062f-698f-489f-a7a9-87a7e38c3e66"
 },
 "outputs": [
 {
 "output_type": "stream",
 "name": "stdout",
 "text": [
 "What are some of the architectures used in building artificial neural networks for LLMs?\n",
 "\n",
 "Doc 1 (first 200 chars):\n",
 "Memory\n",
 "\n",
 "Short-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\n",
 "Long-term memory: This provides the agent with th\n",
 "\n",
 "Doc 2 (first 200 chars):\n",
 "Prompt LLM to generate instruction candidates based on a small set of demonstrations in the form of input-output pairs. E.g. {{Given desired input-output pairs}}\\n\\nThe instruction is.\n",
 "\n",
 "\n",
 "Given a datas\n",
 "\n",
 "Doc 3 (first 200 chars):\n",
 "== Versions ==\n",
 "\n",
 "\n",
 "=== Initial release ===\n",
 "The first version of Llama (stylized as LLaMA and sometimes referred to as Llama 1) was announced on February 24, 2023, via a blog post and a paper describing \n",
 "\n",
 "Doc 4 (first 200 chars):\n",
 "Moving beyond n-gram models, researchers started in 2000 to use neural networks to learn language models. Following the breakthrough of deep neural networks in image classification around 2012, simila\n",
 "\n",
 "Doc 5 (first 200 chars):\n",
 "Weng, Lilian. (Mar 2023). Prompt Engineering. Lil\u2019Log. https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/.\n",
 "\n",
 "Or\n",
 "@article{weng2023prompt,\n",
 " title = \"Prompt Engineering\",\n",
 " author = \"We\n",
 "\n",
 "Doc 6 (first 200 chars):\n",
 "LLM Powered Autonomous Agents\n",
 " \n",
 "Date: June 23, 2023 | Estimated Reading Time: 31 min | Author: Lilian Weng\n",
 "\n",
 "\n",
 "Building agents with LLM (large language model) as its core controller is a cool con\n",
 "\n",
 "Doc 7 (first 200 chars):\n",
 "=== Continual learning ===\n",
 "\n",
 "Another approach is continual learning, which involves methods like adapters and LoRA. These fine-tuning techniques permit efficient, incremental updates to a model without\n",
 "\n",
 "Doc 8 (first 200 chars):\n",
 "Generative artificial intelligence (Generative AI, or GenAI) is a subfield of artificial intelligence that uses generative models to produce text, images, videos, audio, software code or other forms o\n"
 ]
 }
 ],
 "source": "# Review docs associated with question 2\nprint(validation_questions_answers[2]['question'])\n\ndocs = retriever.invoke(validation_questions_answers[2]['question'])\nfor i, doc in enumerate(docs):\n print(f\"\\nDoc {i+1} (first 200 chars):\")\n print(doc.page_content[:200])"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "Ef7FtogoS7jY"
 },
 "source": "### 4.1. Metrics\n\nNow we'd like to hear about your approach to evaluation."
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "kGWUQnzBztm6"
 },
 "source": "**Demonstration:**\n4.1. In 8 to 15 sentences, please define and defend your approach to evaluating your RAG model. Please fill in your answer in the text block below:"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "seICQ4MrU2fZ"
 },
 "source": "The evaluation approach was designed to measure both sides of the RAG pipeline \u2014 retrieval quality (BERTScore) and generation quality (RAGAS). Given the strict time constraints of the POC, we developed an evaluation framework that allowed for rapid iteration while still producing reliable, comparable results. To standardize comparisons across experiments, we used a fixed test set of three questions representing different query styles. Although this limited set enabled fast experimentation, bringing in more samples from the provided answers would be greatly beneficial, however time and API cost constraints (especially with Cohere) prevented expanding it during the POC window.\n\nWe combined these automated metrics with manual reviews, recognizing that quantitative scores alone often miss nuances such as hallucinations or irrelevant retrieval. Using these automated metrics allowed us to quickly iterate on different configurations, while manual inspection helped identify further room for improvement or gaps. Retrieval was evaluated using context precision and recall to determine whether the right chunks were obtained, while generation was assessed through faithfulness and factual correctness to verify the content was grounded in what was retrieved.\n\nTo ensure consistent, controlled comparisons, we changed one variable at a time \u2014 prompt design, embedding model, k-value, and few-shot examples\u2014and re-ran the standardized question set for each. This helped us understand the affect of adjustments to our RAG system. Although the resulting evaluation framework was effective for a constrained POC, we acknowledge that future work should expand the test suite, incorporate more diverse question types, and undertake more manual review."
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "kKktTilyTy_z"
 },
 "source": "### 4.2. Evaluation Comparisons\n\nDocument your key runs here. Feel free to add more text and code cells as needed. Include at least one full run on all 75 examples for your best models/model configuration."
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "n-MvElatv9QH"
 },
 "source": "### Phase 1 Evaluation: Baseline Results\n\n**Configuration:**\n- **LLMs**: Mistral-7B-Instruct-v0.3, Cohere Command-R\n- **Prompts**: Marketing vs Engineering personas\n- **Retrieval**: k=4, multi-qa-mpnet-base-dot-v1 embeddings\n- **Chunking**: RecursiveCharacterTextSplitter (size=1200, overlap=200)\n- **Evaluation Questions**: 3 questions selected to test retrieval across different complexity levels"
 },
 {
 "cell_type": "code",
 "execution_count": 103,
 "metadata": {
 "id": "WXSFKKvtT0m1",
 "executionInfo": {
 "status": "ok",
 "timestamp": 1765128662282,
 "user_tz": 420,
 "elapsed": 315,
 "user": {
 "displayName": "Ryan C",
 "userId": "13217471319505539255"
 }
 }
 },
 "outputs": [],
 "source": "phase_1_results = pd.read_csv(r'/content/drive/MyDrive/Gen AI/phase_1_results.csv')"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "2GckJLr5pzKV"
 },
 "source": "Phase 1: Baseline Performance Issues\n\n**Key Findings:**\n\nOverall model performance was poor - both retrieval quality (BERTScore, manual review) and output grounding (RAGAS metrics) underperformed.\nVerbosity problem: Outputs 3-12x longer than gold answers.\n\n* marketing_cohere: 1,473 chars vs 379 target (3.9x)\n* engineering_cohere: 5,753 chars vs 608 target (9.5x)\n* Root cause: Prompts lack explicit length constraints\n\nQ2 retrieval failure: 0% context precision on technical question\n\nThe engineering Q2 completely failed to retrieve any relevant information. Instead, it retrieved generic LLM introductions, citation lists, and a threat diagram.\n\n* Root cause: the current embedding model (multi-qa-mpnet-base-dot-v1) does surface keyword matching, not semantic understandin. A closer look into these documents show that they do contain relevant architecture info, it was just not retrieved\n* Engineering_cohere reached 59% factual correctness despite this,suggesting the LLM compensated for this despite not having retrieved relevant information.\nThis is an issue, as we would want our RAG system to verify answers against company documentation\n\n**Model comparison:**\n\nMistral: More concise, better BERTScore, but lower faithfulness (0.43-0.89)\nCohere: Extremely verbose, but superior faithfulness (0.52-0.79) and factual correctness (59-72%)\nSelected Cohere for Phase 2: faithfulness harder to fix than verbosity\n\n**Next steps:**\n1. Fix verbosity: Add explicit length constraints to prompts\n2. Fix retrieval: Test all-distilroberta-v1 embedding (better semantic understanding)\n3. Optimize k-value: Test k=8 after fixing embeddings"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "TQ9SbaEjwMhX"
 },
 "source": "### Phase 2A Evaluation:\n\nChanges from Phase 1:\n* Added explicit length constraints (2-3 sentences/250 chars for marketing, 4-5 sentences/500 chars for engineering)\n* Added strict grounding instruction (\"Using ONLY the context provided\")\n* Removed vague terms like \"comprehensive\" and \"detailed\"\n* Added instruction to state when information is missing"
 },
 {
 "cell_type": "code",
 "execution_count": 106,
 "metadata": {
 "id": "nA1-k83A-bej",
 "executionInfo": {
 "status": "ok",
 "timestamp": 1765128662683,
 "user_tz": 420,
 "elapsed": 294,
 "user": {
 "displayName": "Ryan C",
 "userId": "13217471319505539255"
 }
 }
 },
 "outputs": [],
 "source": "phase_2a_results = pd.read_csv(r'/content/drive/MyDrive/Gen AI/phase_2a_results.csv')"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "NJylgUFH-aCh"
 },
 "source": "### Phase 2A Evaluation: Concise Prompts with Strict Grounding\nModification: Added explicit length targets (marketing: ~250 chars, engineering: ~500 chars) and strict grounding instruction (\"Using ONLY the context provided\").\n\n**Key Findings:**\n\nVerbosity resolved: Prompts reduced length 50-86%. marketing_cohere hit 372 chars vs 300 target.\nGrounding vs accuracy trade-off revealed:\n\n* Q0, Q1 (good retrieval): Models gave accurate, grounded answers\n* Q2 (failed retrieval): Models acknowledged missing info instead of hallucinating\n\nmarketing_cohere: 1.0 faithfulness, 0% factual correctness (admits context insufficient)\nengineering_cohere: 0.27 faithfulness, 0% factual correctness (attempted partial answer)\n\n* Q2 still fails: 0% context precision/recall unchanged. Prompt engineering can't fix bad retrieval.\n\nModel differences:\n\n* Mistral: Works with limited context, moderate faithfulness\n* Cohere: Strictly follows grounding, explicitly states \"information needed is missing\"\n\nConclusion:\nPrompt engineering fixed verbosity and improved BERTScore. But Q2 retrieval failure persists\u2014proves we need better embeddings, not better prompts."
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "mk-DgxaO10Jn"
 },
 "source": "### Phase 2B Evaluation:\n\nChanges from Phase 2A:\n* Replaced multi-qa-mpnet-base-dot-v1 \u2192 all-distilroberta-v1 while keeping Phase 2A concise prompts and k=4.\nRationale: Phase 2A proved prompt engineering can't fix retrieval failures."
 },
 {
 "cell_type": "code",
 "execution_count": 108,
 "metadata": {
 "id": "svn6XQJSgMGS",
 "executionInfo": {
 "status": "ok",
 "timestamp": 1765128662925,
 "user_tz": 420,
 "elapsed": 199,
 "user": {
 "displayName": "Ryan C",
 "userId": "13217471319505539255"
 }
 }
 },
 "outputs": [],
 "source": "phase_2b_results = pd.read_csv(r'/content/drive/MyDrive/Gen AI/phase_2b_results.csv')"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "jNgDinMmnUG-"
 },
 "source": "Phase 2B: Embedding Model Optimization\n\nKey Findings:\n\n* Q2 retrieval breakthrough: Phase 2A retrieved generic introductions and threat diagrams (0% precision). Phase 2B retrieved actual technical content: \"LSTM encoder-decoder,\" \"transformer architecture,\" \"Word2Vec,\" \"seq2seq models\" (25% precision). First successful technical retrieval for Q2.\n* Q0/Q1 maintained or improved: BERTScore F1 increased +0.01 to +0.10 across conceptual and process questions. engineering_mistral hit 0.55 F1 on Q1 with perfect faithfulness (1.0) and 77% factual correctness\u2014highest performance across all experiments.\n* Q2 showed first non-zero scores: Factual correctness went from 0% to 0-25%. BERTScore F1 improved +0.08 to +0.15. Models could finally provide substantive answers instead of acknowledging missing information.\nRemaining gap: Q2 context recall still 0%, precision only 25%. Only 1 of 4 chunks highly relevant\u2014coverage incomplete.\n\nConclusion:\nEmbedding optimization fixed the root cause from Phase 2A. all-distilroberta-v1 has better semantic understanding of technical terms, turning Q2 from complete failure to partial success. System now produces concise, grounded responses when retrieval works."
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "S55m283r22Kz"
 },
 "source": "### Phase 2C: k-Value Optimization (k=4 vs k=8)\n\n * Increased retrieval from k=4 \u2192 k=8 chunks while keeping Phase 2B config (distilroberta embeddings, concise prompts)."
 },
 {
 "cell_type": "code",
 "execution_count": 110,
 "metadata": {
 "id": "rz9E8zZ9mxJQ",
 "executionInfo": {
 "status": "ok",
 "timestamp": 1765128663221,
 "user_tz": 420,
 "elapsed": 240,
 "user": {
 "displayName": "Ryan C",
 "userId": "13217471319505539255"
 }
 }
 },
 "outputs": [],
 "source": "phase_2c_results = pd.read_csv(r'/content/drive/MyDrive/Gen AI/phase_2c_results.csv')"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "9cAqVSVX3iKI"
 },
 "source": "### Phase 2D: Few-Shot Prompting\n* Added 3 few-shot examples to both marketing and engineering prompts while keeping Phase 2C config (distilroberta embeddings, k=8, concise instructions)."
 },
 {
 "cell_type": "code",
 "execution_count": 112,
 "metadata": {
 "id": "-ClqJLROqiyS",
 "executionInfo": {
 "status": "ok",
 "timestamp": 1765128663412,
 "user_tz": 420,
 "elapsed": 105,
 "user": {
 "displayName": "Ryan C",
 "userId": "13217471319505539255"
 }
 }
 },
 "outputs": [],
 "source": "phase_2d_results = pd.read_csv(r'/content/drive/MyDrive/Gen AI/phase_2d_results.csv')"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "RZWrnfbLrdWm"
 },
 "source": "**Key Findings:**\n\n* BERTScore improved: +2-9% across both personas. Few-shot examples demonstrated desired structure/vocabulary, leading to better word-level overlap with gold answers.\n* Faithfulness dropped significantly: Engineering -14%, marketing -8%. Models pattern-matched against examples rather than grounding in retrieved context.\nMarketing factual correctness tanked: -35% decline (0.323 \u2192 0.210). Few-shot examples introduced implicit knowledge that led to hallucination when context insufficient\u2014particularly problematic for marketing's shorter response format.\n\n**Critical trade-off identified:**\n\nPhase 2D produces answers that look like gold answers (higher BERTScore) but sacrifice grounding and accuracy (lower faithfulness/correctness). BERTScore gains (+2-9%) don't justify faithfulness/accuracy losses.\n\n**Recommendation:**\nReject Phase 2D. Maintain Phase 2C configuration.\nPhase 2C achieved near-perfect faithfulness (0.970) for engineering. Few-shot examples introduced hallucination risk that outweighs stylistic gains. Marketing -35% factual correctness unacceptable for business communications.\n\n**Final optimal config:**\n* LLM: Cohere\n* Embeddings: all-distilroberta-v1\n* Retrieval: k=8\n* Prompts: Phase 2A concise, grounded prompts (no few-shot)"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "4dU5n7oQr2Lp"
 },
 "source": "### Full Evaluation"
 },
 {
 "cell_type": "code",
 "execution_count": 114,
 "metadata": {
 "colab": {
 "base_uri": "https://localhost:8080/"
 },
 "executionInfo": {
 "elapsed": 78,
 "status": "ok",
 "timestamp": 1765128663569,
 "user": {
 "displayName": "Ryan C",
 "userId": "13217471319505539255"
 },
 "user_tz": 420
 },
 "id": "uF53FUPrsBNM",
 "outputId": "dddbdf7c-fdb3-4a7a-cd87-d0085fadbb2a"
 },
 "outputs": [
 {
 "output_type": "stream",
 "name": "stdout",
 "text": [
 "Deleted checkpoint: /content/drive/MyDrive/evaluation_results_checkpoint.pkl\n"
 ]
 }
 ],
 "source": "import os\n\ncheckpoint_file = '/content/drive/MyDrive/evaluation_results_checkpoint.pkl'\n\nif os.path.exists(checkpoint_file):\n os.remove(checkpoint_file)\n print(f\"Deleted checkpoint: {checkpoint_file}\")\nelse:\n print(\"No checkpoint file found - starting fresh\")"
 },
 {
 "cell_type": "code",
 "execution_count": 115,
 "metadata": {
 "colab": {
 "base_uri": "https://localhost:8080/",
 "height": 1000,
 "referenced_widgets": [
 "1ca385be4bc54a00b5e36a7ab793b4b9",
 "3706c3f374604361bc38a8b1c50df7e8",
 "3071c4cde66446079d17c4f1d656e9b2",
 "42a692d223eb4f02b9f443ab91c5401a",
 "d4a7446d69204b84989611904804d87b",
 "6269eaeaaad742ba8ba17b66fd9a2fc3",
 "7ff5a13bbeb34e798f689d92a22e830a",
 "441b52492ec84061953d74c35b79ce17",
 "aefea3228cb54222aadadf2e5021ec84",
 "83167d53ecc54c61aafc389b12f7ea3a",
 "9a290f503fb84433a444e254cdc0558e"
 ]
 },
 "executionInfo": {
 "elapsed": 2000407,
 "status": "ok",
 "timestamp": 1765130663980,
 "user": {
 "displayName": "Ryan C",
 "userId": "13217471319505539255"
 },
 "user_tz": 420
 },
 "id": "DAhiYunYr3md",
 "outputId": "0170f26e-ba4e-4b38-e023-b9d40a8096f0"
 },
 "outputs": [
 {
 "output_type": "stream",
 "name": "stdout",
 "text": [
 "Total available questions: 78\n",
 "First 20 question IDs: [0, 1, 2, 3, 7, 8, 9, 11, 12, 13, 16, 17, 18, 19, 20, 22, 23, 24, 25, 27]\n",
 "Testing with 78 questions: [0, 1, 2, 3, 7, 8, 9, 11, 12, 13, 16, 17, 18, 19, 20, 22, 23, 24, 25, 27, 28, 30, 33, 34, 35, 36, 38, 39, 41, 43, 44, 46, 47, 48, 50, 51, 52, 53, 54, 55, 59, 60, 61, 62, 63, 64, 65, 67, 69, 73, 74, 75, 76, 78, 80, 81, 82, 84, 85, 86, 87, 88, 89, 91, 92, 93, 94, 95, 96, 101, 102, 103, 104, 105, 106, 107, 108, 110]\n",
 "Starting fresh generation\n",
 "Progress: 0/156 generations complete\n",
 "PHASE 1: Generating RAG responses...\n"
 ]
 },
 {
 "output_type": "stream",
 "name": "stderr",
 "text": [
 "\rQuestions: 0%| | 0/78 [00:00<?, ?it/s]"
 ]
 },
 {
 "output_type": "stream",
 "name": "stdout",
 "text": [
 " Generated Q0-marketing_cohere (1.8s)\n",
 " Generated Q0-engineering_cohere (4.9s)\n"
 ]
 },
 {
 "output_type": "stream",
 "name": "stderr",
 "text": [
 "\rQuestions: 1%|\u258f | 1/78 [00:13<17:40, 13.77s/it]"
 ]
 },
 {
 "output_type": "stream",
 "name": "stdout",
 "text": [
 " Generated Q1-marketing_cohere (1.6s)\n",
 " Generated Q1-engineering_cohere (2.9s)\n"
 ]
 },
 {
 "output_type": "stream",
 "name": "stderr",
 "text": [
 "\rQuestions: 3%|\u258e | 2/78 [00:25<15:51, 12.52s/it]"
 ]
 },
 {
 "output_type": "stream",
 "name": "stdout",
 "text": [
 " Generated Q2-marketing_cohere (8.4s)\n",
 " Generated Q2-engineering_cohere (2.1s)\n"
 ]
 },
 {
 "output_type": "stream",
 "name": "stderr",
 "text": [
 "\rQuestions: 4%|\u258d | 3/78 [00:42<18:32, 14.83s/it]"
 ]
 },
 {
 "output_type": "stream",
 "name": "stdout",
 "text": [
 " Generated Q3-marketing_cohere (1.5s)\n",
 " Generated Q3-engineering_cohere (3.0s)\n"
 ]
 },
 {
 "output_type": "stream",
 "name": "stderr",
 "text": [
 "\rQuestions: 5%|\u258c | 4/78 [00:54<16:42, 13.54s/it]"
 ]
 },
 {
 "output_type": "stream",
 "name": "stdout",
 "text": [
 " Generated Q7-marketing_cohere (1.3s)\n",
 " Generated Q7-engineering_cohere (2.7s)\n"
 ]
 },
 {
 "output_type": "stream",
 "name": "stderr",
 "text": [
 "\rQuestions: 6%|\u258b | 5/78 [01:05<15:23, 12.65s/it]"
 ]
 },
 {
 "output_type": "stream",
 "name": "stdout",
 "text": [
 " Generated Q8-marketing_cohere (1.3s)\n",
 " Generated Q8-engineering_cohere (2.8s)\n"
 ]
 },
 {
 "output_type": "stream",
 "name": "stderr",
 "text": [
 "\rQuestions: 8%|\u258a | 6/78 [01:16<14:35, 12.16s/it]"
 ]
 },
 {
 "output_type": "stream",
 "name": "stdout",
 "text": [
 " Generated Q9-marketing_cohere (1.6s)\n",
 " Generated Q9-engineering_cohere (2.0s)\n"
 ]
 },
 {
 "output_type": "stream",
 "name": "stderr",
 "text": [
 "\rQuestions: 9%|\u2589 | 7/78 [01:27<13:47, 11.65s/it]"
 ]
 },
 {
 "output_type": "stream",
 "name": "stdout",
 "text": [
 " Generated Q11-marketing_cohere (1.9s)\n",
 " Generated Q11-engineering_cohere (2.9s)\n"
 ]
 },
 {
 "output_type": "stream",
 "name": "stderr",
 "text": [
 "\rQuestions: 10%|\u2588 | 8/78 [01:39<13:38, 11.69s/it]"
 ]
 },
 {
 "output_type": "stream",
 "name": "stdout",
 "text": [
 " Generated Q12-marketing_cohere (1.0s)\n",
 " Generated Q12-engineering_cohere (3.1s)\n"
 ]
 },
 {
 "output_type": "stream",
 "name": "stderr",
 "text": [
 "\rQuestions: 12%|\u2588\u258f | 9/78 [01:50<13:13, 11.49s/it]"
 ]
 },
 {
 "output_type": "stream",
 "name": "stdout",
 "text": [
 " Generated Q13-marketing_cohere (2.0s)\n",
 " Generated Q13-engineering_cohere (2.5s)\n"
 ]
 },
 {
 "output_type": "stream",
 "name": "stderr",
 "text": [
 "\rQuestions: 13%|\u2588\u258e | 10/78 [02:01<13:01, 11.50s/it]"
 ]
 },
 {
 "output_type": "stream",
 "name": "stdout",
 "text": [
 " Generated Q16-marketing_cohere (1.8s)\n",
 " Generated Q16-engineering_cohere (3.4s)\n"
 ]
 },
 {
 "output_type": "stream",
 "name": "stderr",
 "text": [
 "\rQuestions: 14%|\u2588\u258d | 11/78 [02:14<13:06, 11.74s/it]"
 ]
 },
 {
 "output_type": "stream",
 "name": "stdout",
 "text": [
 " Generated Q17-marketing_cohere (1.3s)\n",
 " Generated Q17-engineering_cohere (3.2s)\n"
 ]
 },
 {
 "output_type": "stream",
 "name": "stderr",
 "text": [
 "\rQuestions: 15%|\u2588\u258c | 12/78 [02:25<12:50, 11.68s/it]"
 ]
 },
 {
 "output_type": "stream",
 "name": "stdout",
 "text": [
 " Generated Q18-marketing_cohere (1.5s)\n",
 " Generated Q18-engineering_cohere (2.4s)\n"
 ]
 },
 {
 "output_type": "stream",
 "name": "stderr",
 "text": [
 "\rQuestions: 17%|\u2588\u258b | 13/78 [02:36<12:23, 11.45s/it]"
 ]
 },
 {
 "output_type": "stream",
 "name": "stdout",
 "text": [
 " Generated Q19-marketing_cohere (0.9s)\n",
 " Generated Q19-engineering_cohere (2.0s)\n"
 ]
 },
 {
 "output_type": "stream",
 "name": "stderr",
 "text": [
 "\rQuestions: 18%|\u2588\u258a | 14/78 [02:46<11:41, 10.97s/it]"
 ]
 },
 {
 "output_type": "stream",
 "name": "stdout",
 "text": [
 " Generated Q20-marketing_cohere (45.7s)\n",
 " Generated Q20-engineering_cohere (1.8s)\n"
 ]
 },
 {
 "output_type": "stream",
 "name": "stderr",
 "text": [
 "\rQuestions: 19%|\u2588\u2589 | 15/78 [03:40<25:17, 24.09s/it]"
 ]
 },
 {
 "output_type": "stream",
 "name": "stdout",
 "text": [
 " Generated Q22-marketing_cohere (1.5s)\n",
 " Generated Q22-engineering_cohere (3.5s)\n"
 ]
 },
 {
 "output_type": "stream",
 "name": "stderr",
 "text": [
 "\rQuestions: 21%|\u2588\u2588 | 16/78 [03:52<21:09, 20.48s/it]"
 ]
 },
 {
 "output_type": "stream",
 "name": "stdout",
 "text": [
 " Generated Q23-marketing_cohere (1.2s)\n",
 " Generated Q23-engineering_cohere (2.8s)\n"
 ]
 },
 {
 "output_type": "stream",
 "name": "stderr",
 "text": [
 "\rQuestions: 22%|\u2588\u2588\u258f | 17/78 [04:03<17:55, 17.63s/it]"
 ]
 },
 {
 "output_type": "stream",
 "name": "stdout",
 "text": [
 " Generated Q24-marketing_cohere (3.2s)\n",
 " Generated Q24-engineering_cohere (2.5s)\n"
 ]
 },
 {
 "output_type": "stream",
 "name": "stderr",
 "text": [
 "\rQuestions: 23%|\u2588\u2588\u258e | 18/78 [04:16<16:09, 16.16s/it]"
 ]
 },
 {
 "output_type": "stream",
 "name": "stdout",
 "text": [
 " Generated Q25-marketing_cohere (1.8s)\n",
 " Generated Q25-engineering_cohere (2.6s)\n"
 ]
 },
 {
 "output_type": "stream",
 "name": "stderr",
 "text": [
 "\rQuestions: 24%|\u2588\u2588\u258d | 19/78 [04:28<14:31, 14.77s/it]"
 ]
 },
 {
 "output_type": "stream",
 "name": "stdout",
 "text": [
 " Generated Q27-marketing_cohere (1.8s)\n",
 " Generated Q27-engineering_cohere (4.1s)\n"
 ]
 },
 {
 "output_type": "stream",
 "name": "stderr",
 "text": [
 "\rQuestions: 26%|\u2588\u2588\u258c | 20/78 [04:41<13:44, 14.21s/it]"
 ]
 },
 {
 "output_type": "stream",
 "name": "stdout",
 "text": [
 " Generated Q28-marketing_cohere (1.4s)\n",
 " Generated Q28-engineering_cohere (1.2s)\n"
 ]
 },
 {
 "output_type": "stream",
 "name": "stderr",
 "text": [
 "\rQuestions: 27%|\u2588\u2588\u258b | 21/78 [04:50<12:12, 12.84s/it]"
 ]
 },
 {
 "output_type": "stream",
 "name": "stdout",
 "text": [
 " Generated Q30-marketing_cohere (1.7s)\n",
 " Generated Q30-engineering_cohere (3.0s)\n"
 ]
 },
 {
 "output_type": "stream",
 "name": "stderr",
 "text": [
 "\rQuestions: 28%|\u2588\u2588\u258a | 22/78 [05:02<11:41, 12.52s/it]"
 ]
 },
 {
 "output_type": "stream",
 "name": "stdout",
 "text": [
 " Generated Q33-marketing_cohere (1.7s)\n",
 " Generated Q33-engineering_cohere (3.7s)\n"
 ]
 },
 {
 "output_type": "stream",
 "name": "stderr",
 "text": [
 "\rQuestions: 29%|\u2588\u2588\u2589 | 23/78 [05:14<11:25, 12.47s/it]"
 ]
 },
 {
 "output_type": "stream",
 "name": "stdout",
 "text": [
 " Generated Q34-marketing_cohere (1.6s)\n",
 " Generated Q34-engineering_cohere (4.5s)\n"
 ]
 },
 {
 "output_type": "stream",
 "name": "stderr",
 "text": [
 "\rQuestions: 31%|\u2588\u2588\u2588 | 24/78 [05:27<11:22, 12.64s/it]"
 ]
 },
 {
 "output_type": "stream",
 "name": "stdout",
 "text": [
 " Generated Q35-marketing_cohere (2.1s)\n",
 " Generated Q35-engineering_cohere (2.6s)\n"
 ]
 },
 {
 "output_type": "stream",
 "name": "stderr",
 "text": [
 "\rQuestions: 32%|\u2588\u2588\u2588\u258f | 25/78 [05:39<10:54, 12.36s/it]"
 ]
 },
 {
 "output_type": "stream",
 "name": "stdout",
 "text": [
 " Generated Q36-marketing_cohere (1.9s)\n",
 " Generated Q36-engineering_cohere (2.7s)\n"
 ]
 },
 {
 "output_type": "stream",
 "name": "stderr",
 "text": [
 "\rQuestions: 33%|\u2588\u2588\u2588\u258e | 26/78 [05:51<10:31, 12.15s/it]"
 ]
 },
 {
 "output_type": "stream",
 "name": "stdout",
 "text": [
 " Generated Q38-marketing_cohere (1.6s)\n",
 " Generated Q38-engineering_cohere (3.8s)\n"
 ]
 },
 {
 "output_type": "stream",
 "name": "stderr",
 "text": [
 "\rQuestions: 35%|\u2588\u2588\u2588\u258d | 27/78 [06:03<10:25, 12.26s/it]"
 ]
 },
 {
 "output_type": "stream",
 "name": "stdout",
 "text": [
 " Generated Q39-marketing_cohere (1.7s)\n",
 " Generated Q39-engineering_cohere (3.4s)\n"
 ]
 },
 {
 "output_type": "stream",
 "name": "stderr",
 "text": [
 "\rQuestions: 36%|\u2588\u2588\u2588\u258c | 28/78 [06:15<10:11, 12.23s/it]"
 ]
 },
 {
 "output_type": "stream",
 "name": "stdout",
 "text": [
 " Generated Q41-marketing_cohere (1.7s)\n",
 " Generated Q41-engineering_cohere (5.5s)\n"
 ]
 },
 {
 "output_type": "stream",
 "name": "stderr",
 "text": [
 "\rQuestions: 37%|\u2588\u2588\u2588\u258b | 29/78 [06:30<10:28, 12.83s/it]"
 ]
 },
 {
 "output_type": "stream",
 "name": "stdout",
 "text": [
 " Generated Q43-marketing_cohere (1.6s)\n",
 " Generated Q43-engineering_cohere (3.9s)\n"
 ]
 },
 {
 "output_type": "stream",
 "name": "stderr",
 "text": [
 "\rQuestions: 38%|\u2588\u2588\u2588\u258a | 30/78 [06:42<10:11, 12.74s/it]"
 ]
 },
 {
 "output_type": "stream",
 "name": "stdout",
 "text": [
 " Generated Q44-marketing_cohere (1.6s)\n",
 " Generated Q44-engineering_cohere (3.3s)\n"
 ]
 },
 {
 "output_type": "stream",
 "name": "stderr",
 "text": [
 "\rQuestions: 40%|\u2588\u2588\u2588\u2589 | 31/78 [06:54<09:46, 12.48s/it]"
 ]
 },
 {
 "output_type": "stream",
 "name": "stdout",
 "text": [
 " Generated Q46-marketing_cohere (1.7s)\n",
 " Generated Q46-engineering_cohere (4.5s)\n"
 ]
 },
 {
 "output_type": "stream",
 "name": "stderr",
 "text": [
 "\rQuestions: 41%|\u2588\u2588\u2588\u2588 | 32/78 [07:07<09:44, 12.71s/it]"
 ]
 },
 {
 "output_type": "stream",
 "name": "stdout",
 "text": [
 " Generated Q47-marketing_cohere (1.5s)\n",
 " Generated Q47-engineering_cohere (3.4s)\n"
 ]
 },
 {
 "output_type": "stream",
 "name": "stderr",
 "text": [
 "\rQuestions: 42%|\u2588\u2588\u2588\u2588\u258f | 33/78 [07:19<09:20, 12.46s/it]"
 ]
 },
 {
 "output_type": "stream",
 "name": "stdout",
 "text": [
 " Generated Q48-marketing_cohere (1.4s)\n",
 " Generated Q48-engineering_cohere (2.5s)\n"
 ]
 },
 {
 "output_type": "stream",
 "name": "stderr",
 "text": [
 "\rQuestions: 44%|\u2588\u2588\u2588\u2588\u258e | 34/78 [07:30<08:47, 11.99s/it]"
 ]
 },
 {
 "output_type": "stream",
 "name": "stdout",
 "text": [
 " Generated Q50-marketing_cohere (1.4s)\n",
 " Generated Q50-engineering_cohere (3.3s)\n"
 ]
 },
 {
 "output_type": "stream",
 "name": "stderr",
 "text": [
 "\rQuestions: 45%|\u2588\u2588\u2588\u2588\u258d | 35/78 [07:42<08:32, 11.93s/it]"
 ]
 },
 {
 "output_type": "stream",
 "name": "stdout",
 "text": [
 " Generated Q51-marketing_cohere (1.9s)\n",
 " Generated Q51-engineering_cohere (3.1s)\n"
 ]
 },
 {
 "output_type": "stream",
 "name": "stderr",
 "text": [
 "\rQuestions: 46%|\u2588\u2588\u2588\u2588\u258c | 36/78 [07:54<08:21, 11.95s/it]"
 ]
 },
 {
 "output_type": "stream",
 "name": "stdout",
 "text": [
 " Generated Q52-marketing_cohere (1.7s)\n",
 " Generated Q52-engineering_cohere (3.2s)\n"
 ]
 },
 {
 "output_type": "stream",
 "name": "stderr",
 "text": [
 "\rQuestions: 47%|\u2588\u2588\u2588\u2588\u258b | 37/78 [08:06<08:09, 11.94s/it]"
 ]
 },
 {
 "output_type": "stream",
 "name": "stdout",
 "text": [
 " Generated Q53-marketing_cohere (1.3s)\n",
 " Generated Q53-engineering_cohere (3.2s)\n"
 ]
 },
 {
 "output_type": "stream",
 "name": "stderr",
 "text": [
 "\rQuestions: 49%|\u2588\u2588\u2588\u2588\u258a | 38/78 [08:17<07:52, 11.82s/it]"
 ]
 },
 {
 "output_type": "stream",
 "name": "stdout",
 "text": [
 " Generated Q54-marketing_cohere (1.2s)\n",
 " Generated Q54-engineering_cohere (2.7s)\n"
 ]
 },
 {
 "output_type": "stream",
 "name": "stderr",
 "text": [
 "\rQuestions: 50%|\u2588\u2588\u2588\u2588\u2588 | 39/78 [08:28<07:30, 11.55s/it]"
 ]
 },
 {
 "output_type": "stream",
 "name": "stdout",
 "text": [
 " Generated Q55-marketing_cohere (1.5s)\n",
 " Generated Q55-engineering_cohere (12.3s)\n"
 ]
 },
 {
 "output_type": "stream",
 "name": "stderr",
 "text": [
 "\rQuestions: 51%|\u2588\u2588\u2588\u2588\u2588\u258f | 40/78 [08:49<09:04, 14.34s/it]"
 ]
 },
 {
 "output_type": "stream",
 "name": "stdout",
 "text": [
 " Generated Q59-marketing_cohere (1.7s)\n",
 " Generated Q59-engineering_cohere (4.5s)\n"
 ]
 },
 {
 "output_type": "stream",
 "name": "stderr",
 "text": [
 "\rQuestions: 53%|\u2588\u2588\u2588\u2588\u2588\u258e | 41/78 [09:02<08:37, 13.99s/it]"
 ]
 },
 {
 "output_type": "stream",
 "name": "stdout",
 "text": [
 " Generated Q60-marketing_cohere (2.4s)\n",
 " Generated Q60-engineering_cohere (4.3s)\n"
 ]
 },
 {
 "output_type": "stream",
 "name": "stderr",
 "text": [
 "\rQuestions: 54%|\u2588\u2588\u2588\u2588\u2588\u258d | 42/78 [09:16<08:21, 13.92s/it]"
 ]
 },
 {
 "output_type": "stream",
 "name": "stdout",
 "text": [
 " Generated Q61-marketing_cohere (1.5s)\n",
 " Generated Q61-engineering_cohere (2.8s)\n"
 ]
 },
 {
 "output_type": "stream",
 "name": "stderr",
 "text": [
 "\rQuestions: 55%|\u2588\u2588\u2588\u2588\u2588\u258c | 43/78 [09:28<07:40, 13.17s/it]"
 ]
 },
 {
 "output_type": "stream",
 "name": "stdout",
 "text": [
 " Generated Q62-marketing_cohere (1.6s)\n",
 " Generated Q62-engineering_cohere (3.4s)\n"
 ]
 },
 {
 "output_type": "stream",
 "name": "stderr",
 "text": [
 "\rQuestions: 56%|\u2588\u2588\u2588\u2588\u2588\u258b | 44/78 [09:39<07:15, 12.81s/it]"
 ]
 },
 {
 "output_type": "stream",
 "name": "stdout",
 "text": [
 " Generated Q63-marketing_cohere (1.9s)\n",
 " Generated Q63-engineering_cohere (4.1s)\n"
 ]
 },
 {
 "output_type": "stream",
 "name": "stderr",
 "text": [
 "\rQuestions: 58%|\u2588\u2588\u2588\u2588\u2588\u258a | 45/78 [09:53<07:04, 12.87s/it]"
 ]
 },
 {
 "output_type": "stream",
 "name": "stdout",
 "text": [
 " Generated Q64-marketing_cohere (1.6s)\n",
 " Generated Q64-engineering_cohere (3.9s)\n"
 ]
 },
 {
 "output_type": "stream",
 "name": "stderr",
 "text": [
 "\rQuestions: 59%|\u2588\u2588\u2588\u2588\u2588\u2589 | 46/78 [10:05<06:48, 12.77s/it]"
 ]
 },
 {
 "output_type": "stream",
 "name": "stdout",
 "text": [
 " Generated Q65-marketing_cohere (1.6s)\n",
 " Generated Q65-engineering_cohere (3.6s)\n"
 ]
 },
 {
 "output_type": "stream",
 "name": "stderr",
 "text": [
 "\rQuestions: 60%|\u2588\u2588\u2588\u2588\u2588\u2588 | 47/78 [10:17<06:30, 12.61s/it]"
 ]
 },
 {
 "output_type": "stream",
 "name": "stdout",
 "text": [
 " Generated Q67-marketing_cohere (1.3s)\n",
 " Generated Q67-engineering_cohere (3.9s)\n"
 ]
 },
 {
 "output_type": "stream",
 "name": "stderr",
 "text": [
 "\rQuestions: 62%|\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 48/78 [10:30<06:15, 12.51s/it]"
 ]
 },
 {
 "output_type": "stream",
 "name": "stdout",
 "text": [
 " Generated Q69-marketing_cohere (1.7s)\n",
 " Generated Q69-engineering_cohere (3.0s)\n"
 ]
 },
 {
 "output_type": "stream",
 "name": "stderr",
 "text": [
 "\rQuestions: 63%|\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 49/78 [10:41<05:55, 12.27s/it]"
 ]
 },
 {
 "output_type": "stream",
 "name": "stdout",
 "text": [
 " Generated Q73-marketing_cohere (1.1s)\n",
 " Generated Q73-engineering_cohere (1.9s)\n"
 ]
 },
 {
 "output_type": "stream",
 "name": "stderr",
 "text": [
 "\rQuestions: 64%|\u2588\u2588\u2588\u2588\u2588\u2588\u258d | 50/78 [10:51<05:25, 11.62s/it]"
 ]
 },
 {
 "output_type": "stream",
 "name": "stdout",
 "text": [
 " Generated Q74-marketing_cohere (1.6s)\n",
 " Generated Q74-engineering_cohere (3.5s)\n"
 ]
 },
 {
 "output_type": "stream",
 "name": "stderr",
 "text": [
 "\rQuestions: 65%|\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 51/78 [11:03<05:17, 11.76s/it]"
 ]
 },
 {
 "output_type": "stream",
 "name": "stdout",
 "text": [
 " Generated Q75-marketing_cohere (1.7s)\n",
 " Generated Q75-engineering_cohere (2.5s)\n"
 ]
 },
 {
 "output_type": "stream",
 "name": "stderr",
 "text": [
 "\rQuestions: 67%|\u2588\u2588\u2588\u2588\u2588\u2588\u258b | 52/78 [11:15<05:01, 11.60s/it]"
 ]
 },
 {
 "output_type": "stream",
 "name": "stdout",
 "text": [
 " Generated Q76-marketing_cohere (2.2s)\n",
 " Generated Q76-engineering_cohere (3.2s)\n"
 ]
 },
 {
 "output_type": "stream",
 "name": "stderr",
 "text": [
 "\rQuestions: 68%|\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 53/78 [11:27<04:56, 11.84s/it]"
 ]
 },
 {
 "output_type": "stream",
 "name": "stdout",
 "text": [
 " Generated Q78-marketing_cohere (1.8s)\n",
 " Generated Q78-engineering_cohere (3.6s)\n"
 ]
 },
 {
 "output_type": "stream",
 "name": "stderr",
 "text": [
 "\rQuestions: 69%|\u2588\u2588\u2588\u2588\u2588\u2588\u2589 | 54/78 [11:40<04:48, 12.02s/it]"
 ]
 },
 {
 "output_type": "stream",
 "name": "stdout",
 "text": [
 " Generated Q80-marketing_cohere (1.3s)\n",
 " Generated Q80-engineering_cohere (3.0s)\n"
 ]
 },
 {
 "output_type": "stream",
 "name": "stderr",
 "text": [
 "\rQuestions: 71%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 55/78 [11:51<04:31, 11.82s/it]"
 ]
 },
 {
 "output_type": "stream",
 "name": "stdout",
 "text": [
 " Generated Q81-marketing_cohere (2.0s)\n",
 " Generated Q81-engineering_cohere (3.2s)\n"
 ]
 },
 {
 "output_type": "stream",
 "name": "stderr",
 "text": [
 "\rQuestions: 72%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 56/78 [12:03<04:22, 11.93s/it]"
 ]
 },
 {
 "output_type": "stream",
 "name": "stdout",
 "text": [
 " Generated Q82-marketing_cohere (1.5s)\n",
 " Generated Q82-engineering_cohere (3.1s)\n"
 ]
 },
 {
 "output_type": "stream",
 "name": "stderr",
 "text": [
 "\rQuestions: 73%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 57/78 [12:15<04:08, 11.84s/it]"
 ]
 },
 {
 "output_type": "stream",
 "name": "stdout",
 "text": [
 " Generated Q84-marketing_cohere (1.3s)\n",
 " Generated Q84-engineering_cohere (2.5s)\n"
 ]
 },
 {
 "output_type": "stream",
 "name": "stderr",
 "text": [
 "\rQuestions: 74%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d | 58/78 [12:26<03:50, 11.55s/it]"
 ]
 },
 {
 "output_type": "stream",
 "name": "stdout",
 "text": [
 " Generated Q85-marketing_cohere (1.8s)\n",
 " Generated Q85-engineering_cohere (3.3s)\n"
 ]
 },
 {
 "output_type": "stream",
 "name": "stderr",
 "text": [
 "\rQuestions: 76%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 59/78 [12:38<03:42, 11.71s/it]"
 ]
 },
 {
 "output_type": "stream",
 "name": "stdout",
 "text": [
 " Generated Q86-marketing_cohere (1.5s)\n",
 " Generated Q86-engineering_cohere (2.9s)\n"
 ]
 },
 {
 "output_type": "stream",
 "name": "stderr",
 "text": [
 "\rQuestions: 77%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b | 60/78 [12:49<03:29, 11.61s/it]"
 ]
 },
 {
 "output_type": "stream",
 "name": "stdout",
 "text": [
 " Generated Q87-marketing_cohere (2.2s)\n",
 " Generated Q87-engineering_cohere (2.7s)\n"
 ]
 },
 {
 "output_type": "stream",
 "name": "stderr",
 "text": [
 "\rQuestions: 78%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 61/78 [13:01<03:18, 11.70s/it]"
 ]
 },
 {
 "output_type": "stream",
 "name": "stdout",
 "text": [
 " Generated Q88-marketing_cohere (1.5s)\n",
 " Generated Q88-engineering_cohere (2.5s)\n"
 ]
 },
 {
 "output_type": "stream",
 "name": "stderr",
 "text": [
 "\rQuestions: 79%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589 | 62/78 [13:12<03:03, 11.47s/it]"
 ]
 },
 {
 "output_type": "stream",
 "name": "stdout",
 "text": [
 " Generated Q89-marketing_cohere (1.7s)\n",
 " Generated Q89-engineering_cohere (2.7s)\n"
 ]
 },
 {
 "output_type": "stream",
 "name": "stderr",
 "text": [
 "\rQuestions: 81%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 63/78 [13:23<02:51, 11.46s/it]"
 ]
 },
 {
 "output_type": "stream",
 "name": "stdout",
 "text": [
 " Generated Q91-marketing_cohere (1.6s)\n",
 " Generated Q91-engineering_cohere (2.9s)\n"
 ]
 },
 {
 "output_type": "stream",
 "name": "stderr",
 "text": [
 "\rQuestions: 82%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 64/78 [13:35<02:40, 11.48s/it]"
 ]
 },
 {
 "output_type": "stream",
 "name": "stdout",
 "text": [
 " Generated Q92-marketing_cohere (1.1s)\n",
 " Generated Q92-engineering_cohere (2.6s)\n"
 ]
 },
 {
 "output_type": "stream",
 "name": "stderr",
 "text": [
 "\rQuestions: 83%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 65/78 [13:46<02:26, 11.24s/it]"
 ]
 },
 {
 "output_type": "stream",
 "name": "stdout",
 "text": [
 " Generated Q93-marketing_cohere (1.5s)\n",
 " Generated Q93-engineering_cohere (2.7s)\n"
 ]
 },
 {
 "output_type": "stream",
 "name": "stderr",
 "text": [
 "\rQuestions: 85%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d | 66/78 [13:57<02:15, 11.25s/it]"
 ]
 },
 {
 "output_type": "stream",
 "name": "stdout",
 "text": [
 " Generated Q94-marketing_cohere (1.2s)\n",
 " Generated Q94-engineering_cohere (4.2s)\n"
 ]
 },
 {
 "output_type": "stream",
 "name": "stderr",
 "text": [
 "\rQuestions: 86%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 67/78 [14:09<02:07, 11.62s/it]"
 ]
 },
 {
 "output_type": "stream",
 "name": "stdout",
 "text": [
 " Generated Q95-marketing_cohere (1.6s)\n",
 " Generated Q95-engineering_cohere (2.7s)\n"
 ]
 },
 {
 "output_type": "stream",
 "name": "stderr",
 "text": [
 "\rQuestions: 87%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b | 68/78 [14:21<01:55, 11.53s/it]"
 ]
 },
 {
 "output_type": "stream",
 "name": "stdout",
 "text": [
 " Generated Q96-marketing_cohere (1.9s)\n",
 " Generated Q96-engineering_cohere (4.0s)\n"
 ]
 },
 {
 "output_type": "stream",
 "name": "stderr",
 "text": [
 "\rQuestions: 88%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 69/78 [14:34<01:47, 11.97s/it]"
 ]
 },
 {
 "output_type": "stream",
 "name": "stdout",
 "text": [
 " Generated Q101-marketing_cohere (1.5s)\n",
 " Generated Q101-engineering_cohere (3.2s)\n"
 ]
 },
 {
 "output_type": "stream",
 "name": "stderr",
 "text": [
 "\rQuestions: 90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589 | 70/78 [14:45<01:35, 11.88s/it]"
 ]
 },
 {
 "output_type": "stream",
 "name": "stdout",
 "text": [
 " Generated Q102-marketing_cohere (6.1s)\n",
 " Generated Q102-engineering_cohere (3.9s)\n"
 ]
 },
 {
 "output_type": "stream",
 "name": "stderr",
 "text": [
 "\rQuestions: 91%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 71/78 [15:02<01:33, 13.40s/it]"
 ]
 },
 {
 "output_type": "stream",
 "name": "stdout",
 "text": [
 " Generated Q103-marketing_cohere (2.0s)\n",
 " Generated Q103-engineering_cohere (2.9s)\n"
 ]
 },
 {
 "output_type": "stream",
 "name": "stderr",
 "text": [
 "\rQuestions: 92%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f| 72/78 [15:14<01:17, 12.95s/it]"
 ]
 },
 {
 "output_type": "stream",
 "name": "stdout",
 "text": [
 " Generated Q104-marketing_cohere (2.0s)\n",
 " Generated Q104-engineering_cohere (2.6s)\n"
 ]
 },
 {
 "output_type": "stream",
 "name": "stderr",
 "text": [
 "\rQuestions: 94%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e| 73/78 [15:26<01:02, 12.57s/it]"
 ]
 },
 {
 "output_type": "stream",
 "name": "stdout",
 "text": [
 " Generated Q105-marketing_cohere (1.4s)\n",
 " Generated Q105-engineering_cohere (2.3s)\n"
 ]
 },
 {
 "output_type": "stream",
 "name": "stderr",
 "text": [
 "\rQuestions: 95%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d| 74/78 [15:36<00:48, 12.00s/it]"
 ]
 },
 {
 "output_type": "stream",
 "name": "stdout",
 "text": [
 " Generated Q106-marketing_cohere (1.5s)\n",
 " Generated Q106-engineering_cohere (3.1s)\n"
 ]
 },
 {
 "output_type": "stream",
 "name": "stderr",
 "text": [
 "\rQuestions: 96%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 75/78 [15:48<00:35, 11.90s/it]"
 ]
 },
 {
 "output_type": "stream",
 "name": "stdout",
 "text": [
 " Generated Q107-marketing_cohere (1.3s)\n",
 " Generated Q107-engineering_cohere (2.4s)\n"
 ]
 },
 {
 "output_type": "stream",
 "name": "stderr",
 "text": [
 "\rQuestions: 97%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b| 76/78 [15:59<00:23, 11.53s/it]"
 ]
 },
 {
 "output_type": "stream",
 "name": "stdout",
 "text": [
 " Generated Q108-marketing_cohere (3.5s)\n",
 " Generated Q108-engineering_cohere (3.0s)\n"
 ]
 },
 {
 "output_type": "stream",
 "name": "stderr",
 "text": [
 "\rQuestions: 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 77/78 [16:12<00:12, 12.12s/it]"
 ]
 },
 {
 "output_type": "stream",
 "name": "stdout",
 "text": [
 " Generated Q110-marketing_cohere (1.5s)\n",
 " Generated Q110-engineering_cohere (3.8s)\n"
 ]
 },
 {
 "output_type": "stream",
 "name": "stderr",
 "text": [
 "Questions: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 78/78 [16:25<00:00, 12.63s/it]\n"
 ]
 },
 {
 "output_type": "stream",
 "name": "stdout",
 "text": [
 "PHASE 1 Complete: 156 responses generated\n",
 "PHASE 2: Batch Evaluation on 156 items...\n",
 " Running BERTScore in batch mode...\n"
 ]
 },
 {
 "output_type": "stream",
 "name": "stderr",
 "text": [
 "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
 "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
 ]
 },
 {
 "output_type": "stream",
 "name": "stdout",
 "text": [
 " BERTScore complete (6.8s for 156 items)\n",
 " Running RAGAS in batch mode...\n"
 ]
 },
 {
 "output_type": "stream",
 "name": "stderr",
 "text": [
 "/tmp/ipython-input-2192463817.py:124: DeprecationWarning: LangchainLLMWrapper is deprecated and will be removed in a future version. Use llm_factory instead: from openai import OpenAI; from ragas.llms import llm_factory; llm = llm_factory('gpt-4o-mini', client=OpenAI(api_key='...'))\n",
 " evaluator_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4o-mini\"))\n"
 ]
 },
 {
 "output_type": "display_data",
 "data": {
 "text/plain": [
 "Evaluating: 0%| | 0/780 [00:00<?, ?it/s]"
 ],
 "application/vnd.jupyter.widget-view+json": {
 "version_major": 2,
 "version_minor": 0,
 "model_id": "1ca385be4bc54a00b5e36a7ab793b4b9"
 }
 },
 "metadata": {}
 },
 {
 "output_type": "stream",
 "name": "stderr",
 "text": [
 "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
 "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
 "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
 "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
 "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
 "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
 "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
 "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
 "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
 "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
 "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
 "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
 "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
 "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
 "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
 "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
 "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
 "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
 "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
 "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
 "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
 "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
 "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
 "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
 "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
 "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
 "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
 "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
 "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
 "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
 "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
 "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
 "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
 "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
 "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
 "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
 "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
 "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
 "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
 "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
 "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
 "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
 "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
 "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
 "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
 "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
 "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
 "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
 "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
 "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
 "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
 "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
 "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
 "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
 "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
 "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
 "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
 "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
 "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
 "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
 "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
 "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
 "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
 "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
 "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
 "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
 "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
 "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
 "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
 "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
 "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
 "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
 "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
 "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
 "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
 "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
 "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
 "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
 "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
 "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
 "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
 "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
 "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
 "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
 "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
 "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
 "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
 "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
 "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
 "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
 "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
 "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
 "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
 "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
 "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
 "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
 "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
 "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
 "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
 "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
 "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n"
 ]
 },
 {
 "output_type": "stream",
 "name": "stdout",
 "text": [
 " RAGAS complete (1008.4s for 156 items)\n",
 "\n",
 "======================================================================\n",
 "EVALUATION COMPLETE!\n",
 "======================================================================\n",
 "Total items evaluated: 156\n",
 "Questions tested: [np.int64(0), np.int64(1), np.int64(2), np.int64(3), np.int64(7), np.int64(8), np.int64(9), np.int64(11), np.int64(12), np.int64(13), np.int64(16), np.int64(17), np.int64(18), np.int64(19), np.int64(20), np.int64(22), np.int64(23), np.int64(24), np.int64(25), np.int64(27), np.int64(28), np.int64(30), np.int64(33), np.int64(34), np.int64(35), np.int64(36), np.int64(38), np.int64(39), np.int64(41), np.int64(43), np.int64(44), np.int64(46), np.int64(47), np.int64(48), np.int64(50), np.int64(51), np.int64(52), np.int64(53), np.int64(54), np.int64(55), np.int64(59), np.int64(60), np.int64(61), np.int64(62), np.int64(63), np.int64(64), np.int64(65), np.int64(67), np.int64(69), np.int64(73), np.int64(74), np.int64(75), np.int64(76), np.int64(78), np.int64(80), np.int64(81), np.int64(82), np.int64(84), np.int64(85), np.int64(86), np.int64(87), np.int64(88), np.int64(89), np.int64(91), np.int64(92), np.int64(93), np.int64(94), np.int64(95), np.int64(96), np.int64(101), np.int64(102), np.int64(103), np.int64(104), np.int64(105), np.int64(106), np.int64(107), np.int64(108), np.int64(110)]\n",
 "Configs tested: ['engineering_cohere', 'marketing_cohere']\n",
 "Results saved to: /content/drive/MyDrive/evaluation_results_checkpoint.pkl\n",
 "\n",
 "Quick preview:\n",
 " bert_f1 ragas_context_recall ragas_context_precision \\\n",
 "chain_type \n",
 "engineering_cohere 0.233710 0.590385 0.646302 \n",
 "marketing_cohere 0.311656 0.678419 0.567487 \n",
 "\n",
 " ragas_faithfulness ragas_factual_correctness \\\n",
 "chain_type \n",
 "engineering_cohere 0.853450 0.276154 \n",
 "marketing_cohere 0.632257 0.250385 \n",
 "\n",
 " ragas_answer_relevancy \n",
 "chain_type \n",
 "engineering_cohere 0.800231 \n",
 "marketing_cohere 0.821626 \n",
 "\n",
 "======================================================================\n",
 "RETRIEVED DOCUMENT SOURCES\n",
 "======================================================================\n",
 "\n",
 "Q0: What defines a large language model in the context of natura...\n",
 "Sources: ['https://arxiv.org/pdf/2203.15556.pdf', 'https://en.wikipedia.org/wiki/Llama_(language_model)', 'https://en.wikipedia.org/wiki/List_of_large_language_models', 'https://en.wikipedia.org/wiki/Large_language_model', 'https://arxiv.org/pdf/2203.15556.pdf', 'https://en.wikipedia.org/wiki/Large_language_model', 'https://arxiv.org/pdf/2310.06825.pdf', 'https://lilianweng.github.io/posts/2020-10-29-odqa/']\n",
 "\n",
 "Q1: How do large language models like GPT-3 become capable of te...\n",
 "Sources: ['https://en.wikipedia.org/wiki/Llama_(language_model)', 'https://en.wikipedia.org/wiki/Generative_pre-trained_transformer', 'https://en.wikipedia.org/wiki/Generative_pre-trained_transformer', 'https://arxiv.org/pdf/2203.15556.pdf', 'https://en.wikipedia.org/wiki/Large_language_model', 'https://arxiv.org/pdf/2312.10997.pdf', 'https://arxiv.org/pdf/2203.02155.pdf', 'https://lilianweng.github.io/posts/2020-10-29-odqa/']\n",
 "\n",
 "Q2: What are some of the architectures used in building artifici...\n",
 "Sources: ['https://lilianweng.github.io/posts/2023-06-23-agent/', 'https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/', 'https://en.wikipedia.org/wiki/Llama_(language_model)', 'https://en.wikipedia.org/wiki/Large_language_model', 'https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/', 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'https://en.wikipedia.org/wiki/Knowledge_cutoff', 'https://en.wikipedia.org/wiki/Generative_artificial_intelligence']\n",
 "\n",
 "Q3: Can you name some notable large language models and their re...\n",
 "Sources: ['https://en.wikipedia.org/wiki/Llama_(language_model)', 'https://en.wikipedia.org/wiki/List_of_large_language_models', 'https://en.wikipedia.org/wiki/Large_language_model', 'https://en.wikipedia.org/wiki/Llama_(language_model)', 'https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/', 'https://arxiv.org/pdf/2310.06825.pdf', 'https://arxiv.org/pdf/2203.15556.pdf', 'https://arxiv.org/pdf/2203.15556.pdf']\n",
 "\n",
 "Q7: What licensing terms are associated with source-available mo...\n",
 "Sources: ['https://arxiv.org/pdf/2203.02155.pdf', 'https://en.wikipedia.org/wiki/Llama_(language_model)', 'https://arxiv.org/pdf/2410.15944.pdf', 'https://en.wikipedia.org/wiki/Llama_(language_model)', 'https://arxiv.org/pdf/2310.06825.pdf', 'https://en.wikipedia.org/wiki/Contextual_AI', 'https://en.wikipedia.org/wiki/Generative_pre-trained_transformer', 'https://en.wikipedia.org/wiki/Generative_artificial_intelligence']\n",
 "\n",
 "Q8: What are the main applications of language models?...\n",
 "Sources: ['https://arxiv.org/pdf/2203.15556.pdf', 'https://en.wikipedia.org/wiki/Llama_(language_model)', 'https://en.wikipedia.org/wiki/Large_language_model', 'https://arxiv.org/pdf/2203.15556.pdf', 'https://arxiv.org/pdf/2310.06825.pdf', 'https://en.wikipedia.org/wiki/List_of_large_language_models', 'https://arxiv.org/pdf/2005.11401.pdf', 'https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/']\n",
 "\n",
 "Q9: Who proposed the first significant statistical language mode...\n",
 "Sources: ['https://en.wikipedia.org/wiki/Large_language_model', 'https://en.wikipedia.org/wiki/Llama_(language_model)', 'https://arxiv.org/pdf/2203.15556.pdf', 'https://arxiv.org/pdf/2310.06825.pdf', 'https://en.wikipedia.org/wiki/List_of_large_language_models', 'https://arxiv.org/pdf/2401.17268.pdf', 'https://arxiv.org/pdf/2203.15556.pdf', 'https://arxiv.org/pdf/2005.11401.pdf']\n",
 "\n",
 "Q11: Which components have allowed large language models to surpa...\n",
 "Sources: ['https://en.wikipedia.org/wiki/Llama_(language_model)', 'https://arxiv.org/pdf/2203.15556.pdf', 'https://arxiv.org/pdf/2310.06825.pdf', 'https://en.wikipedia.org/wiki/List_of_large_language_models', 'https://en.wikipedia.org/wiki/Large_language_model', 'https://arxiv.org/pdf/2203.15556.pdf', 'https://arxiv.org/pdf/2402.19473.pdf', 'https://arxiv.org/pdf/2203.15556.pdf']\n",
 "\n",
 "Q12: What is a common strategy used by language models to address...\n",
 "Sources: ['https://en.wikipedia.org/wiki/Llama_(language_model)', 'https://arxiv.org/pdf/2203.15556.pdf', 'https://arxiv.org/pdf/2203.15556.pdf', 'https://arxiv.org/pdf/2005.11401.pdf', 'https://arxiv.org/pdf/2203.02155.pdf', 'https://en.wikipedia.org/wiki/Large_language_model', 'https://arxiv.org/pdf/2306.15595.pdf', 'https://arxiv.org/pdf/2310.06825.pdf']\n",
 "\n",
 "Q13: Why might large language models not be considered plausible ...\n",
 "Sources: ['https://en.wikipedia.org/wiki/Llama_(language_model)', 'https://arxiv.org/pdf/2203.15556.pdf', 'https://arxiv.org/pdf/2203.15556.pdf', 'https://arxiv.org/pdf/2310.06825.pdf', 'https://lilianweng.github.io/posts/2020-10-29-odqa/', 'https://arxiv.org/pdf/2005.11401.pdf', 'https://en.wikipedia.org/wiki/List_of_large_language_models', 'https://arxiv.org/pdf/2203.15556.pdf']\n",
 "\n",
 "Q16: What is the purpose of the constitution in training AI syste...\n",
 "Sources: ['https://en.wikipedia.org/wiki/Generative_artificial_intelligence', 'https://arxiv.org/pdf/2203.02155.pdf', 'https://en.wikipedia.org/wiki/Generative_artificial_intelligence', 'https://arxiv.org/pdf/2203.02155.pdf', 'https://arxiv.org/pdf/2401.17268.pdf', 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'https://arxiv.org/pdf/2305.18290.pdf', 'https://arxiv.org/pdf/2203.02155.pdf']\n",
 "\n",
 "Q17: What is the meaning of the term alignment tax in the context...\n",
 "Sources: ['https://arxiv.org/pdf/2203.02155.pdf', 'https://arxiv.org/pdf/2203.02155.pdf', 'https://arxiv.org/pdf/2203.02155.pdf', 'https://arxiv.org/pdf/2203.02155.pdf', 'https://lilianweng.github.io/posts/2018-06-24-attention/', 'https://arxiv.org/pdf/2203.02155.pdf', 'https://arxiv.org/pdf/2203.02155.pdf', 'https://arxiv.org/pdf/2203.02155.pdf']\n",
 "\n",
 "Q18: How does the release of successive models in a language mode...\n",
 "Sources: ['https://arxiv.org/pdf/2005.11401.pdf', 'https://arxiv.org/pdf/2203.15556.pdf', 'https://arxiv.org/pdf/2203.02155.pdf', 'https://arxiv.org/pdf/2306.15595.pdf', 'https://en.wikipedia.org/wiki/Llama_(language_model)', 'https://arxiv.org/pdf/2203.15556.pdf', 'https://en.wikipedia.org/wiki/Knowledge_cutoff', 'https://arxiv.org/pdf/2203.15556.pdf']\n",
 "\n",
 "Q19: What is the significant enhancement in Claude 2.1 compared t...\n",
 "Sources: ['https://arxiv.org/pdf/2406.04744.pdf', 'https://arxiv.org/pdf/2106.09685.pdf', 'https://arxiv.org/pdf/2203.15556.pdf', 'https://arxiv.org/pdf/2106.09685.pdf', 'https://arxiv.org/pdf/2310.06825.pdf', 'https://arxiv.org/pdf/2106.09685.pdf', 'https://en.wikipedia.org/wiki/Llama_(language_model)', 'https://arxiv.org/pdf/2203.15556.pdf']\n",
 "\n",
 "Q20: In what way can a language model demonstrate meta-cognitive ...\n",
 "Sources: ['https://arxiv.org/pdf/2203.15556.pdf', 'https://en.wikipedia.org/wiki/Llama_(language_model)', 'https://en.wikipedia.org/wiki/Prompt_engineering', 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'https://arxiv.org/pdf/2105.03011.pdf', 'https://lilianweng.github.io/posts/2020-10-29-odqa/', 'https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/', 'https://arxiv.org/pdf/2310.06825.pdf']\n",
 "\n",
 "Q22: How can a language model's ability to analyze images expand ...\n",
 "Sources: ['https://en.wikipedia.org/wiki/Llama_(language_model)', 'https://arxiv.org/pdf/2203.15556.pdf', 'https://arxiv.org/pdf/2203.15556.pdf', 'https://arxiv.org/pdf/2310.06825.pdf', 'https://arxiv.org/pdf/2211.12561.pdf', 'https://arxiv.org/pdf/2211.12561.pdf', 'https://arxiv.org/pdf/2211.12561.pdf', 'https://arxiv.org/pdf/2211.12561.pdf']\n",
 "\n",
 "Q23: What are some ethical considerations that come into play whe...\n",
 "Sources: ['https://arxiv.org/pdf/2203.02155.pdf', 'https://en.wikipedia.org/wiki/Llama_(language_model)', 'https://arxiv.org/pdf/2203.02155.pdf', 'https://arxiv.org/pdf/2203.15556.pdf', 'https://arxiv.org/pdf/2203.15556.pdf', 'https://arxiv.org/pdf/2310.06825.pdf', 'https://arxiv.org/pdf/2203.02155.pdf', 'https://arxiv.org/pdf/2203.02155.pdf']\n",
 "\n",
 "Q24: Who developed the language model family known as Chinchilla?...\n",
 "Sources: ['https://arxiv.org/pdf/2203.15556.pdf', 'https://arxiv.org/pdf/2203.15556.pdf', 'https://arxiv.org/pdf/2203.15556.pdf', 'https://arxiv.org/pdf/2203.15556.pdf', 'https://en.wikipedia.org/wiki/Llama_(language_model)', 'https://arxiv.org/pdf/2203.15556.pdf', 'https://arxiv.org/pdf/2203.15556.pdf', 'https://arxiv.org/pdf/2203.15556.pdf']\n",
 "\n",
 "Q25: What benchmark did Chinchilla achieve an average accuracy of...\n",
 "Sources: ['https://arxiv.org/pdf/2203.15556.pdf', 'https://arxiv.org/pdf/2203.15556.pdf', 'https://arxiv.org/pdf/2203.15556.pdf', 'https://arxiv.org/pdf/2203.15556.pdf', 'https://arxiv.org/pdf/2203.15556.pdf', 'https://arxiv.org/pdf/2203.15556.pdf', 'https://arxiv.org/pdf/2203.15556.pdf', 'https://arxiv.org/pdf/2203.15556.pdf']\n",
 "\n",
 "Q27: What is the significance of the accuracy percentage achieved...\n",
 "Sources: ['https://arxiv.org/pdf/2203.15556.pdf', 'https://arxiv.org/pdf/2203.15556.pdf', 'https://arxiv.org/pdf/2203.15556.pdf', 'https://arxiv.org/pdf/2203.15556.pdf', 'https://arxiv.org/pdf/2305.14314.pdf', 'https://arxiv.org/pdf/2203.15556.pdf', 'https://arxiv.org/pdf/2203.15556.pdf', 'https://arxiv.org/pdf/2203.15556.pdf']\n",
 "\n",
 "Q28: Why is Chinchilla considered more efficient in terms of comp...\n",
 "Sources: ['https://arxiv.org/pdf/2203.15556.pdf', 'https://arxiv.org/pdf/2203.15556.pdf', 'https://arxiv.org/pdf/2203.15556.pdf', 'https://arxiv.org/pdf/2203.15556.pdf', 'https://arxiv.org/pdf/2203.15556.pdf', 'https://arxiv.org/pdf/2203.15556.pdf', 'https://arxiv.org/pdf/2203.15556.pdf', 'https://arxiv.org/pdf/2203.15556.pdf']\n",
 "\n",
 "Q30: What is the recommended strategy for training large autoregr...\n",
 "Sources: ['https://arxiv.org/pdf/2203.15556.pdf', 'https://arxiv.org/pdf/2203.15556.pdf', 'https://arxiv.org/pdf/2203.15556.pdf', 'https://arxiv.org/pdf/2203.15556.pdf', 'https://arxiv.org/pdf/2203.15556.pdf', 'https://arxiv.org/pdf/2203.15556.pdf', 'https://arxiv.org/pdf/2401.17268.pdf', 'https://arxiv.org/pdf/2203.15556.pdf']\n",
 "\n",
 "Q33: What assumptions must be met in order for the reparameteriza...\n",
 "Sources: ['https://arxiv.org/pdf/2305.18290.pdf', 'https://arxiv.org/pdf/2305.18290.pdf', 'https://arxiv.org/pdf/2305.18290.pdf', 'https://arxiv.org/pdf/2305.18290.pdf', 'https://arxiv.org/pdf/2305.18290.pdf', 'https://arxiv.org/pdf/2305.18290.pdf', 'https://arxiv.org/pdf/2305.18290.pdf', 'https://arxiv.org/pdf/2305.18290.pdf']\n",
 "\n",
 "Q34: What are some of the limitations of traditional position enc...\n",
 "Sources: ['https://arxiv.org/pdf/2306.15595.pdf', 'https://arxiv.org/pdf/2306.15595.pdf', 'https://arxiv.org/pdf/2306.15595.pdf', 'https://arxiv.org/pdf/2104.09864.pdf', 'https://arxiv.org/pdf/2104.09864.pdf', 'https://arxiv.org/pdf/2104.09864.pdf', 'https://arxiv.org/pdf/2104.09864.pdf', 'https://arxiv.org/pdf/2104.09864.pdf']\n",
 "\n",
 "Q35: How does the Rotary Position Embedding (RoPE) approach in Tr...\n",
 "Sources: ['https://arxiv.org/pdf/2104.09864.pdf', 'https://arxiv.org/pdf/2104.09864.pdf', 'https://arxiv.org/pdf/2104.09864.pdf', 'https://arxiv.org/pdf/2306.15595.pdf', 'https://arxiv.org/pdf/2104.09864.pdf', 'https://arxiv.org/pdf/2104.09864.pdf', 'https://arxiv.org/pdf/2104.09864.pdf', 'https://arxiv.org/pdf/2104.09864.pdf']\n",
 "\n",
 "Q36: What approaches or methods are suggested for improving the a...\n",
 "Sources: ['https://arxiv.org/pdf/2203.02155.pdf', 'https://arxiv.org/pdf/2203.02155.pdf', 'https://arxiv.org/pdf/2203.02155.pdf', 'https://arxiv.org/pdf/2203.02155.pdf', 'https://arxiv.org/pdf/2305.18290.pdf', 'https://arxiv.org/pdf/2203.02155.pdf', 'https://arxiv.org/pdf/2203.02155.pdf', 'https://arxiv.org/pdf/2401.17268.pdf']\n",
 "\n",
 "Q38: What methods have been explored to improve the alignment of ...\n",
 "Sources: ['https://arxiv.org/pdf/2203.02155.pdf', 'https://arxiv.org/pdf/2203.02155.pdf', 'https://arxiv.org/pdf/2203.02155.pdf', 'https://arxiv.org/pdf/2203.02155.pdf', 'https://arxiv.org/pdf/2203.02155.pdf', 'https://arxiv.org/pdf/2203.02155.pdf', 'https://arxiv.org/pdf/2203.02155.pdf', 'https://arxiv.org/pdf/2401.17268.pdf']\n",
 "\n",
 "Q39: How does the uniqueness of a reparameterized reward function...\n",
 "Sources: ['https://arxiv.org/pdf/2305.18290.pdf', 'https://arxiv.org/pdf/2305.18290.pdf', 'https://arxiv.org/pdf/2305.18290.pdf', 'https://arxiv.org/pdf/2305.18290.pdf', 'https://arxiv.org/pdf/2305.18290.pdf', 'https://arxiv.org/pdf/2305.18290.pdf', 'https://arxiv.org/pdf/2305.18290.pdf', 'https://arxiv.org/pdf/2305.18290.pdf']\n",
 "\n",
 "Q41: Question: When conducting demographic and technical assessme...\n",
 "Sources: ['https://arxiv.org/pdf/2410.15944.pdf', 'https://arxiv.org/pdf/2203.02155.pdf', 'https://arxiv.org/pdf/2203.02155.pdf', 'https://arxiv.org/pdf/2309.08872.pdf', 'https://arxiv.org/pdf/2203.02155.pdf', 'https://arxiv.org/pdf/2203.02155.pdf', 'https://arxiv.org/pdf/2203.15556.pdf', 'https://arxiv.org/pdf/2312.10997.pdf']\n",
 "\n",
 "Q43: How does the evaluation process determine the level of align...\n",
 "Sources: ['https://arxiv.org/pdf/2402.01306.pdf', 'https://arxiv.org/pdf/2402.01306.pdf', 'https://arxiv.org/pdf/2203.02155.pdf', 'https://arxiv.org/pdf/2203.02155.pdf', 'https://arxiv.org/pdf/2203.02155.pdf', 'https://arxiv.org/pdf/2203.02155.pdf', 'https://arxiv.org/pdf/2203.02155.pdf', 'https://lilianweng.github.io/posts/2024-07-07-hallucination/']\n",
 "\n",
 "Q44: What approaches can be used to improve the performance of an...\n",
 "Sources: ['https://en.wikipedia.org/wiki/Generative_artificial_intelligence', 'https://en.wikipedia.org/wiki/Contextual_AI', 'https://lilianweng.github.io/posts/2020-10-29-odqa/', 'https://en.wikipedia.org/wiki/Generative_pre-trained_transformer', 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'https://arxiv.org/pdf/2203.02155.pdf', 'https://arxiv.org/pdf/2305.14314.pdf']\n",
 "\n",
 "Q46: What are some common strategies for addressing the ethical a...\n",
 "Sources: ['https://arxiv.org/pdf/2203.02155.pdf', 'https://arxiv.org/pdf/2203.15556.pdf', 'https://arxiv.org/pdf/2203.02155.pdf', 'https://arxiv.org/pdf/2203.02155.pdf', 'https://arxiv.org/pdf/2203.02155.pdf', 'https://arxiv.org/pdf/2203.02155.pdf', 'https://arxiv.org/pdf/2203.15556.pdf', 'https://arxiv.org/pdf/2203.02155.pdf']\n",
 "\n",
 "Q47: What categories are the listed companies classified into, an...\n",
 "Sources: ['https://arxiv.org/pdf/2309.08872.pdf', 'https://arxiv.org/pdf/2401.17268.pdf', 'https://arxiv.org/pdf/2410.12812.pdf', 'https://arxiv.org/pdf/2312.10997.pdf', 'https://arxiv.org/pdf/2401.17268.pdf', 'https://arxiv.org/pdf/2211.09260.pdf', 'https://arxiv.org/pdf/2410.15944.pdf', 'https://arxiv.org/pdf/2401.17268.pdf']\n",
 "\n",
 "Q48: What criteria were used to select labelers to ensure they ca...\n",
 "Sources: ['https://arxiv.org/pdf/2203.02155.pdf', 'https://arxiv.org/pdf/2203.02155.pdf', 'https://arxiv.org/pdf/2203.02155.pdf', 'https://arxiv.org/pdf/2203.02155.pdf', 'https://arxiv.org/pdf/2203.02155.pdf', 'https://arxiv.org/pdf/2203.02155.pdf', 'https://arxiv.org/pdf/2203.02155.pdf', 'https://arxiv.org/pdf/2203.02155.pdf']\n",
 "\n",
 "Q50: How do the evaluation metrics used in a model contribute to ...\n",
 "Sources: ['https://arxiv.org/pdf/2312.10997.pdf', 'https://arxiv.org/pdf/2312.10997.pdf', 'https://arxiv.org/pdf/2309.15217.pdf', 'https://arxiv.org/pdf/2312.10997.pdf', 'https://lilianweng.github.io/posts/2024-07-07-hallucination/', 'https://lilianweng.github.io/posts/2024-07-07-hallucination/', 'https://arxiv.org/pdf/2104.07567.pdf', 'https://arxiv.org/pdf/2312.10997.pdf']\n",
 "\n",
 "Q51: How do the datasets encourage the application of common-sens...\n",
 "Sources: ['https://arxiv.org/pdf/2106.09685.pdf', 'https://arxiv.org/pdf/2005.11401.pdf', 'https://arxiv.org/pdf/2203.15556.pdf', 'https://arxiv.org/pdf/2005.11401.pdf', 'https://arxiv.org/pdf/2402.19473.pdf', 'https://arxiv.org/pdf/2211.09260.pdf', 'https://arxiv.org/pdf/2104.07567.pdf', 'https://arxiv.org/pdf/2105.03011.pdf']\n",
 "\n",
 "Q52: What metrics are used to evaluate the quality of translation...\n",
 "Sources: ['https://arxiv.org/pdf/2401.17268.pdf', 'https://arxiv.org/pdf/2203.02155.pdf', 'https://arxiv.org/pdf/2211.09260.pdf', 'https://arxiv.org/pdf/2402.19473.pdf', 'https://arxiv.org/pdf/2312.10997.pdf', 'https://arxiv.org/pdf/2212.09741.pdf', 'https://arxiv.org/pdf/2402.19473.pdf', 'https://arxiv.org/pdf/2212.09741.pdf']\n",
 "\n",
 "Q53: What are the implications of adding updates on the pretraini...\n",
 "Sources: ['https://arxiv.org/pdf/2203.02155.pdf', 'https://arxiv.org/pdf/2106.09685.pdf', 'https://lilianweng.github.io/posts/2020-10-29-odqa/', 'https://arxiv.org/pdf/2106.09685.pdf', 'https://arxiv.org/pdf/2410.15944.pdf', 'https://arxiv.org/pdf/2312.10997.pdf', 'https://arxiv.org/pdf/2203.02155.pdf', 'https://arxiv.org/pdf/2305.14314.pdf']\n",
 "\n",
 "Q54: How do the capabilities of different AI models compare in te...\n",
 "Sources: ['https://arxiv.org/pdf/2104.07567.pdf', 'https://arxiv.org/pdf/2305.18290.pdf', 'https://arxiv.org/pdf/2104.07567.pdf', 'https://lilianweng.github.io/posts/2024-07-07-hallucination/', 'https://arxiv.org/pdf/2310.06825.pdf', 'https://lilianweng.github.io/posts/2024-07-07-hallucination/', 'https://arxiv.org/pdf/2104.07567.pdf', 'https://arxiv.org/pdf/2104.07567.pdf']\n",
 "\n",
 "Q55: How do human likert scores compare when evaluating PPO with ...\n",
 "Sources: ['https://arxiv.org/pdf/2203.02155.pdf', 'https://arxiv.org/pdf/2203.02155.pdf', 'https://arxiv.org/pdf/2203.02155.pdf', 'https://arxiv.org/pdf/2203.02155.pdf', 'https://arxiv.org/pdf/2203.02155.pdf', 'https://arxiv.org/pdf/2203.02155.pdf', 'https://arxiv.org/pdf/2203.02155.pdf', 'https://arxiv.org/pdf/2402.01306.pdf']\n",
 "\n",
 "Q59: What methodologies can be employed to test the reliability a...\n",
 "Sources: ['https://arxiv.org/pdf/2310.06825.pdf', 'https://arxiv.org/pdf/2203.02155.pdf', 'https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/', 'https://lilianweng.github.io/posts/2024-07-07-hallucination/', 'https://arxiv.org/pdf/2410.12812.pdf', 'https://en.wikipedia.org/wiki/Prompt_engineering', 'https://en.wikipedia.org/wiki/Prompt_engineering', 'https://arxiv.org/pdf/2309.15217.pdf']\n",
 "\n",
 "Q60: How does the subjective nature of human preferences influenc...\n",
 "Sources: ['https://arxiv.org/pdf/2305.14314.pdf', 'https://arxiv.org/pdf/2305.14314.pdf', 'https://arxiv.org/pdf/2305.14314.pdf', 'https://arxiv.org/pdf/2305.14314.pdf', 'https://arxiv.org/pdf/2305.14314.pdf', 'https://arxiv.org/pdf/2305.14314.pdf', 'https://arxiv.org/pdf/2305.14314.pdf', 'https://arxiv.org/pdf/2305.14314.pdf']\n",
 "\n",
 "Q61: What are some potential societal impacts of the widespread u...\n",
 "Sources: ['https://arxiv.org/pdf/2305.14314.pdf', 'https://arxiv.org/pdf/2203.02155.pdf', 'https://arxiv.org/pdf/2305.14314.pdf', 'https://arxiv.org/pdf/2203.02155.pdf', 'https://arxiv.org/pdf/2305.14314.pdf', 'https://arxiv.org/pdf/2305.14314.pdf', 'https://arxiv.org/pdf/2402.01306.pdf', 'https://arxiv.org/pdf/2305.14314.pdf']\n",
 "\n",
 "Q62: What methods or approaches are being investigated or utilize...\n",
 "Sources: ['https://arxiv.org/pdf/2203.02155.pdf', 'https://en.wikipedia.org/wiki/Knowledge_cutoff', 'https://en.wikipedia.org/wiki/Generative_pre-trained_transformer', 'https://arxiv.org/pdf/2401.17268.pdf', 'https://arxiv.org/pdf/2106.09685.pdf', 'https://arxiv.org/pdf/2402.01306.pdf', 'https://arxiv.org/pdf/2402.01306.pdf', 'https://arxiv.org/pdf/2402.01306.pdf']\n",
 "\n",
 "Q63: What are the advantages of applying LoRA to transformer mode...\n",
 "Sources: ['https://arxiv.org/pdf/2106.09685.pdf', 'https://arxiv.org/pdf/2106.09685.pdf', 'https://arxiv.org/pdf/2106.09685.pdf', 'https://arxiv.org/pdf/2305.14314.pdf', 'https://arxiv.org/pdf/2106.09685.pdf', 'https://arxiv.org/pdf/2106.09685.pdf', 'https://arxiv.org/pdf/2106.09685.pdf', 'https://arxiv.org/pdf/2203.15556.pdf']\n",
 "\n",
 "Q64: How does the training cost impact the evaluation of performa...\n",
 "Sources: ['https://arxiv.org/pdf/2106.09685.pdf', 'https://arxiv.org/pdf/2106.09685.pdf', 'https://arxiv.org/pdf/2106.09685.pdf', 'https://arxiv.org/pdf/2203.15556.pdf', 'https://arxiv.org/pdf/2106.09685.pdf', 'https://arxiv.org/pdf/2106.09685.pdf', 'https://arxiv.org/pdf/2203.15556.pdf', 'https://arxiv.org/pdf/2203.15556.pdf']\n",
 "\n",
 "Q65: What are the advantages of using low-rank adaptations during...\n",
 "Sources: ['https://arxiv.org/pdf/2106.09685.pdf', 'https://arxiv.org/pdf/2106.09685.pdf', 'https://arxiv.org/pdf/2106.09685.pdf', 'https://arxiv.org/pdf/2106.09685.pdf', 'https://arxiv.org/pdf/2106.09685.pdf', 'https://arxiv.org/pdf/2106.09685.pdf', 'https://arxiv.org/pdf/2106.09685.pdf', 'https://arxiv.org/pdf/2106.09685.pdf']\n",
 "\n",
 "Q67: What are the implications of fine-tuning large language mode...\n",
 "Sources: ['https://arxiv.org/pdf/2203.15556.pdf', 'https://lilianweng.github.io/posts/2020-10-29-odqa/', 'https://arxiv.org/pdf/2203.15556.pdf', 'https://lilianweng.github.io/posts/2020-10-29-odqa/', 'https://arxiv.org/pdf/2306.15595.pdf', 'https://arxiv.org/pdf/2203.15556.pdf', 'https://arxiv.org/pdf/2203.15556.pdf', 'https://arxiv.org/pdf/2203.15556.pdf']\n",
 "\n",
 "Q69: How does the RoFormer model perform on semantic text matchin...\n",
 "Sources: ['https://arxiv.org/pdf/2104.09864.pdf', 'https://arxiv.org/pdf/2104.09864.pdf', 'https://arxiv.org/pdf/2104.09864.pdf', 'https://arxiv.org/pdf/2104.09864.pdf', 'https://arxiv.org/pdf/2212.09741.pdf', 'https://arxiv.org/pdf/2104.09864.pdf', 'https://arxiv.org/pdf/2211.09260.pdf', 'https://arxiv.org/pdf/2104.09864.pdf']\n",
 "\n",
 "Q73: What is the purpose of using a regularization term (like the...\n",
 "Sources: ['https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/', 'https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/', 'https://arxiv.org/pdf/2402.01306.pdf', 'https://arxiv.org/pdf/2305.18290.pdf', 'https://lilianweng.github.io/posts/2020-10-29-odqa/', 'https://arxiv.org/pdf/2106.09685.pdf', 'https://arxiv.org/pdf/2106.09685.pdf', 'https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/']\n",
 "\n",
 "Q74: What novel attention mechanisms does the language model disc...\n",
 "Sources: ['https://lilianweng.github.io/posts/2018-06-24-attention/', 'https://arxiv.org/pdf/2306.15595.pdf', 'https://arxiv.org/pdf/2306.15595.pdf', 'https://lilianweng.github.io/posts/2018-06-24-attention/', 'https://arxiv.org/pdf/2005.11401.pdf', 'https://arxiv.org/pdf/2306.15595.pdf', 'https://arxiv.org/pdf/2306.15595.pdf', 'https://arxiv.org/pdf/2306.15595.pdf']\n",
 "\n",
 "Q75: How does the performance of the Mistral 7B model in mathemat...\n",
 "Sources: ['https://arxiv.org/pdf/2310.06825.pdf', 'https://arxiv.org/pdf/2310.06825.pdf', 'https://arxiv.org/pdf/2203.02155.pdf', 'https://arxiv.org/pdf/2310.06825.pdf', 'https://arxiv.org/pdf/2402.01306.pdf', 'https://arxiv.org/pdf/2306.15595.pdf', 'https://arxiv.org/pdf/2310.06825.pdf', 'https://arxiv.org/pdf/2203.15556.pdf']\n",
 "\n",
 "Q76: What are the key differences between HALO and non-HALO loss ...\n",
 "Sources: ['https://arxiv.org/pdf/2402.01306.pdf', 'https://arxiv.org/pdf/2402.01306.pdf', 'https://arxiv.org/pdf/2402.01306.pdf', 'https://arxiv.org/pdf/2203.02155.pdf', 'https://arxiv.org/pdf/2402.01306.pdf', 'https://arxiv.org/pdf/2402.01306.pdf', 'https://arxiv.org/pdf/2203.02155.pdf', 'https://arxiv.org/pdf/2402.01306.pdf']\n",
 "\n",
 "Q78: What implications does the lemma from Rafailov et al. (2023)...\n",
 "Sources: ['https://arxiv.org/pdf/2305.18290.pdf', 'https://arxiv.org/pdf/2305.18290.pdf', 'https://arxiv.org/pdf/2402.01306.pdf', 'https://arxiv.org/pdf/2402.01306.pdf', 'https://arxiv.org/pdf/2305.18290.pdf', 'https://arxiv.org/pdf/2305.18290.pdf', 'https://arxiv.org/pdf/2305.18290.pdf', 'https://arxiv.org/pdf/2402.01306.pdf']\n",
 "\n",
 "Q80: What is the significance of the term open-domain in the cont...\n",
 "Sources: ['https://lilianweng.github.io/posts/2020-10-29-odqa/', 'https://arxiv.org/pdf/2311.08377.pdf', 'https://arxiv.org/pdf/2211.09260.pdf', 'https://lilianweng.github.io/posts/2020-10-29-odqa/', 'https://arxiv.org/pdf/2005.11401.pdf', 'https://arxiv.org/pdf/2212.09741.pdf', 'https://arxiv.org/pdf/2401.17268.pdf', 'https://arxiv.org/pdf/2105.03011.pdf']\n",
 "\n",
 "Q81: How do various models handle the retrieval of relevant conte...\n",
 "Sources: ['https://lilianweng.github.io/posts/2020-10-29-odqa/', 'https://arxiv.org/pdf/2402.19473.pdf', 'https://arxiv.org/pdf/2309.15217.pdf', 'https://arxiv.org/pdf/2105.03011.pdf', 'https://lilianweng.github.io/posts/2020-10-29-odqa/', 'https://arxiv.org/pdf/2105.03011.pdf', 'https://arxiv.org/pdf/2311.08377.pdf', 'https://lilianweng.github.io/posts/2020-10-29-odqa/']\n",
 "\n",
 "Q82: What are the differences between open-book and closed-book q...\n",
 "Sources: ['https://lilianweng.github.io/posts/2020-10-29-odqa/', 'https://lilianweng.github.io/posts/2020-10-29-odqa/', 'https://arxiv.org/pdf/2311.08377.pdf', 'https://arxiv.org/pdf/2211.09260.pdf', 'https://arxiv.org/pdf/2309.08872.pdf', 'https://arxiv.org/pdf/2005.11401.pdf', 'https://arxiv.org/pdf/2309.08872.pdf', 'https://arxiv.org/pdf/2311.08377.pdf']\n",
 "\n",
 "Q84: What are some of the concerns related to fine-tuning QA mode...\n",
 "Sources: ['https://lilianweng.github.io/posts/2020-10-29-odqa/', 'https://arxiv.org/pdf/2401.06532.pdf', 'https://lilianweng.github.io/posts/2020-10-29-odqa/', 'https://arxiv.org/pdf/2401.06532.pdf', 'https://arxiv.org/pdf/2410.15944.pdf', 'https://arxiv.org/pdf/2401.06532.pdf', 'https://arxiv.org/pdf/2211.09260.pdf', 'https://arxiv.org/pdf/2401.06532.pdf']\n",
 "\n",
 "Q85: How does the performance of question answering models tend t...\n",
 "Sources: ['https://lilianweng.github.io/posts/2020-10-29-odqa/', 'https://lilianweng.github.io/posts/2024-07-07-hallucination/', 'https://arxiv.org/pdf/2105.03011.pdf', 'https://arxiv.org/pdf/2311.08377.pdf', 'https://arxiv.org/pdf/2212.09741.pdf', 'https://arxiv.org/pdf/2203.15556.pdf', 'https://arxiv.org/pdf/2105.03011.pdf', 'https://lilianweng.github.io/posts/2020-10-29-odqa/']\n",
 "\n",
 "Q86: What are the advantages and limitations of using generative ...\n",
 "Sources: ['https://lilianweng.github.io/posts/2020-10-29-odqa/', 'https://arxiv.org/pdf/2309.15217.pdf', 'https://arxiv.org/pdf/2309.08872.pdf', 'https://arxiv.org/pdf/2410.12812.pdf', 'https://arxiv.org/pdf/2104.07567.pdf', 'https://lilianweng.github.io/posts/2020-10-29-odqa/', 'https://arxiv.org/pdf/2406.04744.pdf', 'https://arxiv.org/pdf/2311.08377.pdf']\n",
 "\n",
 "Q87: What are the main components that complement the central con...\n",
 "Sources: ['https://lilianweng.github.io/posts/2023-06-23-agent/', 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'https://en.wikipedia.org/wiki/Generative_artificial_intelligence', 'https://lilianweng.github.io/posts/2023-06-23-agent/']\n",
 "\n",
 "Q88: How can subgoals and task decomposition improve the handling...\n",
 "Sources: ['https://lilianweng.github.io/posts/2023-06-23-agent/', 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'https://lilianweng.github.io/posts/2023-06-23-agent/']\n",
 "\n",
 "Q89: What types of memory are leveraged in autonomous agents, and...\n",
 "Sources: ['https://lilianweng.github.io/posts/2023-06-23-agent/', 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'https://arxiv.org/pdf/2310.11511.pdf', 'https://lilianweng.github.io/posts/2023-06-23-agent/']\n",
 "\n",
 "Q91: How does planning and reflection contribute to the iterative...\n",
 "Sources: ['https://lilianweng.github.io/posts/2023-06-23-agent/', 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'https://en.wikipedia.org/wiki/Generative_artificial_intelligence', 'https://lilianweng.github.io/posts/2023-06-23-agent/']\n",
 "\n",
 "Q92: In what ways can an agent's performance be evaluated and ref...\n",
 "Sources: ['https://lilianweng.github.io/posts/2023-06-23-agent/', 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'https://arxiv.org/pdf/2203.02155.pdf']\n",
 "\n",
 "Q93: What challenges are faced in enhancing the long-term plannin...\n",
 "Sources: ['https://lilianweng.github.io/posts/2023-06-23-agent/', 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'https://en.wikipedia.org/wiki/Generative_artificial_intelligence', 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'https://lilianweng.github.io/posts/2023-06-23-agent/']\n",
 "\n",
 "Q94: How does the use of tools and external resources extend the ...\n",
 "Sources: ['https://en.wikipedia.org/wiki/Llama_(language_model)', 'https://arxiv.org/pdf/2203.15556.pdf', 'https://arxiv.org/pdf/2203.15556.pdf', 'https://en.wikipedia.org/wiki/Large_language_model', 'https://arxiv.org/pdf/2310.06825.pdf', 'https://arxiv.org/pdf/2005.11401.pdf', 'https://arxiv.org/pdf/2203.15556.pdf', 'https://lilianweng.github.io/posts/2020-10-29-odqa/']\n",
 "\n",
 "Q95: What are the primary techniques involved in steering the beh...\n",
 "Sources: ['https://en.wikipedia.org/wiki/Llama_(language_model)', 'https://arxiv.org/pdf/2203.02155.pdf', 'https://arxiv.org/pdf/2203.02155.pdf', 'https://arxiv.org/pdf/2203.15556.pdf', 'https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/', 'https://arxiv.org/pdf/2005.11401.pdf', 'https://arxiv.org/pdf/2310.06825.pdf', 'https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/']\n",
 "\n",
 "Q96: How can the alignment and steerability of language models be...\n",
 "Sources: ['https://arxiv.org/pdf/2203.02155.pdf', 'https://arxiv.org/pdf/2203.02155.pdf', 'https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/', 'https://arxiv.org/pdf/2203.02155.pdf', 'https://arxiv.org/pdf/2203.15556.pdf', 'https://en.wikipedia.org/wiki/Large_language_model', 'https://arxiv.org/pdf/2203.02155.pdf', 'https://arxiv.org/pdf/2203.02155.pdf']\n",
 "\n",
 "Q101: What innovative approaches are being explored to enhance lan...\n",
 "Sources: ['https://en.wikipedia.org/wiki/Reasoning_model', 'https://en.wikipedia.org/wiki/Large_language_model', 'https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/', 'https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/', 'https://arxiv.org/pdf/2203.15556.pdf', 'https://arxiv.org/pdf/2203.15556.pdf', 'https://arxiv.org/pdf/2312.05708.pdf', 'https://arxiv.org/pdf/2310.06825.pdf']\n",
 "\n",
 "Q102: How can external tools and APIs be integrated with language ...\n",
 "Sources: ['https://lilianweng.github.io/posts/2023-06-23-agent/', 'https://arxiv.org/pdf/2203.02155.pdf', 'https://en.wikipedia.org/wiki/Llama_(language_model)', 'https://arxiv.org/pdf/2401.17268.pdf', 'https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/', 'https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/', 'https://arxiv.org/pdf/2312.10997.pdf', 'https://lilianweng.github.io/posts/2023-06-23-agent/']\n",
 "\n",
 "Q103: What kind of learning challenges does the attention mechanis...\n",
 "Sources: ['https://lilianweng.github.io/posts/2018-06-24-attention/', 'https://lilianweng.github.io/posts/2018-06-24-attention/', 'https://lilianweng.github.io/posts/2018-06-24-attention/', 'https://lilianweng.github.io/posts/2018-06-24-attention/', 'https://arxiv.org/pdf/2306.15595.pdf', 'https://lilianweng.github.io/posts/2018-06-24-attention/', 'https://lilianweng.github.io/posts/2018-06-24-attention/', 'https://lilianweng.github.io/posts/2018-06-24-attention/']\n",
 "\n",
 "Q104: How is the encoder-decoder architecture in seq2seq models af...\n",
 "Sources: ['https://lilianweng.github.io/posts/2018-06-24-attention/', 'https://arxiv.org/pdf/2104.07567.pdf', 'https://arxiv.org/pdf/2105.03011.pdf', 'https://lilianweng.github.io/posts/2018-06-24-attention/', 'https://lilianweng.github.io/posts/2018-06-24-attention/', 'https://arxiv.org/pdf/2104.07567.pdf', 'https://arxiv.org/pdf/2311.08377.pdf', 'https://arxiv.org/pdf/2104.07567.pdf']\n",
 "\n",
 "Q105: What are the differences between soft and hard attention in ...\n",
 "Sources: ['https://lilianweng.github.io/posts/2018-06-24-attention/', 'https://lilianweng.github.io/posts/2018-06-24-attention/', 'https://lilianweng.github.io/posts/2018-06-24-attention/', 'https://arxiv.org/pdf/2402.19473.pdf', 'https://lilianweng.github.io/posts/2018-06-24-attention/', 'https://arxiv.org/pdf/2402.19473.pdf', 'https://arxiv.org/pdf/2211.12561.pdf', 'https://lilianweng.github.io/posts/2018-06-24-attention/']\n",
 "\n",
 "Q106: Can you describe the multi-head self-attention mechanism in ...\n",
 "Sources: ['https://lilianweng.github.io/posts/2018-06-24-attention/', 'https://lilianweng.github.io/posts/2018-06-24-attention/', 'https://lilianweng.github.io/posts/2018-06-24-attention/', 'https://lilianweng.github.io/posts/2018-06-24-attention/', 'https://arxiv.org/pdf/2306.15595.pdf', 'https://lilianweng.github.io/posts/2018-06-24-attention/', 'https://lilianweng.github.io/posts/2018-06-24-attention/', 'https://lilianweng.github.io/posts/2018-06-24-attention/']\n",
 "\n",
 "Q107: In what ways does SNAIL address the issue of positioning in ...\n",
 "Sources: ['https://lilianweng.github.io/posts/2018-06-24-attention/', 'https://arxiv.org/pdf/2104.09864.pdf', 'https://arxiv.org/pdf/2104.09864.pdf', 'https://arxiv.org/pdf/2104.09864.pdf', 'https://lilianweng.github.io/posts/2018-06-24-attention/', 'https://arxiv.org/pdf/2104.09864.pdf', 'https://arxiv.org/pdf/2306.15595.pdf', 'https://arxiv.org/pdf/2104.09864.pdf']\n",
 "\n",
 "Q108: How does the Pointer Network differ from standard seq2seq mo...\n",
 "Sources: ['https://lilianweng.github.io/posts/2018-06-24-attention/', 'https://lilianweng.github.io/posts/2018-06-24-attention/', 'https://lilianweng.github.io/posts/2018-06-24-attention/', 'https://arxiv.org/pdf/2104.07567.pdf', 'https://lilianweng.github.io/posts/2018-06-24-attention/', 'https://arxiv.org/pdf/2104.07567.pdf', 'https://arxiv.org/pdf/2105.03011.pdf', 'https://arxiv.org/pdf/2106.09685.pdf']\n",
 "\n",
 "Q110: How does Neural Turing Machine (NTM) simulate the infinite m...\n",
 "Sources: ['https://lilianweng.github.io/posts/2018-06-24-attention/', 'https://lilianweng.github.io/posts/2018-06-24-attention/', 'https://lilianweng.github.io/posts/2018-06-24-attention/', 'https://lilianweng.github.io/posts/2018-06-24-attention/', 'https://lilianweng.github.io/posts/2018-06-24-attention/', 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'https://en.wikipedia.org/wiki/Large_language_model', 'https://lilianweng.github.io/posts/2023-06-23-agent/']\n"
 ]
 }
 ],
 "source": "import pandas as pd\nfrom tqdm import tqdm\nimport time\nimport pickle\nimport os\nfrom bert_score import score\nfrom datasets import Dataset\n\nconfigs = ['marketing_cohere', 'engineering_cohere']\nresults_file = '/content/drive/MyDrive/evaluation_results_checkpoint.pkl'\n\navailable_question_ids = sorted(validation_questions_answers.keys())\nprint(f\"Total available questions: {len(available_question_ids)}\")\nprint(f\"First 20 question IDs: {available_question_ids[:20]}\")\n\nquestion_ids_to_test = available_question_ids\nprint(f\"Testing with {len(question_ids_to_test)} questions: {question_ids_to_test}\")\n\nif os.path.exists(results_file):\n with open(results_file, 'rb') as f:\n rag_outputs = pickle.load(f)\n print(f\"Loaded {len(rag_outputs)} existing generated responses\")\nelse:\n rag_outputs = []\n print(\"Starting fresh generation\")\n\ncompleted_keys = {(r['question_id'], r['chain_type']) for r in rag_outputs}\ntotal_needed = len(question_ids_to_test) * len(configs)\nprint(f\"Progress: {len(completed_keys)}/{total_needed} generations complete\")\n\nprint(\"PHASE 1: Generating RAG responses...\")\nfor question_id in tqdm(question_ids_to_test, desc=\"Questions\"):\n\n question = validation_questions_answers[question_id]['question']\n retrieved_docs = retriever.invoke(question)\n retrieved_contexts = [doc.page_content for doc in retrieved_docs]\n\n doc_sources = []\n for doc in retrieved_docs:\n if hasattr(doc, 'metadata') and 'source' in doc.metadata:\n doc_sources.append(doc.metadata['source'])\n else:\n doc_sources.append('unknown')\n\n for chain_type in configs:\n if (question_id, chain_type) in completed_keys:\n continue\n\n try:\n start = time.time()\n\n rag_response = ask_rag_question(\n question_id,\n chain_type\n )\n\n entry = {\n 'question_id': question_id,\n 'chain_type': chain_type,\n 'question': rag_response['question'],\n 'rag_answer': rag_response['rag_answer'],\n 'gold_answer': rag_response['gold_answer'],\n 'retrieved_contexts': rag_response['retrieved_contexts'],\n 'retrieved_sources': doc_sources\n }\n rag_outputs.append(entry)\n\n with open(results_file, 'wb') as f:\n pickle.dump(rag_outputs, f)\n\n elapsed = time.time() - start\n print(f\" Generated Q{question_id}-{chain_type} ({elapsed:.1f}s)\")\n\n time.sleep(3.5)\n\n except Exception as e:\n print(f\" Error Q{question_id}-{chain_type}: {e}\")\n time.sleep(3.5)\n\nprint(f\"PHASE 1 Complete: {len(rag_outputs)} responses generated\")\n\nprint(f\"PHASE 2: Batch Evaluation on {len(rag_outputs)} items...\")\n\nif rag_outputs and 'bert_f1' not in rag_outputs[0]:\n print(\" Running BERTScore in batch mode...\")\n start = time.time()\n\n rag_answers = [x['rag_answer'] for x in rag_outputs]\n gold_answers = [x['gold_answer'] for x in rag_outputs]\n\n P, R, F1 = score(rag_answers, gold_answers, lang='en', rescale_with_baseline=True)\n\n for i, entry in enumerate(rag_outputs):\n entry['bert_precision'] = P[i].item()\n entry['bert_recall'] = R[i].item()\n entry['bert_f1'] = F1[i].item()\n\n elapsed = time.time() - start\n print(f\" BERTScore complete ({elapsed:.1f}s for {len(rag_outputs)} items)\")\n\n with open(results_file, 'wb') as f:\n pickle.dump(rag_outputs, f)\nelse:\n print(\" BERTScore already computed, skipping...\")\n\nif rag_outputs and 'ragas_faithfulness' not in rag_outputs[0]:\n print(\" Running RAGAS in batch mode...\")\n start = time.time()\n\n data_dict = {\n \"user_input\": [x['question'] for x in rag_outputs],\n \"retrieved_contexts\": [x['retrieved_contexts'] for x in rag_outputs],\n \"response\": [x['rag_answer'] for x in rag_outputs],\n \"reference\": [x['gold_answer'] for x in rag_outputs]\n }\n\n dataset = Dataset.from_dict(data_dict)\n\n from ragas import evaluate\n from ragas.metrics import LLMContextRecall, ContextPrecision, Faithfulness, FactualCorrectness, AnswerRelevancy\n from ragas.llms import LangchainLLMWrapper\n from langchain_openai import ChatOpenAI\n\n evaluator_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4o-mini\"))\n metrics = [LLMContextRecall(), ContextPrecision(), Faithfulness(), FactualCorrectness(), AnswerRelevancy()]\n\n ragas_results = evaluate(\n dataset=dataset,\n metrics=metrics,\n llm=evaluator_llm\n )\n\n for i, entry in enumerate(rag_outputs):\n entry['ragas_context_recall'] = ragas_results['context_recall'][i]\n entry['ragas_context_precision'] = ragas_results['context_precision'][i]\n entry['ragas_faithfulness'] = ragas_results['faithfulness'][i]\n entry['ragas_factual_correctness'] = ragas_results['factual_correctness(mode=f1)'][i]\n entry['ragas_answer_relevancy'] = ragas_results['answer_relevancy'][i]\n\n elapsed = time.time() - start\n print(f\" RAGAS complete ({elapsed:.1f}s for {len(rag_outputs)} items)\")\n\n with open(results_file, 'wb') as f:\n pickle.dump(rag_outputs, f)\nelse:\n print(\" RAGAS already computed, skipping...\")\n\nresults_df = pd.DataFrame(rag_outputs)\nprint(f\"\\n{'='*70}\")\nprint(f\"EVALUATION COMPLETE!\")\nprint(f\"{'='*70}\")\nprint(f\"Total items evaluated: {len(results_df)}\")\nprint(f\"Questions tested: {sorted(results_df['question_id'].unique())}\")\nprint(f\"Configs tested: {sorted(results_df['chain_type'].unique())}\")\nprint(f\"Results saved to: {results_file}\")\nprint(f\"\\nQuick preview:\")\nprint(results_df.groupby('chain_type')[['bert_f1', 'ragas_context_recall',\n 'ragas_context_precision',\n 'ragas_faithfulness',\n 'ragas_factual_correctness',\n 'ragas_answer_relevancy']].mean())\n\nprint(f\"\\n{'='*70}\")\nprint(\"RETRIEVED DOCUMENT SOURCES\")\nprint(f\"{'='*70}\")\nfor q_id in question_ids_to_test:\n q_data = results_df[results_df['question_id'] == q_id].iloc[0]\n print(f\"\\nQ{q_id}: {q_data['question'][:60]}...\")\n print(f\"Sources: {q_data['retrieved_sources']}\")"
 },
 {
 "cell_type": "code",
 "execution_count": 116,
 "metadata": {
 "id": "67z-_k0r4LWS",
 "executionInfo": {
 "status": "ok",
 "timestamp": 1765130664283,
 "user_tz": 420,
 "elapsed": 301,
 "user": {
 "displayName": "Ryan C",
 "userId": "13217471319505539255"
 }
 }
 },
 "outputs": [],
 "source": "results_df.to_csv(r'/content/drive/MyDrive/Gen AI/full_evaluation_results.csv', index=False)"
 },
 {
 "cell_type": "code",
 "execution_count": 117,
 "metadata": {
 "id": "u4WCsMu24bDi",
 "executionInfo": {
 "status": "ok",
 "timestamp": 1765130664316,
 "user_tz": 420,
 "elapsed": 21,
 "user": {
 "displayName": "Ryan C",
 "userId": "13217471319505539255"
 }
 }
 },
 "outputs": [],
 "source": "results_df['rag_answer_length'] = results_df['rag_answer'].str.len()\nresults_df['gold_answer_length'] = results_df['gold_answer'].str.len()"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "JdOhlN23AYiA"
 },
 "source": "## 5. Results\n\n### 5.1 Model Specifications\n\nDocument the detailed specs of your choices. Also comment on how you valued the needs of the marketing team vs the needs of the researchers, in case you had to make a trade-off.\n\n**Demonstration:**\n5.1.a Provide the detailed specification of your pipelines."
 },
 {
 "cell_type": "markdown",
 "source": "* LLM: Cohere\n* Embedding Model: all-distilroberta-v1 (384 dimensions)\n* Vector Database: Qdrant (in-memory)\n* Chunk Size: 1200 characters\n* Chunk Overlap200 characters\n* Retrievalk=8 chunks, cosine similarity\n\n**Trade-Offs**\n\nPrimary decision: Optimized for engineering for POC as it was more difficult to manage - due to the larger content requirement and preciseness required.\n\n* k=8 for both personas (engineering benefits +63% faithfulness on Q2, marketing drops -15% BERTScore but acceptable)\n* Cohere over Mistral (engineering needs faithfulness 0.89 vs 0.71, marketing handles verbosity with prompt constraints)\n* Single unified config for POC to reduce testing time (simplifies deployment, engineering is primary stakeholder with 300-person org)\n* Result: Engineering faithfulness 0.89, marketing faithfulness 0.65 on 78-question validation. Both exceed 0.83 answer relevancy.",
 "metadata": {
 "id": "Y8D0y_1D9Jc_"
 }
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "LDhusYguRGJ4"
 },
 "source": "**Demonstration:**\n5.1.b: What is the prompt you used for the engineering research group?"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "5UpGh8IsXe-h"
 },
 "source": "engineering_template = \"\"\"[INST]You are a technical AI assistant helping engineers understand AI and NLP concepts.\n\nUsing ONLY the context provided below, answer the question with technical precision. Your answer must:\n- Be 4-5 sentences (approximately 500 characters)\n- Use precise technical terminology\n- Focus on implementation details and architectures\n- Stay grounded in the provided context\n\nIf the context doesn't contain sufficient information to answer the question, state what information is missing.\n\nContext:\n{context}\n\nQuestion: {question}\n\nProvide a concise, technical answer (4-5 sentences):[/INST]\"\"\""
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "QWd1ossRTF77"
 },
 "source": "**Demonstration:**\n5.1.c.: What is the prompt you used for the marketing group??"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "spFXUwvaYAFh"
 },
 "source": "marketing_template = \"\"\"[INST]You are an AI assistant helping marketing professionals understand AI concepts for business communication.\n\nUsing ONLY the context provided below, answer the question in accessible language. Your answer must:\n- Be 2-3 sentences (approximately 250 characters)\n- Avoid technical jargon\n- Focus on business value and practical applications\n- Stay grounded in the provided context\n\nIf the context doesn't contain sufficient information to answer the question, state what information is missing.\n\nContext:\n{context}\n\nQuestion: {question}\n\nProvide a brief, business-focused answer (2-3 sentences):[/INST]\"\"\""
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "L7J41pWiyh06"
 },
 "source": "## 5.2 Some Test Questions\n\nPlease study the answers generated by your chosen setup for these specific test questions:\n\n1. \"What defines a large language model in the context of natural language processing tasks?\" (Question 0)\n\n2. \"What are the advantages of applying LoRA to transformer models in terms of computational efficiency during training and deployment?\" (Question 63)\n\n3. \"What actor played the role of Thanos in the Marvel Universe and the role of Cable in Deadpool 2?\" (Question 109, no labeled answers)\n\nFor each of the three questions above please provide:\n\na) The RAG results (research and marketing response) \nb) The context provided \nc) The document sources for the context \n\nThen, for questions 1 and 2,\n\nd) Also discuss your metric(s) for the two examples (for both responses) compared to the gold responses and comment on how well you feel your metrics captured the differences and similarities between your answer and the gold answer?\n\nPut your answers to these questions into the appropriae cells below.\n\n### 5.2.1 Test Question 1\n\nPlease run the test question 0 through your RAG pipeline show the results in the questions below:\n\n**Demonstration:**\n5.2.1.a.i. What is the engineering response for question 1?"
 },
 {
 "cell_type": "code",
 "source": "evaluation_results = pd.read_csv(r'/content/drive/MyDrive/Gen AI/full_evaluation_results.csv')",
 "metadata": {
 "id": "lAppxMZe-t1a",
 "executionInfo": {
 "status": "ok",
 "timestamp": 1765130664393,
 "user_tz": 420,
 "elapsed": 22,
 "user": {
 "displayName": "Ryan C",
 "userId": "13217471319505539255"
 }
 }
 },
 "execution_count": 122,
 "outputs": []
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "FvCmUOTbun7U"
 },
 "source": "5.2.1.a.i. A large language model (LLM) is defined by its vast number of parameters, typically ranging from billions to trillions, and its training on extensive text corpora using self-supervised learning. Architecturally, LLMs are often based on transformer models, leveraging attention mechanisms for sequence modeling. Their key characteristic is the ability to generalize across diverse natural language processing tasks with minimal task-specific fine-tuning, enabled by pre-training on large datasets. This scalability in model size and data has been shown to improve performance, though it introduces challenges like high computational costs and the need for high-quality training data. LLMs are particularly effective in generative tasks, such as language generation, summarization, and conversational agents, due to their capacity to capture complex linguistic patterns and semantics."
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "nFCZ7X5luor6"
 },
 "source": "**Demonstration:**\n5.2.1.a.ii. What is the marketing response for question 1?"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "vdxiRQFLxsow"
 },
 "source": "5.2.1.a.ii. A large language model (LLM) is a powerful tool for businesses, designed to handle natural language processing tasks like language generation. LLMs are defined by their vast number of parameters (billions to trillions) and training on massive text datasets, enabling them to generalize across tasks, from conversational agents to automated reasoning, with minimal task-specific supervision."
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "jMSkhEINuqXq"
 },
 "source": "**Demonstration:**\n5.2.1.b: What is the context you passed to the LLM for question 1?"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "rTlFS8TVzKvA"
 },
 "source": "5.2.1.b:\n\n--- Chunk 1 ---\nbillion parameters (Smith et al., 2022). The drive to train larger and larger models is clear\u2014so far\nincreasing the size of language models has been responsible for improving the state-of-the-art in many\nlanguage modelling tasks. Nonetheless, large language models face several challenges, including\ntheir overwhelming computational requirements (the cost of training and inference increase with\nmodel size) (Rae et al., 2021; Thoppilan et al., 2022) and the need for acquiring more high-quality\ntraining data. In fact, in this work we \ufb01nd that larger, high quality datasets will play a key role in any\nfurther scaling of language models.\nModelling the scaling behavior.\nUnderstanding the scaling behaviour of language models and\ntheir transfer properties has been important in the development of recent large models (Hernandez\net al., 2021; Kaplan et al., 2020). Kaplan et al. (2020) \ufb01rst showed a predictable relationship between\nmodel size and loss over many orders of magnitude. The authors investigate the question of choosing\nthe optimal model size to train for a given compute budget. Similar to us, they address this question\n\n--- Chunk 2 ---\n== Background ==\nAfter the release of large language models such as GPT-3, a focus of research was up-scaling models, which in some instances showed major increases in emergent capabilities. The release of ChatGPT and its surprise success caused an increase in attention to large language models.\nCompared with other responses to ChatGPT, Meta's Chief AI scientist Yann LeCun stated that large language models are best for aiding with writing.\n\n== Versions ==\n\n--- Chunk 3 ---\nA large language model (LLM) is a type of machine learning model designed for natural language processing tasks such as language generation. LLMs are language models with many parameters, and are trained with self-supervised learning on a vast amount of text.\n\n== List ==\nFor the training cost column, 1 petaFLOP-day = 1 petaFLOP/sec \u00d7 1 day = 8.64E19 FLOP. Also, only the largest model's cost is written.\n\n== Timeline ==\n\n== See also ==\nList of chatbots\nList of language model benchmarks\n\n== Notes ==\n\n== References ==\n\n--- Chunk 4 ---\n== History ==\n\nBefore the emergence of transformer-based models in 2017, some language models were considered large relative to the computational and data constraints of their time. In the early 1990s, IBM's statistical models pioneered word alignment techniques for machine translation, laying the groundwork for corpus-based language modeling. In 2001, a smoothed n-gram model, such as those employing Kneser\u2013Ney smoothing, trained on 300 million words, achieved state-of-the-art perplexity on benchmark tests. During the 2000s, with the rise of widespread internet access, researchers began compiling massive text datasets from the web (\"web as corpus\") to train statistical language models.\n\n--- Chunk 5 ---\n# Tasks\nExamples\nLanguage Modelling\n20\nWikiText-103, The Pile: PG-19, arXiv, FreeLaw, . . .\nReading Comprehension\n3\nRACE-m, RACE-h, LAMBADA\nQuestion Answering\n3\nNatural Questions, TriviaQA, TruthfulQA\nCommon Sense\n5\nHellaSwag, Winogrande, PIQA, SIQA, BoolQ\nMMLU\n57\nHigh School Chemistry, Astronomy, Clinical Knowledge, . . .\nBIG-bench\n62\nCausal Judgement, Epistemic Reasoning, Temporal Sequences, . . .\nTable 5 | All evaluation tasks. We evaluate Chinchilla on a collection of language modelling along\nwith downstream tasks. We evaluate on largely the same tasks as in Rae et al. (2021), to allow for\ndirect comparison.\n4.2. Results\nWe perform an extensive evaluation of Chinchilla, comparing against various large language models.\nWe evaluate on a large subset of the tasks presented in Rae et al. (2021), shown in Table 5. As\nthe focus of this work is on optimal model scaling, we included a large representative subset, and\nintroduce a few new evaluations to allow for better comparison to other existing large models. The\nevaluation details for all tasks are the same as described in Rae et al. (2021).\n4.2.1. Language modelling\npubmed_abstracts\nnih_exporter\nuspto_backgrounds\npubmed_central\n\n--- Chunk 6 ---\nA large language model (LLM) is a language model trained with self-supervised machine learning on a vast amount of text, designed for natural language processing tasks, especially language generation. The largest and most capable LLMs are generative pre-trained transformers (GPTs) and provide the core capabilities of modern chatbots. LLMs can be fine-tuned for specific tasks or guided by prompt engineering. These models acquire predictive power regarding syntax, semantics, and ontologies inherent in human language corpora, but they also inherit inaccuracies and biases present in the data they are trained on.\nThey consist of billions to trillions of parameters and operate as general-purpose sequence models, generating, summarizing, translating, and reasoning over text. LLMs represent a significant new technology in their ability to generalize across tasks with minimal task-specific supervision, enabling capabilities like conversational agents, code generation, knowledge retrieval, and automated reasoning that previously required bespoke systems.\n\n--- Chunk 7 ---\nth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open\nand efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.\n[26] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei,\nNikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open\nfoundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.\n[27] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\n\u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information\nprocessing systems, 30, 2017.\n[28] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can a\nmachine really finish your sentence? arXiv preprint arXiv:1905.07830, 2019.\n[29] Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin Saied,\nWeizhu Chen, and Nan Duan. Agieval: A human-centric benchmark for evaluating foundation\nmodels. arXiv preprint arXiv:2304.06364, 2023.\n9\n\n--- Chunk 8 ---\nNote that they did fine-tune the pretrained LM independently for each dataset.\nClosed-book QA: Generative Language Model#\nBig language models have been pre-trained on a large collection of unsupervised textual corpus. Given enough parameters, these models are able to memorize some factual knowledge within parameter weights. Therefore, we can use these models to do question-answering without explicit context, just like in a closed-book exam. The pre-trained language models produce free text to respond to questions, no explicit reading comprehension.\n\nThe amount of computation used for training big language models of different sizes is getting big. (Image source: Brown et al., 2020).\n\nRoberts et al. (2020) measured the practical utility of a language model by fine-tuning a pre-trained model to answer questions without access to any external context or knowledge. They fine-tuned the T5 language model (same architecture as the original Transformer) to answer questions without inputting any additional information or context. Such setup enforces the language model to answer questions based on \u201cknowledge\u201d that it internalized during pre-training."
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "seBLWPbNuqOL"
 },
 "source": "**Demonstration:**\n5.2.1.c: List the doc_source for each of the documents in the context for question 1\n\n1. https://arxiv.org/pdf/2203.15556.pdf\n2. https://en.wikipedia.org/wiki/Llama_(language_model)\n3. https://en.wikipedia.org/wiki/List_of_large_language_models\n4. https://en.wikipedia.org/wiki/Large_language_model\n5. https://arxiv.org/pdf/2203.15556.pdf\n6. https://en.wikipedia.org/wiki/Large_language_model\n7. https://arxiv.org/pdf/2310.06825.pdf\n8. https://lilianweng.github.io/posts/2020-10-29-odqa/"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "GnVLvZQkuqA4"
 },
 "source": "**Demonstration:**\n5.2.1.d.i: How well does your model perform relative to the gold answer we provided for engineering on question 1?"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "QBejy1Zj0Pxv"
 },
 "source": "5.2.1.d.i:\n\n[('BERTScore Precision', 0.268),\n ('BERTScore Recall', 0.341),\n ('BERTScore F1', 0.306),\n ('Context Recall', 1.000),\n ('Context Precision', 0.347),\n ('Faithfulness', 1.000),\n ('Factual Correctness', 0.360),\n ('Answer Relevancy', 0.925)]"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "nGGYQF1_upi2"
 },
 "source": "**Demonstration:**\n5.2.1.d.ii: How well does your model perform relative to the gold answer we provided for marketing on question 1?"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "bE9b3YeW09CH"
 },
 "source": "5.2.1.d.ii.\n\n[('BERTScore Precision', 0.322),\n ('BERTScore Recall', 0.392),\n ('BERTScore F1', 0.358),\n ('Context Recall', 1.000),\n ('Context Precision', 0.347),\n ('Faithfulness', 0.857),\n ('Factual Correctness', 0.290),\n ('Answer Relevancy', 0.880)]"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "PZdkNySoUDy3"
 },
 "source": "### 5.2.2 Test Question 2\n\nPlease run question 63 through your RAG pipeline:"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "zJks-ptB1w8s"
 },
 "source": "**Demonstration:**\n5.2.2.a.i: What is the engineering response for question 2?"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "CTQGVBAk250c"
 },
 "source": "5.2.2.a.i:\nApplying LoRA to Transformer models significantly enhances computational efficiency during training and deployment. By injecting low-rank matrices (L1 \u2208 R^(h\u00d7r) and L2 \u2208 R^(r\u00d7o)) into the linear projection layers, LoRA reduces the number of trainable parameters, lowering VRAM usage by up to 2/3 and enabling training with fewer GPUs. During training, LoRA avoids calculating gradients for frozen parameters, resulting in a 25% speedup for GPT-3 175B. At deployment, the trained low-rank matrices can be merged with frozen weights (W = W0 + BA), eliminating inference latency compared to full fine-tuning. Additionally, task switching is efficient, requiring only the replacement of LoRA weights (B\u2032A\u2032) with minimal memory overhead."
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "K9tlUP_i1x6Z"
 },
 "source": "**Demonstration:**\n5.2.2.a.ii: What is the marketing response for question 2?"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "b8XEEfM_4PBw"
 },
 "source": "5.2.2.a.ii:\nApplying LoRA to transformer models significantly reduces memory and storage usage, allowing businesses to train models with fewer GPUs and avoid costly I/O bottlenecks. This efficiency enables faster task switching and the creation of customized models at a lower cost, while also speeding up training by up to 25% and introducing no additional inference latency during deployment."
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "BQykHsye1yc0"
 },
 "source": "**Demonstration:**\n5.2.2.b: What is the context you passed to the LLM for question 2?"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "bVAZuY-a3o6_"
 },
 "source": "5.2.2.b:\n\n--- Chunk 1 ---\nPractical Bene\ufb01ts and Limitations.\nThe most signi\ufb01cant bene\ufb01t comes from the reduction in\nmemory and storage usage. For a large Transformer trained with Adam, we reduce that VRAM\nusage by up to 2/3 if r \u226admodel as we do not need to store the optimizer states for the frozen\nparameters. On GPT-3 175B, we reduce the VRAM consumption during training from 1.2TB to\n350GB. With r = 4 and only the query and value projection matrices being adapted, the checkpoint\nsize is reduced by roughly 10,000\u00d7 (from 350GB to 35MB)4. This allows us to train with signi\ufb01-\ncantly fewer GPUs and avoid I/O bottlenecks. Another bene\ufb01t is that we can switch between tasks\nwhile deployed at a much lower cost by only swapping the LoRA weights as opposed to all the\nparameters. This allows for the creation of many customized models that can be swapped in and out\non the \ufb02y on machines that store the pre-trained weights in VRAM. We also observe a 25% speedup\nduring training on GPT-3 175B compared to full \ufb01ne-tuning5 as we do not need to calculate the\ngradient for the vast majority of the parameters.\nLoRA also has its limitations. For example, it is not straightforward to batch inputs to different tasks\n\n--- Chunk 2 ---\nwhen the full rank (i.e., d) is as high as 12,288, making LoRA both storage- and compute-ef\ufb01cient.\nLoRA possesses several key advantages.\n\u2022 A pre-trained model can be shared and used to build many small LoRA modules for dif-\nferent tasks. We can freeze the shared model and ef\ufb01ciently switch tasks by replacing the\nmatrices A and B in Figure 1, reducing the storage requirement and task-switching over-\nhead signi\ufb01cantly.\n\u2022 LoRA makes training more ef\ufb01cient and lowers the hardware barrier to entry by up to 3\ntimes when using adaptive optimizers since we do not need to calculate the gradients or\nmaintain the optimizer states for most parameters. Instead, we only optimize the injected,\nmuch smaller low-rank matrices.\n\u2022 Our simple linear design allows us to merge the trainable matrices with the frozen weights\nwhen deployed, introducing no inference latency compared to a fully \ufb01ne-tuned model, by\nconstruction.\n\u2022 LoRA is orthogonal to many prior methods and can be combined with many of them, such\nas pre\ufb01x-tuning. We provide an example in Appendix E.\nTerminologies and Conventions\nWe make frequent references to the Transformer architecture\n\n--- Chunk 3 ---\nness of full \ufb01ne-tuning by setting the LoRA rank r to the rank of the pre-trained weight matrices. In\nother words, as we increase the number of trainable parameters 3, training LoRA roughly converges\nto training the original model, while adapter-based methods converges to an MLP and pre\ufb01x-based\nmethods to a model that cannot take long input sequences.\nNo Additional Inference Latency.\nWhen deployed in production, we can explicitly compute and\nstore W = W0 + BA and perform inference as usual. Note that both W0 and BA are in Rd\u00d7k.\nWhen we need to switch to another downstream task, we can recover W0 by subtracting BA and\nthen adding a different B\u2032A\u2032, a quick operation with very little memory overhead. Critically, this\n2They represent a negligible number of parameters compared to weights.\n3An inevitability when adapting to hard tasks.\n4\n\n--- Chunk 4 ---\nthe full model parameters which remain fixed. Gradients during stochastic gradient descent are\npassed through the fixed pretrained model weights to the adapter, which is updated to optimize the\nloss function. LoRA augments a linear projection through an additional factorized projection. Given\na projection XW = Y with X \u2208Rb\u00d7h, W \u2208Rh\u00d7o LoRA computes:\nY = XW + sXL1L2,\n(3)\nwhere L1 \u2208Rh\u00d7r and L2 \u2208Rr\u00d7o, and s is a scalar.\nMemory Requirement of Parameter-Efficient Finetuning\nOne important point of discussion is\nthe memory requirement of LoRA during training both in terms of the number and size of adapters\nused. Since the memory footprint of LoRA is so minimal, we can use more adapters to improve\nperformance without significantly increasing the total memory used. While LoRA was designed as a\n3\n\n--- Chunk 5 ---\nHyperparameters\nFine-Tune\nPreEmbed\nPreLayer\nBitFit\nAdapterH\nLoRA\nOptimizer\nAdamW\nBatch Size\n128\n# Epoch\n2\nWarmup Tokens\n250,000\nLR Schedule\nLinear\nLearning Rate\n5.00E-06\n5.00E-04\n1.00E-04\n1.6E-03\n1.00E-04\n2.00E-04\nTable 12: The training hyperparameters used for different GPT-3 adaption methods. We use the\nsame hyperparameters for all datasets after tuning learning rate.\nrally, we replace them after every Transformer block with an input agnostic vector. Thus, both the\nembeddings and subsequent Transformer block activations are treated as trainable parameters. For\nmore on pre\ufb01x-layer tuning, see Section 5.1.\nIn Table 15, we show the evaluation results of LoRA+PE and LoRA+PL on WikiSQL and MultiNLI.\nFirst of all, LoRA+PE signi\ufb01cantly outperforms both LoRA and pre\ufb01x-embedding tuning on\nWikiSQL, which indicates that LoRA is somewhat orthogonal to pre\ufb01x-embedding tuning. On\nMultiNLI, the combination of LoRA+PE doesn\u2019t perform better than LoRA, possibly because LoRA\non its own already achieves performance comparable to the human baseline. Secondly, we notice\nthat LoRA+PL performs slightly worse than LoRA even with more trainable parameters. We at-\n\n--- Chunk 6 ---\nof 128 to match the adapter baselines. Second, we initialize the model to the pre-trained model for\nMRPC, RTE, and STS-B, not a model already adapted to MNLI like the \ufb01ne-tuning baseline. Runs\nfollowing this more restricted setup from Houlsby et al. (2019) are labeled with \u2020. The result is\npresented in Table 2 (Top Three Sections). See Section D.1 for details on the hyperparameters used.\n5.3\nDEBERTA XXL\nDeBERTa (He et al., 2021) is a more recent variant of BERT that is trained on a much larger\nscale and performs very competitively on benchmarks such as GLUE (Wang et al., 2019) and Su-\nperGLUE (Wang et al., 2020). We evaluate if LoRA can still match the performance of a fully\n\ufb01ne-tuned DeBERTa XXL (1.5B) on GLUE. The result is presented in Table 2 (Bottom Section).\nSee Section D.2 for details on the hyperparameters used.\n5.4\nGPT-2 MEDIUM/LARGE\nHaving shown that LoRA can be a competitive alternative to full \ufb01ne-tuning on NLU, we hope to\nanswer if LoRA still prevails on NLG models, such as GPT-2 medium and large (Radford et al.,\nb). We keep our setup as close as possible to Li & Liang (2021) for a direct comparison. Due\n\n--- Chunk 7 ---\nguarantees that we do not introduce any additional latency during inference compared to a \ufb01ne-tuned\nmodel by construction.\n4.2\nAPPLYING LORA TO TRANSFORMER\nIn principle, we can apply LoRA to any subset of weight matrices in a neural network to reduce the\nnumber of trainable parameters. In the Transformer architecture, there are four weight matrices in\nthe self-attention module (Wq, Wk, Wv, Wo) and two in the MLP module. We treat Wq (or Wk, Wv)\nas a single matrix of dimension dmodel \u00d7dmodel, even though the output dimension is usually sliced\ninto attention heads. We limit our study to only adapting the attention weights for downstream\ntasks and freeze the MLP modules (so they are not trained in downstream tasks) both for simplicity\nand parameter-ef\ufb01ciency.We further study the effect on adapting different types of attention weight\nmatrices in a Transformer in Section 7.1. We leave the empirical investigation of adapting the MLP\nlayers, LayerNorm layers, and biases to a future work.\nPractical Bene\ufb01ts and Limitations.\nThe most signi\ufb01cant bene\ufb01t comes from the reduction in\nmemory and storage usage. For a large Transformer trained with Adam, we reduce that VRAM\n\n--- Chunk 8 ---\nTable 1 | Current LLMs. We show \ufb01ve of the current largest dense transformer models, their size,\nand the number of training tokens. Other than LaMDA (Thoppilan et al., 2022), most models are\ntrained for approximately 300 billion tokens. We introduce Chinchilla, a substantially smaller model,\ntrained for much longer than 300B tokens.\nModel\nSize (# Parameters)\nTraining Tokens\nLaMDA (Thoppilan et al., 2022)\n137 Billion\n168 Billion\nGPT-3 (Brown et al., 2020)\n175 Billion\n300 Billion\nJurassic (Lieber et al., 2021)\n178 Billion\n300 Billion\nGopher (Rae et al., 2021)\n280 Billion\n300 Billion\nMT-NLG 530B (Smith et al., 2022)\n530 Billion\n270 Billion\nChinchilla\n70 Billion\n1.4 Trillion\n2. Related Work\nLarge language models.\nA variety of large language models have been introduced in the last few\nyears. These include both dense transformer models (Brown et al., 2020; Lieber et al., 2021; Rae\net al., 2021; Smith et al., 2022; Thoppilan et al., 2022) and mixture-of-expert (MoE) models (Du\net al., 2021; Fedus et al., 2021; Zoph et al., 2022). The largest dense transformers have passed 500\nbillion parameters (Smith et al., 2022). The drive to train larger and larger models is clear\u2014so far"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "kko2RRpd1zCx"
 },
 "source": "**Demonstration:**\n5.2.2.c: List the doc_source for each of the documents in the context for question 2"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "-dVkKU-b4ti8"
 },
 "source": "5.2.2.c:\n\n1. https://arxiv.org/pdf/2106.09685.pdf\n2. https://arxiv.org/pdf/2106.09685.pdf\n3. https://arxiv.org/pdf/2106.09685.pdf\n4. https://arxiv.org/pdf/2305.14314.pdf\n5. https://arxiv.org/pdf/2106.09685.pdf\n6. https://arxiv.org/pdf/2106.09685.pdf\n7. https://arxiv.org/pdf/2106.09685.pdf\n8. https://arxiv.org/pdf/2203.15556.pdf"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "mHuwlNYx1zpZ"
 },
 "source": "**Demonstration:**\n5.2.2.d.i: How well does your model perform relative to the gold answer we provided for engineering on question 2?"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "ZPVmPe9H5O_H"
 },
 "source": "5.2.2.d.i:\n\n[('BERTScore Precision', 0.128),\n ('BERTScore Recall', 0.424),\n ('BERTScore F1', 0.273),\n ('Context Recall', 0.750),\n ('Context Precision', 0.976),\n ('Faithfulness', 1.000),\n ('Factual Correctness', 0.130),\n ('Answer Relevancy', 0.957)]"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "T4EtUEgW10b5"
 },
 "source": "**Demonstration:**\n5.2.2.d.ii: How well does your model perform relative to the gold answer we provided for marketing on question 2?"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "VPPw56fq5tWY"
 },
 "source": "5.2.2.d.ii:\n\n[('BERTScore Precision', 0.323),\n ('BERTScore Recall', 0.489),\n ('BERTScore F1', 0.405),\n ('Context Recall', 1.000),\n ('Context Precision', 0.976),\n ('Faithfulness', 1.000),\n ('Factual Correctness', 0.200),\n ('Answer Relevancy', 0.955)]"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "VxDhzMXsT48H"
 },
 "source": "#### 5.2.3 Test Question 3\n\nPlease run question 109 through your RAG pipeline:"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "s92FBcy2600q"
 },
 "source": "**Demonstration:**\n5.2.3.a.i: What is the engineering response for question 3?"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "5GDt2z2s61cM"
 },
 "source": "5.2.3.a.i:\n\nThe context provided does not contain any information about the Marvel Universe, Thanos, Cable, Deadpool 2, or the actors associated with these roles. Therefore, it is impossible to answer the question using the given context. Information about the specific actors and their roles in these films or franchises is entirely missing."
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "WeUc-7_o62Me"
 },
 "source": "**Demonstration:**\n5.2.3.a.ii: What is the marketing response for question 3?"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "74LMUBxC62xp"
 },
 "source": "5.2.3.a.ii:\n\nThe context provided does not contain information about the actor who plays Thanos in the Marvel Universe or Cable in Deadpool 2. Therefore, I cannot answer the question with the given information."
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "YqcVZ9aG65B8"
 },
 "source": "**Demonstration:**\n5.2.3.b: What is the context you passed to the LLM for question 3?"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "onhduo1364te"
 },
 "source": "5.2.3_b:\n\n--- Chunk 1 ---\nQuestion\nContext\nAnswer\nWho directed the film Op-\npenheimer and who stars\nas J. Robert Oppenheimer\nin the film?\nOppenheimer is a 2023 biographical thriller film written\nand directed by Christopher Nolan. Based on the 2005\nbiography American Prometheus by Kai Bird and Mar-\ntin J. Sherwin, the film chronicles the life of J. Robert\nOppenheimer, a theoretical physicist who was pivotal in\ndeveloping the first nuclear weapons as part of the Man-\nhattan Project, and thereby ushering in the Atomic Age.\nCillian Murphy stars as Oppenheimer, with Emily Blunt\nas Oppenheimer\u2019s wife Katherine \"Kitty\" Oppenheimer.\nHigh Faithfulness: Christopher\nNolan directed the film Oppen-\nheimer. Cillian Murphy stars as J.\nRobert Oppenheimer in the film.\nLow\nFaithfulness:\nJames\nCameron directed the film Op-\npenheimer. Tom Cruise stars as J.\nRobert Oppenheimer in the film.\nTable 2: Example from WikiEval, showing answers with high and low faithfulness.\nQuestion\nAnswer\nWhen is the scheduled\nlaunch date and time for\nthe PSLV-C56 mission,\nand where will it be\nlaunched from?\nHigh answer relevance: The PSLV-C56 mission is scheduled to be launched on Sunday, 30\n\n--- Chunk 2 ---\nwhere $l$ is the hidden dimension of the bidirectional LSTM module. $\\mathbf{W}^g \\in \\mathbb{R}^{l\\times l}$, $\\mathbf{b}^g \\in \\mathbb{R}^l$, and $\\mathbf{W}^m \\in \\mathbb{R}^{2l \\times 4l}$ are parameters to learn. The operator $\\otimes \\mathbf{e}_{d_x}$ is the outer product to repeat the column vector $\\mathbf{b}^g$ $d_x$ times.\nThe ranker and reader components share the same Match-LSTM module with two separate prediction heads in the last layer, resulting in $\\mathbf{H}^\\text{rank}$ and $\\mathbf{H}^\\text{reader}$.\n\nThe overview of R^3 (reinforced ranker-reader) architecture. Both components share the same Match-LSTM module. (Image source: Wang, et al., 2017)\n\nThe retriever runs a max-pooling operation per passage and then aggregates to output a probability of each passage entailing the answer.\n\n$$\n\\begin{aligned}\n\\mathbf{u}_i &= \\text{max-pooling}(\\mathbf{H}^\\text{rank}_i) \\in \\mathbb{R}^l \\\\\n\\mathbf{C} &= \\text{tanh}(\\mathbf{W}^c[\\mathbf{u}_1;\\dots;\\mathbf{u}_N] + \\mathbf{b}^c \\otimes \\mathbf{e}_N) \\in \\mathbb{R}^{l \\times n} \\\\\n\\gamma &= \\text{softmax}(\\mathbf{w}^c \\mathbf{C}) \\in \\mathbb{R}^n\n\\end{aligned}\n$$\n\n--- Chunk 3 ---\nOfir Press, Noah Smith, and Mike Lewis. Train short, test long: Attention with linear biases enables\ninput length extrapolation. In International Conference on Learning Representations, 2022. URL\nhttps://openreview.net/forum?id=R8sQPpGCv0.\nJack W. Rae, Anna Potapenko, Siddhant M. Jayakumar, Chloe Hillier, and Timothy P. Lilli-\ncrap. Compressive transformers for long-range sequence modelling. In International Confer-\nence on Learning Representations, 2020. URL https://openreview.net/forum?id=\nSylKikSYDH.\n13\n\n--- Chunk 4 ---\nThe model is trained from scratch for five days on 256 A100\nGPUs. Our implementation is in PyTorch (Paszke et al.,\n2019) using Metaseq (Zhang et al., 2022). We use model\nparallelism over 4 GPUs and a batch size of 16 sequences\nper GPU. The optimization uses a linear learning rate decay\nwith 1500 warmup steps, a peak learning rate of 1e-4, a\ngradient clipping of 1.0, and the Adam optimizer with \u03b21 =\n0.9, \u03b22 = 0.98 (Kingma & Ba, 2015).\nApproach\nCIDEr (\u2191)\nRetrieval Baseline\n84.1\nDALL-ESmall (Wang, 2021)\n20.2\nruDALL-E-XL (Forever, 2021)\n38.7\nminDALL-E (Kim et al., 2021)\n48.0\nX-LXMERT (Cho et al., 2020)\n55.8\nParti (Yu et al., 2022)\n83.9\nFlamingo (3B; 4-shot) (Alayrac et al., 2022)\n85\nFlamingo (80B; 4-shot) (Alayrac et al., 2022)\n103\nVanilla CM3\n71.9\nRA-CM3 (2.7B) (Ours)\n89.1\nTable 3. Image-to-caption generation performance on MS-\nCOCO (with no finetuning). Our retrieval-augmented CM3 signif-\nicantly outperforms the baseline CM3 with no retrieval. Moreover,\nour model outperforms other strong models such as Parti (20B\nparameters) and Flamingo (3B; 4-shot), despite using just \u223c3B\nparameters and 2-shot in-context examples.\nBaseline.\nFor our baseline, we train a vanilla CM3 with no\n\n--- Chunk 5 ---\nthe dataset license in the paper or a repository. The\nrest is over view as follows:\n\u2022 Apache License 2.0 license: GOV2, TREC-\n9https://quoradata.quora.com/\nFirst-Quora-Dataset-Release-Question-Pairs\n10https://huggingface.co/BeIR\n11https://creativecommons.org/licenses/by-sa/4.\n0/\nRobust, CODEC, CNN/DM\n\u2022 MIT license: TREC-CAsT, GECOR, ORCAS-\nI, MIMICS, MIMICS-Duo, XSum\n\u2022 CC BY 4.0: Query2Doc, MSRP, SQuAD\n\u2022 CC BY-SA 4.0: CANARD, BEIR, HotpotQA,\nQuAC\n\u2022 CC BY-SA 3.0: QReCC, FEVER, BoolQ\n\u2022 CC BY-NC 2.0: SciFact\n\u2022 Provided under the \u201cDataset License Agree-\nment\u201d: TREC-COVID, Multi-News, MS MARCO\nNote that CoQA contains several datasets under\ndifferent licenses. They are listed on the Hugging-\nFace page.12\nB\nIn-domain Evaluation Details\nIn this evaluation, we split the full dataset into\ntraining, validation, and test sets. The split pro-\ncess is designed based on the size and structure of\nthe original datasets. Specifically, if the original\ndatasets do not contain a test set, then: For original\ndatasets with over 10,400 samples, we randomly se-\nlect 10,000 samples for constructing training data,\n200 samples for validation, and 200 samples for\n\n--- Chunk 6 ---\ncompared to CNNs1.\n1A stack of multiple CNN layers can also capture longer intra-token relation, here we only consider single layer setting.\narXiv:2104.09864v5 [cs.CL] 8 Nov 2023\n\n--- Chunk 7 ---\nIlya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Confer-\nence on Learning Representations, 2019. URL https://openreview.net/forum?id=\nBkg6RiCqY7.\nPedro Henrique Martins, Zita Marinho, and Andr\u00b4e F. T. Martins. \u221e-former: Infinite memory trans-\nformer. 2021.\nAmirkeivan Mohtashami and Martin Jaggi. Landmark attention: Random-access infinite context\nlength for transformers. arXiv preprint arXiv:2305.16300, 2023.\nJesse Mu, Xiang Lisa Li, and Noah Goodman. Learning to compress prompts with gist tokens.\n2023.\nAdam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor\nKilleen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas K\u00a8opf, Ed-\nward Yang, Zach DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner,\nLu Fang, Junjie Bai, and Soumith Chintala. PyTorch: An Imperative Style, High-Performance\nDeep Learning Library. Curran Associates Inc., Red Hook, NY, USA, 2019.\nOfir Press, Noah Smith, and Mike Lewis. Train short, test long: Attention with linear biases enables\ninput length extrapolation. In International Conference on Learning Representations, 2022. URL\n\n--- Chunk 8 ---\n$$\n\\arg\\min_{\\mathbf{e}'_i \\in \\mathcal{V}} [\\mathbf{e}'_i - \\mathbf{e}_i]^\\top \\nabla_{\\mathbf{e}_i} \\mathcal{L}_\\text{adv}\n$$\n\nwhere $\\mathcal{V}$ is the embedding matrix of all the tokens. $\\nabla_{\\mathbf{e}_i} \\mathcal{L}_\\text{adv}$ is the average gradient of the task loss over a batch around the current embedding of the $i$-th token in the adversarial triggering sequence $\\mathbf{t}$. We can brute-force the optimal $\\mathbf{e}\u2019_i$ by a big dot product of size embedding of the entire vocabulary $\\vert \\mathcal{V} \\vert$ $\\times$ the embedding dimension $d$. Matrix multiplication of this size is cheap and can be run in parallel.\nAutoPrompt (Shin et al., 2020) utilizes the same gradient-based search strategy to find the most effective prompt template for a diverse set of tasks.\nThe above token search method can be augmented with beam search. When looking for the optimal token embedding $\\mathbf{e}\u2019_i$, we can pick top-$k$ candidates instead of a single one, searching from left to right and score each beam by $\\mathcal{L}_\\text{adv}$ on the current data batch.\n\nIllustration of how Universal Adversarial Triggers (UAT) works. (Image source: Wallace et al. 2019)"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "nG1egPAn64DT"
 },
 "source": "**Demonstration:**\n5.2.3.c: List the doc_source for each of the documents in the context for question 3"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "u1EFnMkc63k0"
 },
 "source": "5.2.3.c:\n\n1. https://arxiv.org/pdf/2309.15217.pdf\n2. https://lilianweng.github.io/posts/2020-10-29-odqa/\n3. https://arxiv.org/pdf/2306.15595.pdf\n4. https://arxiv.org/pdf/2211.12561.pdf\n5. https://arxiv.org/pdf/2401.06532.pdf\n6. https://arxiv.org/pdf/2104.09864.pdf\n7. https://arxiv.org/pdf/2306.15595.pdf\n8. https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "24UZvRmhUOWk"
 },
 "source": "**Demonstration:**\n5.2.4.a: For questions 1 and 2, how well do you feel your metrics captured the differences and similarities between your answer and the gold answer?"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "zSnRHMpK8tZw"
 },
 "source": "5.2.4.a: Enter five sentence answer here.\n\nThe metrics captured aspects of answer quality but with some limitations. BERTScore effectively measured semantic similarity, with higher scores for marketing (0.358 F1) reflecting closer alignment to concise gold answers versus engineering (0.306 F1) whose longer responses introduced additional content not in gold answers. Faithfulness scores (1.0 for engineering, 0.857 for marketing) accurately showed how well responses stayed grounded in the context, as we can see that engineering's strict adherence to the material was correctly rewarded. However, factual correctness (0.29-0.36) may underestimate actual accuracy since both responses contain correct information that's simply phrased differently than gold answers. Context precision (0.347) appropriately flagged that only ~3 of 8 retrieved chunks were highly relevant. Overall, the multi-metric approach provided a more complete picture than any single metric would\u2014BERTScore alone would miss grounding issues, while faithfulness alone would miss semantic quality.\n\nHowever, if time permitted I would supplement metrics with further manually review."
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "44h038A7AUdn"
 },
 "source": "### 5.3 Other Questions\n\nBelow are a few questions that you should think about. Please answer them in the text cells directly (in a short paragraph) and also see whether they may be relevant for your final write-up.\n\n**Demonstration:**\n5.3.a. How would you expect your response quality to change if you had a chunk size of 50?"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "5dus2cuzDRJt"
 },
 "source": "==== ENTER YOUR 5.3.a.\n\nA chunk size of 50 characters would severely degrade response quality. Each chunk would contain only ~8-10 words\u2014far too small to capture meaningful content. Initially, I tested my data on the default chunk size provided (128) and retrieved useless information that was often cut off mid-sentence. Additionally, we'd need many more chunks (higher k) to gather sufficient context."
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "JahcG67cDT7l"
 },
 "source": "**Demonstration:**\n5.3.b. How would you expect your response quality to change if you had a chunk size of 5000?"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "hwIMS4QONVcu"
 },
 "source": "==== ENTER YOUR 5.3.b. ANSWER IN THIS TEXT BLOCK.\n\nA chunk size of 5000 characters would reduce precision while improving recall. Each chunk would contain ~800-1000 words, meaning relevant information gets bundled with irrelevant content. The LLM would receive more context but struggle to identify the specific relevant portions, potentially leading to verbose or unfocused responses. Context precision would drop since chunks would be \"partially relevant\" rather than highly targeted. However, for complex questions requiring broader context, larger chunks might help by keeping related concepts together. Overall I anticipate performance would drop dramatically."
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "CBcdtlB7Db_L"
 },
 "source": "**Demonstration:**\n5.3.c. If you had time, how do you think fine-tuning of the LLM could help? What type of data would you want for that? And which training approach would you take?"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "WpThz8J0NlEB"
 },
 "source": "==== ENTER YOUR 5.3.c. ANSWER IN THIS TEXT BLOCK.\n\nFine-tuning could help in two ways: adapting output style to match persona requirements (concise marketing vs detailed engineering), and mproving domain-specific terminology understanding for LLM/NLP content. I would use QLoRA (4-bit quantization with Low-Rank Adaptation) for parameter-efficient fine-tuning, requiring less compute while preserving base model capabilities. Training data would include ~500-1000 question-answer pairs matching our gold answer format\u2014concise 2-3 sentence marketing responses and detailed 4-5 sentence engineering responses grounded in retrieved context. This would reduce reliance on prompt engineering for length/style control."
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "4LdoXvV3DpWc"
 },
 "source": "**Demonstration:**\n5.3.d. What was your design philosophy of the prompts? How did they differ between engineering and marketing support?"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "-tjj65SWNpJw"
 },
 "source": "==== ENTER YOUR 5.3.d. ANSWER IN THIS TEXT BLOCK.\n\nThe core design philosophy was grounding + conciseness. Both prompts include \"Using ONLY the context provided\" to prevent hallucination and ensure verifiable answers. The key difference is output: marketing prompts request 2-3 sentences (250 chars) in accessible, jargon-free language focused on business value; engineering prompts request 4-5 sentences (500 chars) with technical precision and implementation details. (as confirmed by the different average lengths in the gold standard answers) Both prompts instruct models to acknowledge missing information rather than hallucinate when context is insufficient."
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "p-Cvt51zDueZ"
 },
 "source": "**Demonstration:**\n5.3.e. What are your average and peak load estimates for the system? Given that, would you suggest a pay-per-use deployment or one that reserves the LLM?"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "LKgiRt8eOEo4"
 },
 "source": "==== ENTER YOUR 5.3.e.\n\nI would guess average load of 30-100 queries/day during normal operations (depending on popularity of the tool), but peak loads of 200+ queries during product launches or documentation sprints. Given this bursty, unpredictable usage pattern, pay-per-use deployment is recommended. Reserved LLM infrastructure would sit idle most of the time, wasting budget. Pay-per-use scales automatically for peaks and costs nothing during quiet periods."
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "aJXu3lyJDzfo"
 },
 "source": "**Demonstration:**\n5.3.f. What type of limitations/risks would you see in using this system?"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "-0C-usvjOXO3"
 },
 "source": "==== ENTER YOUR 5.3.f.\n\nKey limitations:\n1. Retrieval failures\u2014technical queries may not retrieve relevant chunks despite content existing in the knowledge base (as seen with Q2 architectures at 25% precision).\n2. No real-time updates\u2014the system cannot answer questions about information added after the last index update.\n3. Hallucination risk\u2014despite grounding instructions, models occasionally generate plausible-sounding but unverifiable claims (marketing faithfulness dropped to 0.65 on full validation).\n4. Verbosity variance\u2014response length still varies despite prompt constraints.\n5. Latency and cost\u2014retrieval + LLM inference adds 2-4 seconds per query, and API costs scale with usage.\n6. Not production-ready\u2014current performance - the tool requires huge optimization before deployment. Further work needed on embedding model selection, chunk size tuning, and potentially fine-tuning to meet production quality standards."
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "RUYmImBZD-CC"
 },
 "source": "### Link to your 4-5 page pdf report\n\nEnter a link to your report that you've stored in your Google Drive. Put it in a folder and give edit permissions to the following people:\n\nmhbutler@.edu\n\npenpen1986@.edu\n\npeterg@.edu\n\ncornelia.ilin@.edu\n\nrahmfeld@.edu\n\nmakahmad@.edu\n\ntimtung@.edu"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "XAbDcYIs9g-2"
 },
 "source": "https://drive.google.com/file/d/18fn-hHzR8OqT74prrN2Kg0QJO058d35L/view?usp=sharing"
 }
 ],
 "metadata": {
 "accelerator": "GPU",
 "colab": {
 "collapsed_sections": [
 "xlSHHPW-f3ZL",
 "dM7gS9kGNOJp",
 "D2Y_wjQGW6l3"
 ],
 "gpuType": "T4",
 "provenance": []
 },
 "kernelspec": {
 "display_name": "Python 3 (ipykernel)",
 "language": "python",
 "name": "python3"
 },
 "language_info": {
 "codemirror_mode": {
 "name": "ipython",
 "version": 3
 },
 "file_extension": ".py",
 "mimetype": "text/x-python",
 "name": "python",
 "nbconvert_exporter": "python",
 "pygments_lexer": "ipython3",
 "version": "3.11.5"
 },
 "widgets": {
 "application/vnd.jupyter.widget-state+json": {
 "ad4f6a07ecd248b79cacfc7ef83a3d1c": {
 "model_module": "@jupyter-widgets/controls",
 "model_name": "HBoxModel",
 "model_module_version": "1.5.0",
 "state": {
 "_dom_classes": [],
 "_model_module": "@jupyter-widgets/controls",
 "_model_module_version": "1.5.0",
 "_model_name": "HBoxModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/controls",
 "_view_module_version": "1.5.0",
 "_view_name": "HBoxView",
 "box_style": "",
 "children": [
 "IPY_MODEL_e32200249e7c4d3e85ef2653456aa560",
 "IPY_MODEL_4bd2d1d86e8844d391c91ea02d96cde7",
 "IPY_MODEL_cfa9d46a31734cffbc488a15203fc2ae"
 ],
 "layout": "IPY_MODEL_c282f0f2c67145d5bfac5bc0c4b21c84"
 }
 },
 "e32200249e7c4d3e85ef2653456aa560": {
 "model_module": "@jupyter-widgets/controls",
 "model_name": "HTMLModel",
 "model_module_version": "1.5.0",
 "state": {
 "_dom_classes": [],
 "_model_module": "@jupyter-widgets/controls",
 "_model_module_version": "1.5.0",
 "_model_name": "HTMLModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/controls",
 "_view_module_version": "1.5.0",
 "_view_name": "HTMLView",
 "description": "",
 "description_tooltip": null,
 "layout": "IPY_MODEL_6985c0ecd8c643af89be0a849c849db3",
 "placeholder": "\u200b",
 "style": "IPY_MODEL_861900cf7c724daa810e64ab286750b6",
 "value": "tokenizer_config.json:\u2007"
 }
 },
 "4bd2d1d86e8844d391c91ea02d96cde7": {
 "model_module": "@jupyter-widgets/controls",
 "model_name": "FloatProgressModel",
 "model_module_version": "1.5.0",
 "state": {
 "_dom_classes": [],
 "_model_module": "@jupyter-widgets/controls",
 "_model_module_version": "1.5.0",
 "_model_name": "FloatProgressModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/controls",
 "_view_module_version": "1.5.0",
 "_view_name": "ProgressView",
 "bar_style": "success",
 "description": "",
 "description_tooltip": null,
 "layout": "IPY_MODEL_25dbb4477693457e90d52cbe694b3f26",
 "max": 1,
 "min": 0,
 "orientation": "horizontal",
 "style": "IPY_MODEL_9e5109aa85f848a9a8fde9fbd829a1ea",
 "value": 1
 }
 },
 "cfa9d46a31734cffbc488a15203fc2ae": {
 "model_module": "@jupyter-widgets/controls",
 "model_name": "HTMLModel",
 "model_module_version": "1.5.0",
 "state": {
 "_dom_classes": [],
 "_model_module": "@jupyter-widgets/controls",
 "_model_module_version": "1.5.0",
 "_model_name": "HTMLModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/controls",
 "_view_module_version": "1.5.0",
 "_view_name": "HTMLView",
 "description": "",
 "description_tooltip": null,
 "layout": "IPY_MODEL_69b212b297ae4dc1b7198f0bd687f7e7",
 "placeholder": "\u200b",
 "style": "IPY_MODEL_5ca6bf272a9b4fe7ab539cee14265ff6",
 "value": "\u2007141k/?\u2007[00:00&lt;00:00,\u20078.85MB/s]"
 }
 },
 "c282f0f2c67145d5bfac5bc0c4b21c84": {
 "model_module": "@jupyter-widgets/base",
 "model_name": "LayoutModel",
 "model_module_version": "1.2.0",
 "state": {
 "_model_module": "@jupyter-widgets/base",
 "_model_module_version": "1.2.0",
 "_model_name": "LayoutModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/base",
 "_view_module_version": "1.2.0",
 "_view_name": "LayoutView",
 "align_content": null,
 "align_items": null,
 "align_self": null,
 "border": null,
 "bottom": null,
 "display": null,
 "flex": null,
 "flex_flow": null,
 "grid_area": null,
 "grid_auto_columns": null,
 "grid_auto_flow": null,
 "grid_auto_rows": null,
 "grid_column": null,
 "grid_gap": null,
 "grid_row": null,
 "grid_template_areas": null,
 "grid_template_columns": null,
 "grid_template_rows": null,
 "height": null,
 "justify_content": null,
 "justify_items": null,
 "left": null,
 "margin": null,
 "max_height": null,
 "max_width": null,
 "min_height": null,
 "min_width": null,
 "object_fit": null,
 "object_position": null,
 "order": null,
 "overflow": null,
 "overflow_x": null,
 "overflow_y": null,
 "padding": null,
 "right": null,
 "top": null,
 "visibility": null,
 "width": null
 }
 },
 "6985c0ecd8c643af89be0a849c849db3": {
 "model_module": "@jupyter-widgets/base",
 "model_name": "LayoutModel",
 "model_module_version": "1.2.0",
 "state": {
 "_model_module": "@jupyter-widgets/base",
 "_model_module_version": "1.2.0",
 "_model_name": "LayoutModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/base",
 "_view_module_version": "1.2.0",
 "_view_name": "LayoutView",
 "align_content": null,
 "align_items": null,
 "align_self": null,
 "border": null,
 "bottom": null,
 "display": null,
 "flex": null,
 "flex_flow": null,
 "grid_area": null,
 "grid_auto_columns": null,
 "grid_auto_flow": null,
 "grid_auto_rows": null,
 "grid_column": null,
 "grid_gap": null,
 "grid_row": null,
 "grid_template_areas": null,
 "grid_template_columns": null,
 "grid_template_rows": null,
 "height": null,
 "justify_content": null,
 "justify_items": null,
 "left": null,
 "margin": null,
 "max_height": null,
 "max_width": null,
 "min_height": null,
 "min_width": null,
 "object_fit": null,
 "object_position": null,
 "order": null,
 "overflow": null,
 "overflow_x": null,
 "overflow_y": null,
 "padding": null,
 "right": null,
 "top": null,
 "visibility": null,
 "width": null
 }
 },
 "861900cf7c724daa810e64ab286750b6": {
 "model_module": "@jupyter-widgets/controls",
 "model_name": "DescriptionStyleModel",
 "model_module_version": "1.5.0",
 "state": {
 "_model_module": "@jupyter-widgets/controls",
 "_model_module_version": "1.5.0",
 "_model_name": "DescriptionStyleModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/base",
 "_view_module_version": "1.2.0",
 "_view_name": "StyleView",
 "description_width": ""
 }
 },
 "25dbb4477693457e90d52cbe694b3f26": {
 "model_module": "@jupyter-widgets/base",
 "model_name": "LayoutModel",
 "model_module_version": "1.2.0",
 "state": {
 "_model_module": "@jupyter-widgets/base",
 "_model_module_version": "1.2.0",
 "_model_name": "LayoutModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/base",
 "_view_module_version": "1.2.0",
 "_view_name": "LayoutView",
 "align_content": null,
 "align_items": null,
 "align_self": null,
 "border": null,
 "bottom": null,
 "display": null,
 "flex": null,
 "flex_flow": null,
 "grid_area": null,
 "grid_auto_columns": null,
 "grid_auto_flow": null,
 "grid_auto_rows": null,
 "grid_column": null,
 "grid_gap": null,
 "grid_row": null,
 "grid_template_areas": null,
 "grid_template_columns": null,
 "grid_template_rows": null,
 "height": null,
 "justify_content": null,
 "justify_items": null,
 "left": null,
 "margin": null,
 "max_height": null,
 "max_width": null,
 "min_height": null,
 "min_width": null,
 "object_fit": null,
 "object_position": null,
 "order": null,
 "overflow": null,
 "overflow_x": null,
 "overflow_y": null,
 "padding": null,
 "right": null,
 "top": null,
 "visibility": null,
 "width": "20px"
 }
 },
 "9e5109aa85f848a9a8fde9fbd829a1ea": {
 "model_module": "@jupyter-widgets/controls",
 "model_name": "ProgressStyleModel",
 "model_module_version": "1.5.0",
 "state": {
 "_model_module": "@jupyter-widgets/controls",
 "_model_module_version": "1.5.0",
 "_model_name": "ProgressStyleModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/base",
 "_view_module_version": "1.2.0",
 "_view_name": "StyleView",
 "bar_color": null,
 "description_width": ""
 }
 },
 "69b212b297ae4dc1b7198f0bd687f7e7": {
 "model_module": "@jupyter-widgets/base",
 "model_name": "LayoutModel",
 "model_module_version": "1.2.0",
 "state": {
 "_model_module": "@jupyter-widgets/base",
 "_model_module_version": "1.2.0",
 "_model_name": "LayoutModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/base",
 "_view_module_version": "1.2.0",
 "_view_name": "LayoutView",
 "align_content": null,
 "align_items": null,
 "align_self": null,
 "border": null,
 "bottom": null,
 "display": null,
 "flex": null,
 "flex_flow": null,
 "grid_area": null,
 "grid_auto_columns": null,
 "grid_auto_flow": null,
 "grid_auto_rows": null,
 "grid_column": null,
 "grid_gap": null,
 "grid_row": null,
 "grid_template_areas": null,
 "grid_template_columns": null,
 "grid_template_rows": null,
 "height": null,
 "justify_content": null,
 "justify_items": null,
 "left": null,
 "margin": null,
 "max_height": null,
 "max_width": null,
 "min_height": null,
 "min_width": null,
 "object_fit": null,
 "object_position": null,
 "order": null,
 "overflow": null,
 "overflow_x": null,
 "overflow_y": null,
 "padding": null,
 "right": null,
 "top": null,
 "visibility": null,
 "width": null
 }
 },
 "5ca6bf272a9b4fe7ab539cee14265ff6": {
 "model_module": "@jupyter-widgets/controls",
 "model_name": "DescriptionStyleModel",
 "model_module_version": "1.5.0",
 "state": {
 "_model_module": "@jupyter-widgets/controls",
 "_model_module_version": "1.5.0",
 "_model_name": "DescriptionStyleModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/base",
 "_view_module_version": "1.2.0",
 "_view_name": "StyleView",
 "description_width": ""
 }
 },
 "aee68dd9066e48e5b5642c2bb1aee228": {
 "model_module": "@jupyter-widgets/controls",
 "model_name": "HBoxModel",
 "model_module_version": "1.5.0",
 "state": {
 "_dom_classes": [],
 "_model_module": "@jupyter-widgets/controls",
 "_model_module_version": "1.5.0",
 "_model_name": "HBoxModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/controls",
 "_view_module_version": "1.5.0",
 "_view_name": "HBoxView",
 "box_style": "",
 "children": [
 "IPY_MODEL_28c27d29978e4478b1c9a6b97275a936",
 "IPY_MODEL_3370563daf9847bba12608d30c3bd826",
 "IPY_MODEL_066e509d01894a61bbd9828290876811"
 ],
 "layout": "IPY_MODEL_9a24014fbf8f467d8fec856855d70088"
 }
 },
 "28c27d29978e4478b1c9a6b97275a936": {
 "model_module": "@jupyter-widgets/controls",
 "model_name": "HTMLModel",
 "model_module_version": "1.5.0",
 "state": {
 "_dom_classes": [],
 "_model_module": "@jupyter-widgets/controls",
 "_model_module_version": "1.5.0",
 "_model_name": "HTMLModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/controls",
 "_view_module_version": "1.5.0",
 "_view_name": "HTMLView",
 "description": "",
 "description_tooltip": null,
 "layout": "IPY_MODEL_d9068e76323d490389d8161496f19326",
 "placeholder": "\u200b",
 "style": "IPY_MODEL_24e5884ef9eb4207843d55c5f53ab45d",
 "value": "tokenizer.model:\u2007100%"
 }
 },
 "3370563daf9847bba12608d30c3bd826": {
 "model_module": "@jupyter-widgets/controls",
 "model_name": "FloatProgressModel",
 "model_module_version": "1.5.0",
 "state": {
 "_dom_classes": [],
 "_model_module": "@jupyter-widgets/controls",
 "_model_module_version": "1.5.0",
 "_model_name": "FloatProgressModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/controls",
 "_view_module_version": "1.5.0",
 "_view_name": "ProgressView",
 "bar_style": "success",
 "description": "",
 "description_tooltip": null,
 "layout": "IPY_MODEL_34a8f8da7d01480f92cf5d2758161d60",
 "max": 587404,
 "min": 0,
 "orientation": "horizontal",
 "style": "IPY_MODEL_98db90ab654847bd81c929df5c9cc620",
 "value": 587404
 }
 },
 "066e509d01894a61bbd9828290876811": {
 "model_module": "@jupyter-widgets/controls",
 "model_name": "HTMLModel",
 "model_module_version": "1.5.0",
 "state": {
 "_dom_classes": [],
 "_model_module": "@jupyter-widgets/controls",
 "_model_module_version": "1.5.0",
 "_model_name": "HTMLModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/controls",
 "_view_module_version": "1.5.0",
 "_view_name": "HTMLView",
 "description": "",
 "description_tooltip": null,
 "layout": "IPY_MODEL_9573427506e0455fa1534eca766320a8",
 "placeholder": "\u200b",
 "style": "IPY_MODEL_1dc69664a4844e71baa9c7e0e3c7195f",
 "value": "\u2007587k/587k\u2007[00:00&lt;00:00,\u20072.17MB/s]"
 }
 },
 "9a24014fbf8f467d8fec856855d70088": {
 "model_module": "@jupyter-widgets/base",
 "model_name": "LayoutModel",
 "model_module_version": "1.2.0",
 "state": {
 "_model_module": "@jupyter-widgets/base",
 "_model_module_version": "1.2.0",
 "_model_name": "LayoutModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/base",
 "_view_module_version": "1.2.0",
 "_view_name": "LayoutView",
 "align_content": null,
 "align_items": null,
 "align_self": null,
 "border": null,
 "bottom": null,
 "display": null,
 "flex": null,
 "flex_flow": null,
 "grid_area": null,
 "grid_auto_columns": null,
 "grid_auto_flow": null,
 "grid_auto_rows": null,
 "grid_column": null,
 "grid_gap": null,
 "grid_row": null,
 "grid_template_areas": null,
 "grid_template_columns": null,
 "grid_template_rows": null,
 "height": null,
 "justify_content": null,
 "justify_items": null,
 "left": null,
 "margin": null,
 "max_height": null,
 "max_width": null,
 "min_height": null,
 "min_width": null,
 "object_fit": null,
 "object_position": null,
 "order": null,
 "overflow": null,
 "overflow_x": null,
 "overflow_y": null,
 "padding": null,
 "right": null,
 "top": null,
 "visibility": null,
 "width": null
 }
 },
 "d9068e76323d490389d8161496f19326": {
 "model_module": "@jupyter-widgets/base",
 "model_name": "LayoutModel",
 "model_module_version": "1.2.0",
 "state": {
 "_model_module": "@jupyter-widgets/base",
 "_model_module_version": "1.2.0",
 "_model_name": "LayoutModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/base",
 "_view_module_version": "1.2.0",
 "_view_name": "LayoutView",
 "align_content": null,
 "align_items": null,
 "align_self": null,
 "border": null,
 "bottom": null,
 "display": null,
 "flex": null,
 "flex_flow": null,
 "grid_area": null,
 "grid_auto_columns": null,
 "grid_auto_flow": null,
 "grid_auto_rows": null,
 "grid_column": null,
 "grid_gap": null,
 "grid_row": null,
 "grid_template_areas": null,
 "grid_template_columns": null,
 "grid_template_rows": null,
 "height": null,
 "justify_content": null,
 "justify_items": null,
 "left": null,
 "margin": null,
 "max_height": null,
 "max_width": null,
 "min_height": null,
 "min_width": null,
 "object_fit": null,
 "object_position": null,
 "order": null,
 "overflow": null,
 "overflow_x": null,
 "overflow_y": null,
 "padding": null,
 "right": null,
 "top": null,
 "visibility": null,
 "width": null
 }
 },
 "24e5884ef9eb4207843d55c5f53ab45d": {
 "model_module": "@jupyter-widgets/controls",
 "model_name": "DescriptionStyleModel",
 "model_module_version": "1.5.0",
 "state": {
 "_model_module": "@jupyter-widgets/controls",
 "_model_module_version": "1.5.0",
 "_model_name": "DescriptionStyleModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/base",
 "_view_module_version": "1.2.0",
 "_view_name": "StyleView",
 "description_width": ""
 }
 },
 "34a8f8da7d01480f92cf5d2758161d60": {
 "model_module": "@jupyter-widgets/base",
 "model_name": "LayoutModel",
 "model_module_version": "1.2.0",
 "state": {
 "_model_module": "@jupyter-widgets/base",
 "_model_module_version": "1.2.0",
 "_model_name": "LayoutModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/base",
 "_view_module_version": "1.2.0",
 "_view_name": "LayoutView",
 "align_content": null,
 "align_items": null,
 "align_self": null,
 "border": null,
 "bottom": null,
 "display": null,
 "flex": null,
 "flex_flow": null,
 "grid_area": null,
 "grid_auto_columns": null,
 "grid_auto_flow": null,
 "grid_auto_rows": null,
 "grid_column": null,
 "grid_gap": null,
 "grid_row": null,
 "grid_template_areas": null,
 "grid_template_columns": null,
 "grid_template_rows": null,
 "height": null,
 "justify_content": null,
 "justify_items": null,
 "left": null,
 "margin": null,
 "max_height": null,
 "max_width": null,
 "min_height": null,
 "min_width": null,
 "object_fit": null,
 "object_position": null,
 "order": null,
 "overflow": null,
 "overflow_x": null,
 "overflow_y": null,
 "padding": null,
 "right": null,
 "top": null,
 "visibility": null,
 "width": null
 }
 },
 "98db90ab654847bd81c929df5c9cc620": {
 "model_module": "@jupyter-widgets/controls",
 "model_name": "ProgressStyleModel",
 "model_module_version": "1.5.0",
 "state": {
 "_model_module": "@jupyter-widgets/controls",
 "_model_module_version": "1.5.0",
 "_model_name": "ProgressStyleModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/base",
 "_view_module_version": "1.2.0",
 "_view_name": "StyleView",
 "bar_color": null,
 "description_width": ""
 }
 },
 "9573427506e0455fa1534eca766320a8": {
 "model_module": "@jupyter-widgets/base",
 "model_name": "LayoutModel",
 "model_module_version": "1.2.0",
 "state": {
 "_model_module": "@jupyter-widgets/base",
 "_model_module_version": "1.2.0",
 "_model_name": "LayoutModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/base",
 "_view_module_version": "1.2.0",
 "_view_name": "LayoutView",
 "align_content": null,
 "align_items": null,
 "align_self": null,
 "border": null,
 "bottom": null,
 "display": null,
 "flex": null,
 "flex_flow": null,
 "grid_area": null,
 "grid_auto_columns": null,
 "grid_auto_flow": null,
 "grid_auto_rows": null,
 "grid_column": null,
 "grid_gap": null,
 "grid_row": null,
 "grid_template_areas": null,
 "grid_template_columns": null,
 "grid_template_rows": null,
 "height": null,
 "justify_content": null,
 "justify_items": null,
 "left": null,
 "margin": null,
 "max_height": null,
 "max_width": null,
 "min_height": null,
 "min_width": null,
 "object_fit": null,
 "object_position": null,
 "order": null,
 "overflow": null,
 "overflow_x": null,
 "overflow_y": null,
 "padding": null,
 "right": null,
 "top": null,
 "visibility": null,
 "width": null
 }
 },
 "1dc69664a4844e71baa9c7e0e3c7195f": {
 "model_module": "@jupyter-widgets/controls",
 "model_name": "DescriptionStyleModel",
 "model_module_version": "1.5.0",
 "state": {
 "_model_module": "@jupyter-widgets/controls",
 "_model_module_version": "1.5.0",
 "_model_name": "DescriptionStyleModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/base",
 "_view_module_version": "1.2.0",
 "_view_name": "StyleView",
 "description_width": ""
 }
 },
 "ee3a715fd0a04f8196a634d8952fee13": {
 "model_module": "@jupyter-widgets/controls",
 "model_name": "HBoxModel",
 "model_module_version": "1.5.0",
 "state": {
 "_dom_classes": [],
 "_model_module": "@jupyter-widgets/controls",
 "_model_module_version": "1.5.0",
 "_model_name": "HBoxModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/controls",
 "_view_module_version": "1.5.0",
 "_view_name": "HBoxView",
 "box_style": "",
 "children": [
 "IPY_MODEL_211050f71cc24786a13cbbee3042074f",
 "IPY_MODEL_58c9ebc16374407d8b4a5e92f3590faf",
 "IPY_MODEL_0fd37791e2c24960b3ec87db6d1b5654"
 ],
 "layout": "IPY_MODEL_7b59aed4e88f42dab5fdef583b01fd7d"
 }
 },
 "211050f71cc24786a13cbbee3042074f": {
 "model_module": "@jupyter-widgets/controls",
 "model_name": "HTMLModel",
 "model_module_version": "1.5.0",
 "state": {
 "_dom_classes": [],
 "_model_module": "@jupyter-widgets/controls",
 "_model_module_version": "1.5.0",
 "_model_name": "HTMLModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/controls",
 "_view_module_version": "1.5.0",
 "_view_name": "HTMLView",
 "description": "",
 "description_tooltip": null,
 "layout": "IPY_MODEL_b196199206a34657ae972ce1a0f6f8d2",
 "placeholder": "\u200b",
 "style": "IPY_MODEL_3b04b6185c4e41f1b11b80892c6dc3a0",
 "value": "tokenizer.json:\u2007"
 }
 },
 "58c9ebc16374407d8b4a5e92f3590faf": {
 "model_module": "@jupyter-widgets/controls",
 "model_name": "FloatProgressModel",
 "model_module_version": "1.5.0",
 "state": {
 "_dom_classes": [],
 "_model_module": "@jupyter-widgets/controls",
 "_model_module_version": "1.5.0",
 "_model_name": "FloatProgressModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/controls",
 "_view_module_version": "1.5.0",
 "_view_name": "ProgressView",
 "bar_style": "success",
 "description": "",
 "description_tooltip": null,
 "layout": "IPY_MODEL_c0343fb543954a5c948ce30e72c057fb",
 "max": 1,
 "min": 0,
 "orientation": "horizontal",
 "style": "IPY_MODEL_f5651379546d4d9da64111938b7d9449",
 "value": 1
 }
 },
 "0fd37791e2c24960b3ec87db6d1b5654": {
 "model_module": "@jupyter-widgets/controls",
 "model_name": "HTMLModel",
 "model_module_version": "1.5.0",
 "state": {
 "_dom_classes": [],
 "_model_module": "@jupyter-widgets/controls",
 "_model_module_version": "1.5.0",
 "_model_name": "HTMLModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/controls",
 "_view_module_version": "1.5.0",
 "_view_name": "HTMLView",
 "description": "",
 "description_tooltip": null,
 "layout": "IPY_MODEL_e3cb1dda1c3f4de4b752e5a4608a18e9",
 "placeholder": "\u200b",
 "style": "IPY_MODEL_0a4ada2df78d4770b5e373e2bd194c08",
 "value": "\u20071.96M/?\u2007[00:00&lt;00:00,\u200778.0MB/s]"
 }
 },
 "7b59aed4e88f42dab5fdef583b01fd7d": {
 "model_module": "@jupyter-widgets/base",
 "model_name": "LayoutModel",
 "model_module_version": "1.2.0",
 "state": {
 "_model_module": "@jupyter-widgets/base",
 "_model_module_version": "1.2.0",
 "_model_name": "LayoutModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/base",
 "_view_module_version": "1.2.0",
 "_view_name": "LayoutView",
 "align_content": null,
 "align_items": null,
 "align_self": null,
 "border": null,
 "bottom": null,
 "display": null,
 "flex": null,
 "flex_flow": null,
 "grid_area": null,
 "grid_auto_columns": null,
 "grid_auto_flow": null,
 "grid_auto_rows": null,
 "grid_column": null,
 "grid_gap": null,
 "grid_row": null,
 "grid_template_areas": null,
 "grid_template_columns": null,
 "grid_template_rows": null,
 "height": null,
 "justify_content": null,
 "justify_items": null,
 "left": null,
 "margin": null,
 "max_height": null,
 "max_width": null,
 "min_height": null,
 "min_width": null,
 "object_fit": null,
 "object_position": null,
 "order": null,
 "overflow": null,
 "overflow_x": null,
 "overflow_y": null,
 "padding": null,
 "right": null,
 "top": null,
 "visibility": null,
 "width": null
 }
 },
 "b196199206a34657ae972ce1a0f6f8d2": {
 "model_module": "@jupyter-widgets/base",
 "model_name": "LayoutModel",
 "model_module_version": "1.2.0",
 "state": {
 "_model_module": "@jupyter-widgets/base",
 "_model_module_version": "1.2.0",
 "_model_name": "LayoutModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/base",
 "_view_module_version": "1.2.0",
 "_view_name": "LayoutView",
 "align_content": null,
 "align_items": null,
 "align_self": null,
 "border": null,
 "bottom": null,
 "display": null,
 "flex": null,
 "flex_flow": null,
 "grid_area": null,
 "grid_auto_columns": null,
 "grid_auto_flow": null,
 "grid_auto_rows": null,
 "grid_column": null,
 "grid_gap": null,
 "grid_row": null,
 "grid_template_areas": null,
 "grid_template_columns": null,
 "grid_template_rows": null,
 "height": null,
 "justify_content": null,
 "justify_items": null,
 "left": null,
 "margin": null,
 "max_height": null,
 "max_width": null,
 "min_height": null,
 "min_width": null,
 "object_fit": null,
 "object_position": null,
 "order": null,
 "overflow": null,
 "overflow_x": null,
 "overflow_y": null,
 "padding": null,
 "right": null,
 "top": null,
 "visibility": null,
 "width": null
 }
 },
 "3b04b6185c4e41f1b11b80892c6dc3a0": {
 "model_module": "@jupyter-widgets/controls",
 "model_name": "DescriptionStyleModel",
 "model_module_version": "1.5.0",
 "state": {
 "_model_module": "@jupyter-widgets/controls",
 "_model_module_version": "1.5.0",
 "_model_name": "DescriptionStyleModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/base",
 "_view_module_version": "1.2.0",
 "_view_name": "StyleView",
 "description_width": ""
 }
 },
 "c0343fb543954a5c948ce30e72c057fb": {
 "model_module": "@jupyter-widgets/base",
 "model_name": "LayoutModel",
 "model_module_version": "1.2.0",
 "state": {
 "_model_module": "@jupyter-widgets/base",
 "_model_module_version": "1.2.0",
 "_model_name": "LayoutModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/base",
 "_view_module_version": "1.2.0",
 "_view_name": "LayoutView",
 "align_content": null,
 "align_items": null,
 "align_self": null,
 "border": null,
 "bottom": null,
 "display": null,
 "flex": null,
 "flex_flow": null,
 "grid_area": null,
 "grid_auto_columns": null,
 "grid_auto_flow": null,
 "grid_auto_rows": null,
 "grid_column": null,
 "grid_gap": null,
 "grid_row": null,
 "grid_template_areas": null,
 "grid_template_columns": null,
 "grid_template_rows": null,
 "height": null,
 "justify_content": null,
 "justify_items": null,
 "left": null,
 "margin": null,
 "max_height": null,
 "max_width": null,
 "min_height": null,
 "min_width": null,
 "object_fit": null,
 "object_position": null,
 "order": null,
 "overflow": null,
 "overflow_x": null,
 "overflow_y": null,
 "padding": null,
 "right": null,
 "top": null,
 "visibility": null,
 "width": "20px"
 }
 },
 "f5651379546d4d9da64111938b7d9449": {
 "model_module": "@jupyter-widgets/controls",
 "model_name": "ProgressStyleModel",
 "model_module_version": "1.5.0",
 "state": {
 "_model_module": "@jupyter-widgets/controls",
 "_model_module_version": "1.5.0",
 "_model_name": "ProgressStyleModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/base",
 "_view_module_version": "1.2.0",
 "_view_name": "StyleView",
 "bar_color": null,
 "description_width": ""
 }
 },
 "e3cb1dda1c3f4de4b752e5a4608a18e9": {
 "model_module": "@jupyter-widgets/base",
 "model_name": "LayoutModel",
 "model_module_version": "1.2.0",
 "state": {
 "_model_module": "@jupyter-widgets/base",
 "_model_module_version": "1.2.0",
 "_model_name": "LayoutModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/base",
 "_view_module_version": "1.2.0",
 "_view_name": "LayoutView",
 "align_content": null,
 "align_items": null,
 "align_self": null,
 "border": null,
 "bottom": null,
 "display": null,
 "flex": null,
 "flex_flow": null,
 "grid_area": null,
 "grid_auto_columns": null,
 "grid_auto_flow": null,
 "grid_auto_rows": null,
 "grid_column": null,
 "grid_gap": null,
 "grid_row": null,
 "grid_template_areas": null,
 "grid_template_columns": null,
 "grid_template_rows": null,
 "height": null,
 "justify_content": null,
 "justify_items": null,
 "left": null,
 "margin": null,
 "max_height": null,
 "max_width": null,
 "min_height": null,
 "min_width": null,
 "object_fit": null,
 "object_position": null,
 "order": null,
 "overflow": null,
 "overflow_x": null,
 "overflow_y": null,
 "padding": null,
 "right": null,
 "top": null,
 "visibility": null,
 "width": null
 }
 },
 "0a4ada2df78d4770b5e373e2bd194c08": {
 "model_module": "@jupyter-widgets/controls",
 "model_name": "DescriptionStyleModel",
 "model_module_version": "1.5.0",
 "state": {
 "_model_module": "@jupyter-widgets/controls",
 "_model_module_version": "1.5.0",
 "_model_name": "DescriptionStyleModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/base",
 "_view_module_version": "1.2.0",
 "_view_name": "StyleView",
 "description_width": ""
 }
 },
 "e32d7fc81a1b4abda7c687c244d9e601": {
 "model_module": "@jupyter-widgets/controls",
 "model_name": "HBoxModel",
 "model_module_version": "1.5.0",
 "state": {
 "_dom_classes": [],
 "_model_module": "@jupyter-widgets/controls",
 "_model_module_version": "1.5.0",
 "_model_name": "HBoxModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/controls",
 "_view_module_version": "1.5.0",
 "_view_name": "HBoxView",
 "box_style": "",
 "children": [
 "IPY_MODEL_f61d4fed82014f429f3379f11cb8c733",
 "IPY_MODEL_f7ed551041754535896aca95679ed170",
 "IPY_MODEL_a8d6691ad2de4c7493b95f3e6e77b50d"
 ],
 "layout": "IPY_MODEL_ca9d1a9b532642c0b89805b8c3086c81"
 }
 },
 "f61d4fed82014f429f3379f11cb8c733": {
 "model_module": "@jupyter-widgets/controls",
 "model_name": "HTMLModel",
 "model_module_version": "1.5.0",
 "state": {
 "_dom_classes": [],
 "_model_module": "@jupyter-widgets/controls",
 "_model_module_version": "1.5.0",
 "_model_name": "HTMLModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/controls",
 "_view_module_version": "1.5.0",
 "_view_name": "HTMLView",
 "description": "",
 "description_tooltip": null,
 "layout": "IPY_MODEL_2e22177f31b14107a0eaa336709b2791",
 "placeholder": "\u200b",
 "style": "IPY_MODEL_9f2a021a76084da2b10120e700dc76e7",
 "value": "special_tokens_map.json:\u2007100%"
 }
 },
 "f7ed551041754535896aca95679ed170": {
 "model_module": "@jupyter-widgets/controls",
 "model_name": "FloatProgressModel",
 "model_module_version": "1.5.0",
 "state": {
 "_dom_classes": [],
 "_model_module": "@jupyter-widgets/controls",
 "_model_module_version": "1.5.0",
 "_model_name": "FloatProgressModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/controls",
 "_view_module_version": "1.5.0",
 "_view_name": "ProgressView",
 "bar_style": "success",
 "description": "",
 "description_tooltip": null,
 "layout": "IPY_MODEL_0eb59101835a4b1da4bee3fa1a7bb0da",
 "max": 414,
 "min": 0,
 "orientation": "horizontal",
 "style": "IPY_MODEL_76c685ca1b9c4ca8b53fcd8fddbbaef0",
 "value": 414
 }
 },
 "a8d6691ad2de4c7493b95f3e6e77b50d": {
 "model_module": "@jupyter-widgets/controls",
 "model_name": "HTMLModel",
 "model_module_version": "1.5.0",
 "state": {
 "_dom_classes": [],
 "_model_module": "@jupyter-widgets/controls",
 "_model_module_version": "1.5.0",
 "_model_name": "HTMLModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/controls",
 "_view_module_version": "1.5.0",
 "_view_name": "HTMLView",
 "description": "",
 "description_tooltip": null,
 "layout": "IPY_MODEL_3d8376b6671e4530be0b88c28ef5939a",
 "placeholder": "\u200b",
 "style": "IPY_MODEL_c5c04afbcfef42c68cf4cfa88836a000",
 "value": "\u2007414/414\u2007[00:00&lt;00:00,\u200748.3kB/s]"
 }
 },
 "ca9d1a9b532642c0b89805b8c3086c81": {
 "model_module": "@jupyter-widgets/base",
 "model_name": "LayoutModel",
 "model_module_version": "1.2.0",
 "state": {
 "_model_module": "@jupyter-widgets/base",
 "_model_module_version": "1.2.0",
 "_model_name": "LayoutModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/base",
 "_view_module_version": "1.2.0",
 "_view_name": "LayoutView",
 "align_content": null,
 "align_items": null,
 "align_self": null,
 "border": null,
 "bottom": null,
 "display": null,
 "flex": null,
 "flex_flow": null,
 "grid_area": null,
 "grid_auto_columns": null,
 "grid_auto_flow": null,
 "grid_auto_rows": null,
 "grid_column": null,
 "grid_gap": null,
 "grid_row": null,
 "grid_template_areas": null,
 "grid_template_columns": null,
 "grid_template_rows": null,
 "height": null,
 "justify_content": null,
 "justify_items": null,
 "left": null,
 "margin": null,
 "max_height": null,
 "max_width": null,
 "min_height": null,
 "min_width": null,
 "object_fit": null,
 "object_position": null,
 "order": null,
 "overflow": null,
 "overflow_x": null,
 "overflow_y": null,
 "padding": null,
 "right": null,
 "top": null,
 "visibility": null,
 "width": null
 }
 },
 "2e22177f31b14107a0eaa336709b2791": {
 "model_module": "@jupyter-widgets/base",
 "model_name": "LayoutModel",
 "model_module_version": "1.2.0",
 "state": {
 "_model_module": "@jupyter-widgets/base",
 "_model_module_version": "1.2.0",
 "_model_name": "LayoutModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/base",
 "_view_module_version": "1.2.0",
 "_view_name": "LayoutView",
 "align_content": null,
 "align_items": null,
 "align_self": null,
 "border": null,
 "bottom": null,
 "display": null,
 "flex": null,
 "flex_flow": null,
 "grid_area": null,
 "grid_auto_columns": null,
 "grid_auto_flow": null,
 "grid_auto_rows": null,
 "grid_column": null,
 "grid_gap": null,
 "grid_row": null,
 "grid_template_areas": null,
 "grid_template_columns": null,
 "grid_template_rows": null,
 "height": null,
 "justify_content": null,
 "justify_items": null,
 "left": null,
 "margin": null,
 "max_height": null,
 "max_width": null,
 "min_height": null,
 "min_width": null,
 "object_fit": null,
 "object_position": null,
 "order": null,
 "overflow": null,
 "overflow_x": null,
 "overflow_y": null,
 "padding": null,
 "right": null,
 "top": null,
 "visibility": null,
 "width": null
 }
 },
 "9f2a021a76084da2b10120e700dc76e7": {
 "model_module": "@jupyter-widgets/controls",
 "model_name": "DescriptionStyleModel",
 "model_module_version": "1.5.0",
 "state": {
 "_model_module": "@jupyter-widgets/controls",
 "_model_module_version": "1.5.0",
 "_model_name": "DescriptionStyleModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/base",
 "_view_module_version": "1.2.0",
 "_view_name": "StyleView",
 "description_width": ""
 }
 },
 "0eb59101835a4b1da4bee3fa1a7bb0da": {
 "model_module": "@jupyter-widgets/base",
 "model_name": "LayoutModel",
 "model_module_version": "1.2.0",
 "state": {
 "_model_module": "@jupyter-widgets/base",
 "_model_module_version": "1.2.0",
 "_model_name": "LayoutModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/base",
 "_view_module_version": "1.2.0",
 "_view_name": "LayoutView",
 "align_content": null,
 "align_items": null,
 "align_self": null,
 "border": null,
 "bottom": null,
 "display": null,
 "flex": null,
 "flex_flow": null,
 "grid_area": null,
 "grid_auto_columns": null,
 "grid_auto_flow": null,
 "grid_auto_rows": null,
 "grid_column": null,
 "grid_gap": null,
 "grid_row": null,
 "grid_template_areas": null,
 "grid_template_columns": null,
 "grid_template_rows": null,
 "height": null,
 "justify_content": null,
 "justify_items": null,
 "left": null,
 "margin": null,
 "max_height": null,
 "max_width": null,
 "min_height": null,
 "min_width": null,
 "object_fit": null,
 "object_position": null,
 "order": null,
 "overflow": null,
 "overflow_x": null,
 "overflow_y": null,
 "padding": null,
 "right": null,
 "top": null,
 "visibility": null,
 "width": null
 }
 },
 "76c685ca1b9c4ca8b53fcd8fddbbaef0": {
 "model_module": "@jupyter-widgets/controls",
 "model_name": "ProgressStyleModel",
 "model_module_version": "1.5.0",
 "state": {
 "_model_module": "@jupyter-widgets/controls",
 "_model_module_version": "1.5.0",
 "_model_name": "ProgressStyleModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/base",
 "_view_module_version": "1.2.0",
 "_view_name": "StyleView",
 "bar_color": null,
 "description_width": ""
 }
 },
 "3d8376b6671e4530be0b88c28ef5939a": {
 "model_module": "@jupyter-widgets/base",
 "model_name": "LayoutModel",
 "model_module_version": "1.2.0",
 "state": {
 "_model_module": "@jupyter-widgets/base",
 "_model_module_version": "1.2.0",
 "_model_name": "LayoutModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/base",
 "_view_module_version": "1.2.0",
 "_view_name": "LayoutView",
 "align_content": null,
 "align_items": null,
 "align_self": null,
 "border": null,
 "bottom": null,
 "display": null,
 "flex": null,
 "flex_flow": null,
 "grid_area": null,
 "grid_auto_columns": null,
 "grid_auto_flow": null,
 "grid_auto_rows": null,
 "grid_column": null,
 "grid_gap": null,
 "grid_row": null,
 "grid_template_areas": null,
 "grid_template_columns": null,
 "grid_template_rows": null,
 "height": null,
 "justify_content": null,
 "justify_items": null,
 "left": null,
 "margin": null,
 "max_height": null,
 "max_width": null,
 "min_height": null,
 "min_width": null,
 "object_fit": null,
 "object_position": null,
 "order": null,
 "overflow": null,
 "overflow_x": null,
 "overflow_y": null,
 "padding": null,
 "right": null,
 "top": null,
 "visibility": null,
 "width": null
 }
 },
 "c5c04afbcfef42c68cf4cfa88836a000": {
 "model_module": "@jupyter-widgets/controls",
 "model_name": "DescriptionStyleModel",
 "model_module_version": "1.5.0",
 "state": {
 "_model_module": "@jupyter-widgets/controls",
 "_model_module_version": "1.5.0",
 "_model_name": "DescriptionStyleModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/base",
 "_view_module_version": "1.2.0",
 "_view_name": "StyleView",
 "description_width": ""
 }
 },
 "ac986c9b87424e8c935a348c83ea02ca": {
 "model_module": "@jupyter-widgets/controls",
 "model_name": "HBoxModel",
 "model_module_version": "1.5.0",
 "state": {
 "_dom_classes": [],
 "_model_module": "@jupyter-widgets/controls",
 "_model_module_version": "1.5.0",
 "_model_name": "HBoxModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/controls",
 "_view_module_version": "1.5.0",
 "_view_name": "HBoxView",
 "box_style": "",
 "children": [
 "IPY_MODEL_87d929364fd845f28603bf8dad4a2f32",
 "IPY_MODEL_81e96dbf342b4d85b6770b201cc22dbd",
 "IPY_MODEL_3f3263e023c64a14aad67561540f7b5c"
 ],
 "layout": "IPY_MODEL_8476640fe3bd462da979d54ec9d65818"
 }
 },
 "87d929364fd845f28603bf8dad4a2f32": {
 "model_module": "@jupyter-widgets/controls",
 "model_name": "HTMLModel",
 "model_module_version": "1.5.0",
 "state": {
 "_dom_classes": [],
 "_model_module": "@jupyter-widgets/controls",
 "_model_module_version": "1.5.0",
 "_model_name": "HTMLModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/controls",
 "_view_module_version": "1.5.0",
 "_view_name": "HTMLView",
 "description": "",
 "description_tooltip": null,
 "layout": "IPY_MODEL_3390fab8a87f467abd136f58b78f4f20",
 "placeholder": "\u200b",
 "style": "IPY_MODEL_e1d86a65e99b4690b484de16ecb162e9",
 "value": "tokenizer_config.json:\u2007100%"
 }
 },
 "81e96dbf342b4d85b6770b201cc22dbd": {
 "model_module": "@jupyter-widgets/controls",
 "model_name": "FloatProgressModel",
 "model_module_version": "1.5.0",
 "state": {
 "_dom_classes": [],
 "_model_module": "@jupyter-widgets/controls",
 "_model_module_version": "1.5.0",
 "_model_name": "FloatProgressModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/controls",
 "_view_module_version": "1.5.0",
 "_view_name": "ProgressView",
 "bar_style": "success",
 "description": "",
 "description_tooltip": null,
 "layout": "IPY_MODEL_30e06035b5ad4219a7c7dbfc9c75e9af",
 "max": 25,
 "min": 0,
 "orientation": "horizontal",
 "style": "IPY_MODEL_94e7accc3f1d4ae3b990e64e9fe3a3a1",
 "value": 25
 }
 },
 "3f3263e023c64a14aad67561540f7b5c": {
 "model_module": "@jupyter-widgets/controls",
 "model_name": "HTMLModel",
 "model_module_version": "1.5.0",
 "state": {
 "_dom_classes": [],
 "_model_module": "@jupyter-widgets/controls",
 "_model_module_version": "1.5.0",
 "_model_name": "HTMLModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/controls",
 "_view_module_version": "1.5.0",
 "_view_name": "HTMLView",
 "description": "",
 "description_tooltip": null,
 "layout": "IPY_MODEL_7383d151f62c4d0cae7bd2fd60c654e6",
 "placeholder": "\u200b",
 "style": "IPY_MODEL_44b572a936ec4c46a4c318d5279f88c3",
 "value": "\u200725.0/25.0\u2007[00:00&lt;00:00,\u20071.77kB/s]"
 }
 },
 "8476640fe3bd462da979d54ec9d65818": {
 "model_module": "@jupyter-widgets/base",
 "model_name": "LayoutModel",
 "model_module_version": "1.2.0",
 "state": {
 "_model_module": "@jupyter-widgets/base",
 "_model_module_version": "1.2.0",
 "_model_name": "LayoutModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/base",
 "_view_module_version": "1.2.0",
 "_view_name": "LayoutView",
 "align_content": null,
 "align_items": null,
 "align_self": null,
 "border": null,
 "bottom": null,
 "display": null,
 "flex": null,
 "flex_flow": null,
 "grid_area": null,
 "grid_auto_columns": null,
 "grid_auto_flow": null,
 "grid_auto_rows": null,
 "grid_column": null,
 "grid_gap": null,
 "grid_row": null,
 "grid_template_areas": null,
 "grid_template_columns": null,
 "grid_template_rows": null,
 "height": null,
 "justify_content": null,
 "justify_items": null,
 "left": null,
 "margin": null,
 "max_height": null,
 "max_width": null,
 "min_height": null,
 "min_width": null,
 "object_fit": null,
 "object_position": null,
 "order": null,
 "overflow": null,
 "overflow_x": null,
 "overflow_y": null,
 "padding": null,
 "right": null,
 "top": null,
 "visibility": null,
 "width": null
 }
 },
 "3390fab8a87f467abd136f58b78f4f20": {
 "model_module": "@jupyter-widgets/base",
 "model_name": "LayoutModel",
 "model_module_version": "1.2.0",
 "state": {
 "_model_module": "@jupyter-widgets/base",
 "_model_module_version": "1.2.0",
 "_model_name": "LayoutModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/base",
 "_view_module_version": "1.2.0",
 "_view_name": "LayoutView",
 "align_content": null,
 "align_items": null,
 "align_self": null,
 "border": null,
 "bottom": null,
 "display": null,
 "flex": null,
 "flex_flow": null,
 "grid_area": null,
 "grid_auto_columns": null,
 "grid_auto_flow": null,
 "grid_auto_rows": null,
 "grid_column": null,
 "grid_gap": null,
 "grid_row": null,
 "grid_template_areas": null,
 "grid_template_columns": null,
 "grid_template_rows": null,
 "height": null,
 "justify_content": null,
 "justify_items": null,
 "left": null,
 "margin": null,
 "max_height": null,
 "max_width": null,
 "min_height": null,
 "min_width": null,
 "object_fit": null,
 "object_position": null,
 "order": null,
 "overflow": null,
 "overflow_x": null,
 "overflow_y": null,
 "padding": null,
 "right": null,
 "top": null,
 "visibility": null,
 "width": null
 }
 },
 "e1d86a65e99b4690b484de16ecb162e9": {
 "model_module": "@jupyter-widgets/controls",
 "model_name": "DescriptionStyleModel",
 "model_module_version": "1.5.0",
 "state": {
 "_model_module": "@jupyter-widgets/controls",
 "_model_module_version": "1.5.0",
 "_model_name": "DescriptionStyleModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/base",
 "_view_module_version": "1.2.0",
 "_view_name": "StyleView",
 "description_width": ""
 }
 },
 "30e06035b5ad4219a7c7dbfc9c75e9af": {
 "model_module": "@jupyter-widgets/base",
 "model_name": "LayoutModel",
 "model_module_version": "1.2.0",
 "state": {
 "_model_module": "@jupyter-widgets/base",
 "_model_module_version": "1.2.0",
 "_model_name": "LayoutModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/base",
 "_view_module_version": "1.2.0",
 "_view_name": "LayoutView",
 "align_content": null,
 "align_items": null,
 "align_self": null,
 "border": null,
 "bottom": null,
 "display": null,
 "flex": null,
 "flex_flow": null,
 "grid_area": null,
 "grid_auto_columns": null,
 "grid_auto_flow": null,
 "grid_auto_rows": null,
 "grid_column": null,
 "grid_gap": null,
 "grid_row": null,
 "grid_template_areas": null,
 "grid_template_columns": null,
 "grid_template_rows": null,
 "height": null,
 "justify_content": null,
 "justify_items": null,
 "left": null,
 "margin": null,
 "max_height": null,
 "max_width": null,
 "min_height": null,
 "min_width": null,
 "object_fit": null,
 "object_position": null,
 "order": null,
 "overflow": null,
 "overflow_x": null,
 "overflow_y": null,
 "padding": null,
 "right": null,
 "top": null,
 "visibility": null,
 "width": null
 }
 },
 "94e7accc3f1d4ae3b990e64e9fe3a3a1": {
 "model_module": "@jupyter-widgets/controls",
 "model_name": "ProgressStyleModel",
 "model_module_version": "1.5.0",
 "state": {
 "_model_module": "@jupyter-widgets/controls",
 "_model_module_version": "1.5.0",
 "_model_name": "ProgressStyleModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/base",
 "_view_module_version": "1.2.0",
 "_view_name": "StyleView",
 "bar_color": null,
 "description_width": ""
 }
 },
 "7383d151f62c4d0cae7bd2fd60c654e6": {
 "model_module": "@jupyter-widgets/base",
 "model_name": "LayoutModel",
 "model_module_version": "1.2.0",
 "state": {
 "_model_module": "@jupyter-widgets/base",
 "_model_module_version": "1.2.0",
 "_model_name": "LayoutModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/base",
 "_view_module_version": "1.2.0",
 "_view_name": "LayoutView",
 "align_content": null,
 "align_items": null,
 "align_self": null,
 "border": null,
 "bottom": null,
 "display": null,
 "flex": null,
 "flex_flow": null,
 "grid_area": null,
 "grid_auto_columns": null,
 "grid_auto_flow": null,
 "grid_auto_rows": null,
 "grid_column": null,
 "grid_gap": null,
 "grid_row": null,
 "grid_template_areas": null,
 "grid_template_columns": null,
 "grid_template_rows": null,
 "height": null,
 "justify_content": null,
 "justify_items": null,
 "left": null,
 "margin": null,
 "max_height": null,
 "max_width": null,
 "min_height": null,
 "min_width": null,
 "object_fit": null,
 "object_position": null,
 "order": null,
 "overflow": null,
 "overflow_x": null,
 "overflow_y": null,
 "padding": null,
 "right": null,
 "top": null,
 "visibility": null,
 "width": null
 }
 },
 "44b572a936ec4c46a4c318d5279f88c3": {
 "model_module": "@jupyter-widgets/controls",
 "model_name": "DescriptionStyleModel",
 "model_module_version": "1.5.0",
 "state": {
 "_model_module": "@jupyter-widgets/controls",
 "_model_module_version": "1.5.0",
 "_model_name": "DescriptionStyleModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/base",
 "_view_module_version": "1.2.0",
 "_view_name": "StyleView",
 "description_width": ""
 }
 },
 "d1af6efb53e34b1a9fc35fac50bebf76": {
 "model_module": "@jupyter-widgets/controls",
 "model_name": "HBoxModel",
 "model_module_version": "1.5.0",
 "state": {
 "_dom_classes": [],
 "_model_module": "@jupyter-widgets/controls",
 "_model_module_version": "1.5.0",
 "_model_name": "HBoxModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/controls",
 "_view_module_version": "1.5.0",
 "_view_name": "HBoxView",
 "box_style": "",
 "children": [
 "IPY_MODEL_5be1bc95feb54774a9245426923fd335",
 "IPY_MODEL_3b7f3a3a86d34f9d90d0900e523c4dfe",
 "IPY_MODEL_542a891afa694295b6d260d4deaa0f56"
 ],
 "layout": "IPY_MODEL_e292432837774a89b79cf8bd7e4fd6c8"
 }
 },
 "5be1bc95feb54774a9245426923fd335": {
 "model_module": "@jupyter-widgets/controls",
 "model_name": "HTMLModel",
 "model_module_version": "1.5.0",
 "state": {
 "_dom_classes": [],
 "_model_module": "@jupyter-widgets/controls",
 "_model_module_version": "1.5.0",
 "_model_name": "HTMLModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/controls",
 "_view_module_version": "1.5.0",
 "_view_name": "HTMLView",
 "description": "",
 "description_tooltip": null,
 "layout": "IPY_MODEL_4b2405d391384bf2a56fc8c0cc041964",
 "placeholder": "\u200b",
 "style": "IPY_MODEL_00ba8be551204bf7a40a8ec48ae4447b",
 "value": "config.json:\u2007100%"
 }
 },
 "3b7f3a3a86d34f9d90d0900e523c4dfe": {
 "model_module": "@jupyter-widgets/controls",
 "model_name": "FloatProgressModel",
 "model_module_version": "1.5.0",
 "state": {
 "_dom_classes": [],
 "_model_module": "@jupyter-widgets/controls",
 "_model_module_version": "1.5.0",
 "_model_name": "FloatProgressModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/controls",
 "_view_module_version": "1.5.0",
 "_view_name": "ProgressView",
 "bar_style": "success",
 "description": "",
 "description_tooltip": null,
 "layout": "IPY_MODEL_ba351316e08542adb67110ad79366744",
 "max": 482,
 "min": 0,
 "orientation": "horizontal",
 "style": "IPY_MODEL_32ad1bdf1ac042e58b3a7aaaeedc6f87",
 "value": 482
 }
 },
 "542a891afa694295b6d260d4deaa0f56": {
 "model_module": "@jupyter-widgets/controls",
 "model_name": "HTMLModel",
 "model_module_version": "1.5.0",
 "state": {
 "_dom_classes": [],
 "_model_module": "@jupyter-widgets/controls",
 "_model_module_version": "1.5.0",
 "_model_name": "HTMLModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/controls",
 "_view_module_version": "1.5.0",
 "_view_name": "HTMLView",
 "description": "",
 "description_tooltip": null,
 "layout": "IPY_MODEL_fb33497c962643069813b0586fa25d18",
 "placeholder": "\u200b",
 "style": "IPY_MODEL_0d71168e4d404f719aac3f509e95fb6f",
 "value": "\u2007482/482\u2007[00:00&lt;00:00,\u200718.8kB/s]"
 }
 },
 "e292432837774a89b79cf8bd7e4fd6c8": {
 "model_module": "@jupyter-widgets/base",
 "model_name": "LayoutModel",
 "model_module_version": "1.2.0",
 "state": {
 "_model_module": "@jupyter-widgets/base",
 "_model_module_version": "1.2.0",
 "_model_name": "LayoutModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/base",
 "_view_module_version": "1.2.0",
 "_view_name": "LayoutView",
 "align_content": null,
 "align_items": null,
 "align_self": null,
 "border": null,
 "bottom": null,
 "display": null,
 "flex": null,
 "flex_flow": null,
 "grid_area": null,
 "grid_auto_columns": null,
 "grid_auto_flow": null,
 "grid_auto_rows": null,
 "grid_column": null,
 "grid_gap": null,
 "grid_row": null,
 "grid_template_areas": null,
 "grid_template_columns": null,
 "grid_template_rows": null,
 "height": null,
 "justify_content": null,
 "justify_items": null,
 "left": null,
 "margin": null,
 "max_height": null,
 "max_width": null,
 "min_height": null,
 "min_width": null,
 "object_fit": null,
 "object_position": null,
 "order": null,
 "overflow": null,
 "overflow_x": null,
 "overflow_y": null,
 "padding": null,
 "right": null,
 "top": null,
 "visibility": null,
 "width": null
 }
 },
 "4b2405d391384bf2a56fc8c0cc041964": {
 "model_module": "@jupyter-widgets/base",
 "model_name": "LayoutModel",
 "model_module_version": "1.2.0",
 "state": {
 "_model_module": "@jupyter-widgets/base",
 "_model_module_version": "1.2.0",
 "_model_name": "LayoutModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/base",
 "_view_module_version": "1.2.0",
 "_view_name": "LayoutView",
 "align_content": null,
 "align_items": null,
 "align_self": null,
 "border": null,
 "bottom": null,
 "display": null,
 "flex": null,
 "flex_flow": null,
 "grid_area": null,
 "grid_auto_columns": null,
 "grid_auto_flow": null,
 "grid_auto_rows": null,
 "grid_column": null,
 "grid_gap": null,
 "grid_row": null,
 "grid_template_areas": null,
 "grid_template_columns": null,
 "grid_template_rows": null,
 "height": null,
 "justify_content": null,
 "justify_items": null,
 "left": null,
 "margin": null,
 "max_height": null,
 "max_width": null,
 "min_height": null,
 "min_width": null,
 "object_fit": null,
 "object_position": null,
 "order": null,
 "overflow": null,
 "overflow_x": null,
 "overflow_y": null,
 "padding": null,
 "right": null,
 "top": null,
 "visibility": null,
 "width": null
 }
 },
 "00ba8be551204bf7a40a8ec48ae4447b": {
 "model_module": "@jupyter-widgets/controls",
 "model_name": "DescriptionStyleModel",
 "model_module_version": "1.5.0",
 "state": {
 "_model_module": "@jupyter-widgets/controls",
 "_model_module_version": "1.5.0",
 "_model_name": "DescriptionStyleModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/base",
 "_view_module_version": "1.2.0",
 "_view_name": "StyleView",
 "description_width": ""
 }
 },
 "ba351316e08542adb67110ad79366744": {
 "model_module": "@jupyter-widgets/base",
 "model_name": "LayoutModel",
 "model_module_version": "1.2.0",
 "state": {
 "_model_module": "@jupyter-widgets/base",
 "_model_module_version": "1.2.0",
 "_model_name": "LayoutModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/base",
 "_view_module_version": "1.2.0",
 "_view_name": "LayoutView",
 "align_content": null,
 "align_items": null,
 "align_self": null,
 "border": null,
 "bottom": null,
 "display": null,
 "flex": null,
 "flex_flow": null,
 "grid_area": null,
 "grid_auto_columns": null,
 "grid_auto_flow": null,
 "grid_auto_rows": null,
 "grid_column": null,
 "grid_gap": null,
 "grid_row": null,
 "grid_template_areas": null,
 "grid_template_columns": null,
 "grid_template_rows": null,
 "height": null,
 "justify_content": null,
 "justify_items": null,
 "left": null,
 "margin": null,
 "max_height": null,
 "max_width": null,
 "min_height": null,
 "min_width": null,
 "object_fit": null,
 "object_position": null,
 "order": null,
 "overflow": null,
 "overflow_x": null,
 "overflow_y": null,
 "padding": null,
 "right": null,
 "top": null,
 "visibility": null,
 "width": null
 }
 },
 "32ad1bdf1ac042e58b3a7aaaeedc6f87": {
 "model_module": "@jupyter-widgets/controls",
 "model_name": "ProgressStyleModel",
 "model_module_version": "1.5.0",
 "state": {
 "_model_module": "@jupyter-widgets/controls",
 "_model_module_version": "1.5.0",
 "_model_name": "ProgressStyleModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/base",
 "_view_module_version": "1.2.0",
 "_view_name": "StyleView",
 "bar_color": null,
 "description_width": ""
 }
 },
 "fb33497c962643069813b0586fa25d18": {
 "model_module": "@jupyter-widgets/base",
 "model_name": "LayoutModel",
 "model_module_version": "1.2.0",
 "state": {
 "_model_module": "@jupyter-widgets/base",
 "_model_module_version": "1.2.0",
 "_model_name": "LayoutModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/base",
 "_view_module_version": "1.2.0",
 "_view_name": "LayoutView",
 "align_content": null,
 "align_items": null,
 "align_self": null,
 "border": null,
 "bottom": null,
 "display": null,
 "flex": null,
 "flex_flow": null,
 "grid_area": null,
 "grid_auto_columns": null,
 "grid_auto_flow": null,
 "grid_auto_rows": null,
 "grid_column": null,
 "grid_gap": null,
 "grid_row": null,
 "grid_template_areas": null,
 "grid_template_columns": null,
 "grid_template_rows": null,
 "height": null,
 "justify_content": null,
 "justify_items": null,
 "left": null,
 "margin": null,
 "max_height": null,
 "max_width": null,
 "min_height": null,
 "min_width": null,
 "object_fit": null,
 "object_position": null,
 "order": null,
 "overflow": null,
 "overflow_x": null,
 "overflow_y": null,
 "padding": null,
 "right": null,
 "top": null,
 "visibility": null,
 "width": null
 }
 },
 "0d71168e4d404f719aac3f509e95fb6f": {
 "model_module": "@jupyter-widgets/controls",
 "model_name": "DescriptionStyleModel",
 "model_module_version": "1.5.0",
 "state": {
 "_model_module": "@jupyter-widgets/controls",
 "_model_module_version": "1.5.0",
 "_model_name": "DescriptionStyleModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/base",
 "_view_module_version": "1.2.0",
 "_view_name": "StyleView",
 "description_width": ""
 }
 },
 "c71360e64cde441bbb6f9a43e50b6275": {
 "model_module": "@jupyter-widgets/controls",
 "model_name": "HBoxModel",
 "model_module_version": "1.5.0",
 "state": {
 "_dom_classes": [],
 "_model_module": "@jupyter-widgets/controls",
 "_model_module_version": "1.5.0",
 "_model_name": "HBoxModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/controls",
 "_view_module_version": "1.5.0",
 "_view_name": "HBoxView",
 "box_style": "",
 "children": [
 "IPY_MODEL_4e53df3bd14848b1a45653667e863d34",
 "IPY_MODEL_ab50f14c81bc4ac4b95349ea0cba4f44",
 "IPY_MODEL_1fa1db49248a4a4b8ce9b0c7e0bebd3a"
 ],
 "layout": "IPY_MODEL_f32492742e7b435488d6627105c0b7ba"
 }
 },
 "4e53df3bd14848b1a45653667e863d34": {
 "model_module": "@jupyter-widgets/controls",
 "model_name": "HTMLModel",
 "model_module_version": "1.5.0",
 "state": {
 "_dom_classes": [],
 "_model_module": "@jupyter-widgets/controls",
 "_model_module_version": "1.5.0",
 "_model_name": "HTMLModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/controls",
 "_view_module_version": "1.5.0",
 "_view_name": "HTMLView",
 "description": "",
 "description_tooltip": null,
 "layout": "IPY_MODEL_028ef42b972f4786a1ed93ee3a06a483",
 "placeholder": "\u200b",
 "style": "IPY_MODEL_55c0b8a49bfb4a7087de0873dd40a4a2",
 "value": "vocab.json:\u2007100%"
 }
 },
 "ab50f14c81bc4ac4b95349ea0cba4f44": {
 "model_module": "@jupyter-widgets/controls",
 "model_name": "FloatProgressModel",
 "model_module_version": "1.5.0",
 "state": {
 "_dom_classes": [],
 "_model_module": "@jupyter-widgets/controls",
 "_model_module_version": "1.5.0",
 "_model_name": "FloatProgressModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/controls",
 "_view_module_version": "1.5.0",
 "_view_name": "ProgressView",
 "bar_style": "success",
 "description": "",
 "description_tooltip": null,
 "layout": "IPY_MODEL_e7dd126e72dc4de6974eef82b2416075",
 "max": 898823,
 "min": 0,
 "orientation": "horizontal",
 "style": "IPY_MODEL_6dca418abd0f49e5a3dc9e859aff916f",
 "value": 898823
 }
 },
 "1fa1db49248a4a4b8ce9b0c7e0bebd3a": {
 "model_module": "@jupyter-widgets/controls",
 "model_name": "HTMLModel",
 "model_module_version": "1.5.0",
 "state": {
 "_dom_classes": [],
 "_model_module": "@jupyter-widgets/controls",
 "_model_module_version": "1.5.0",
 "_model_name": "HTMLModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/controls",
 "_view_module_version": "1.5.0",
 "_view_name": "HTMLView",
 "description": "",
 "description_tooltip": null,
 "layout": "IPY_MODEL_50728ced30684790b76a4a136789a265",
 "placeholder": "\u200b",
 "style": "IPY_MODEL_ed0bcfbf927c49e19b9d473596be91f8",
 "value": "\u2007899k/899k\u2007[00:00&lt;00:00,\u200729.7MB/s]"
 }
 },
 "f32492742e7b435488d6627105c0b7ba": {
 "model_module": "@jupyter-widgets/base",
 "model_name": "LayoutModel",
 "model_module_version": "1.2.0",
 "state": {
 "_model_module": "@jupyter-widgets/base",
 "_model_module_version": "1.2.0",
 "_model_name": "LayoutModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/base",
 "_view_module_version": "1.2.0",
 "_view_name": "LayoutView",
 "align_content": null,
 "align_items": null,
 "align_self": null,
 "border": null,
 "bottom": null,
 "display": null,
 "flex": null,
 "flex_flow": null,
 "grid_area": null,
 "grid_auto_columns": null,
 "grid_auto_flow": null,
 "grid_auto_rows": null,
 "grid_column": null,
 "grid_gap": null,
 "grid_row": null,
 "grid_template_areas": null,
 "grid_template_columns": null,
 "grid_template_rows": null,
 "height": null,
 "justify_content": null,
 "justify_items": null,
 "left": null,
 "margin": null,
 "max_height": null,
 "max_width": null,
 "min_height": null,
 "min_width": null,
 "object_fit": null,
 "object_position": null,
 "order": null,
 "overflow": null,
 "overflow_x": null,
 "overflow_y": null,
 "padding": null,
 "right": null,
 "top": null,
 "visibility": null,
 "width": null
 }
 },
 "028ef42b972f4786a1ed93ee3a06a483": {
 "model_module": "@jupyter-widgets/base",
 "model_name": "LayoutModel",
 "model_module_version": "1.2.0",
 "state": {
 "_model_module": "@jupyter-widgets/base",
 "_model_module_version": "1.2.0",
 "_model_name": "LayoutModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/base",
 "_view_module_version": "1.2.0",
 "_view_name": "LayoutView",
 "align_content": null,
 "align_items": null,
 "align_self": null,
 "border": null,
 "bottom": null,
 "display": null,
 "flex": null,
 "flex_flow": null,
 "grid_area": null,
 "grid_auto_columns": null,
 "grid_auto_flow": null,
 "grid_auto_rows": null,
 "grid_column": null,
 "grid_gap": null,
 "grid_row": null,
 "grid_template_areas": null,
 "grid_template_columns": null,
 "grid_template_rows": null,
 "height": null,
 "justify_content": null,
 "justify_items": null,
 "left": null,
 "margin": null,
 "max_height": null,
 "max_width": null,
 "min_height": null,
 "min_width": null,
 "object_fit": null,
 "object_position": null,
 "order": null,
 "overflow": null,
 "overflow_x": null,
 "overflow_y": null,
 "padding": null,
 "right": null,
 "top": null,
 "visibility": null,
 "width": null
 }
 },
 "55c0b8a49bfb4a7087de0873dd40a4a2": {
 "model_module": "@jupyter-widgets/controls",
 "model_name": "DescriptionStyleModel",
 "model_module_version": "1.5.0",
 "state": {
 "_model_module": "@jupyter-widgets/controls",
 "_model_module_version": "1.5.0",
 "_model_name": "DescriptionStyleModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/base",
 "_view_module_version": "1.2.0",
 "_view_name": "StyleView",
 "description_width": ""
 }
 },
 "e7dd126e72dc4de6974eef82b2416075": {
 "model_module": "@jupyter-widgets/base",
 "model_name": "LayoutModel",
 "model_module_version": "1.2.0",
 "state": {
 "_model_module": "@jupyter-widgets/base",
 "_model_module_version": "1.2.0",
 "_model_name": "LayoutModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/base",
 "_view_module_version": "1.2.0",
 "_view_name": "LayoutView",
 "align_content": null,
 "align_items": null,
 "align_self": null,
 "border": null,
 "bottom": null,
 "display": null,
 "flex": null,
 "flex_flow": null,
 "grid_area": null,
 "grid_auto_columns": null,
 "grid_auto_flow": null,
 "grid_auto_rows": null,
 "grid_column": null,
 "grid_gap": null,
 "grid_row": null,
 "grid_template_areas": null,
 "grid_template_columns": null,
 "grid_template_rows": null,
 "height": null,
 "justify_content": null,
 "justify_items": null,
 "left": null,
 "margin": null,
 "max_height": null,
 "max_width": null,
 "min_height": null,
 "min_width": null,
 "object_fit": null,
 "object_position": null,
 "order": null,
 "overflow": null,
 "overflow_x": null,
 "overflow_y": null,
 "padding": null,
 "right": null,
 "top": null,
 "visibility": null,
 "width": null
 }
 },
 "6dca418abd0f49e5a3dc9e859aff916f": {
 "model_module": "@jupyter-widgets/controls",
 "model_name": "ProgressStyleModel",
 "model_module_version": "1.5.0",
 "state": {
 "_model_module": "@jupyter-widgets/controls",
 "_model_module_version": "1.5.0",
 "_model_name": "ProgressStyleModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/base",
 "_view_module_version": "1.2.0",
 "_view_name": "StyleView",
 "bar_color": null,
 "description_width": ""
 }
 },
 "50728ced30684790b76a4a136789a265": {
 "model_module": "@jupyter-widgets/base",
 "model_name": "LayoutModel",
 "model_module_version": "1.2.0",
 "state": {
 "_model_module": "@jupyter-widgets/base",
 "_model_module_version": "1.2.0",
 "_model_name": "LayoutModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/base",
 "_view_module_version": "1.2.0",
 "_view_name": "LayoutView",
 "align_content": null,
 "align_items": null,
 "align_self": null,
 "border": null,
 "bottom": null,
 "display": null,
 "flex": null,
 "flex_flow": null,
 "grid_area": null,
 "grid_auto_columns": null,
 "grid_auto_flow": null,
 "grid_auto_rows": null,
 "grid_column": null,
 "grid_gap": null,
 "grid_row": null,
 "grid_template_areas": null,
 "grid_template_columns": null,
 "grid_template_rows": null,
 "height": null,
 "justify_content": null,
 "justify_items": null,
 "left": null,
 "margin": null,
 "max_height": null,
 "max_width": null,
 "min_height": null,
 "min_width": null,
 "object_fit": null,
 "object_position": null,
 "order": null,
 "overflow": null,
 "overflow_x": null,
 "overflow_y": null,
 "padding": null,
 "right": null,
 "top": null,
 "visibility": null,
 "width": null
 }
 },
 "ed0bcfbf927c49e19b9d473596be91f8": {
 "model_module": "@jupyter-widgets/controls",
 "model_name": "DescriptionStyleModel",
 "model_module_version": "1.5.0",
 "state": {
 "_model_module": "@jupyter-widgets/controls",
 "_model_module_version": "1.5.0",
 "_model_name": "DescriptionStyleModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/base",
 "_view_module_version": "1.2.0",
 "_view_name": "StyleView",
 "description_width": ""
 }
 },
 "3672ddcdc58e4c57852f4e057a7d2f6a": {
 "model_module": "@jupyter-widgets/controls",
 "model_name": "HBoxModel",
 "model_module_version": "1.5.0",
 "state": {
 "_dom_classes": [],
 "_model_module": "@jupyter-widgets/controls",
 "_model_module_version": "1.5.0",
 "_model_name": "HBoxModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/controls",
 "_view_module_version": "1.5.0",
 "_view_name": "HBoxView",
 "box_style": "",
 "children": [
 "IPY_MODEL_0e8b713ae3eb4c21b079ad04b941381a",
 "IPY_MODEL_bfb566ef9baa49b087f294391a38b21c",
 "IPY_MODEL_ac45ef00b991402aa3eb2bfa744b4705"
 ],
 "layout": "IPY_MODEL_5542e1edfddc4854ba707ce9b15c5c40"
 }
 },
 "0e8b713ae3eb4c21b079ad04b941381a": {
 "model_module": "@jupyter-widgets/controls",
 "model_name": "HTMLModel",
 "model_module_version": "1.5.0",
 "state": {
 "_dom_classes": [],
 "_model_module": "@jupyter-widgets/controls",
 "_model_module_version": "1.5.0",
 "_model_name": "HTMLModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/controls",
 "_view_module_version": "1.5.0",
 "_view_name": "HTMLView",
 "description": "",
 "description_tooltip": null,
 "layout": "IPY_MODEL_dd0abe2a38f2446daef5ac79dbd6f438",
 "placeholder": "\u200b",
 "style": "IPY_MODEL_898525e3faa1496a84d6235aa5f4d31f",
 "value": "merges.txt:\u2007100%"
 }
 },
 "bfb566ef9baa49b087f294391a38b21c": {
 "model_module": "@jupyter-widgets/controls",
 "model_name": "FloatProgressModel",
 "model_module_version": "1.5.0",
 "state": {
 "_dom_classes": [],
 "_model_module": "@jupyter-widgets/controls",
 "_model_module_version": "1.5.0",
 "_model_name": "FloatProgressModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/controls",
 "_view_module_version": "1.5.0",
 "_view_name": "ProgressView",
 "bar_style": "success",
 "description": "",
 "description_tooltip": null,
 "layout": "IPY_MODEL_b2f4a9019d7e4a3bb38dea50169b7d96",
 "max": 456318,
 "min": 0,
 "orientation": "horizontal",
 "style": "IPY_MODEL_4d2c36de32f04a8c9221e5f91d1556b4",
 "value": 456318
 }
 },
 "ac45ef00b991402aa3eb2bfa744b4705": {
 "model_module": "@jupyter-widgets/controls",
 "model_name": "HTMLModel",
 "model_module_version": "1.5.0",
 "state": {
 "_dom_classes": [],
 "_model_module": "@jupyter-widgets/controls",
 "_model_module_version": "1.5.0",
 "_model_name": "HTMLModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/controls",
 "_view_module_version": "1.5.0",
 "_view_name": "HTMLView",
 "description": "",
 "description_tooltip": null,
 "layout": "IPY_MODEL_93db32c6ab384c539889db92eb42b9fa",
 "placeholder": "\u200b",
 "style": "IPY_MODEL_85dc5a5d6da04665b338103c6951454a",
 "value": "\u2007456k/456k\u2007[00:00&lt;00:00,\u200716.5MB/s]"
 }
 },
 "5542e1edfddc4854ba707ce9b15c5c40": {
 "model_module": "@jupyter-widgets/base",
 "model_name": "LayoutModel",
 "model_module_version": "1.2.0",
 "state": {
 "_model_module": "@jupyter-widgets/base",
 "_model_module_version": "1.2.0",
 "_model_name": "LayoutModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/base",
 "_view_module_version": "1.2.0",
 "_view_name": "LayoutView",
 "align_content": null,
 "align_items": null,
 "align_self": null,
 "border": null,
 "bottom": null,
 "display": null,
 "flex": null,
 "flex_flow": null,
 "grid_area": null,
 "grid_auto_columns": null,
 "grid_auto_flow": null,
 "grid_auto_rows": null,
 "grid_column": null,
 "grid_gap": null,
 "grid_row": null,
 "grid_template_areas": null,
 "grid_template_columns": null,
 "grid_template_rows": null,
 "height": null,
 "justify_content": null,
 "justify_items": null,
 "left": null,
 "margin": null,
 "max_height": null,
 "max_width": null,
 "min_height": null,
 "min_width": null,
 "object_fit": null,
 "object_position": null,
 "order": null,
 "overflow": null,
 "overflow_x": null,
 "overflow_y": null,
 "padding": null,
 "right": null,
 "top": null,
 "visibility": null,
 "width": null
 }
 },
 "dd0abe2a38f2446daef5ac79dbd6f438": {
 "model_module": "@jupyter-widgets/base",
 "model_name": "LayoutModel",
 "model_module_version": "1.2.0",
 "state": {
 "_model_module": "@jupyter-widgets/base",
 "_model_module_version": "1.2.0",
 "_model_name": "LayoutModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/base",
 "_view_module_version": "1.2.0",
 "_view_name": "LayoutView",
 "align_content": null,
 "align_items": null,
 "align_self": null,
 "border": null,
 "bottom": null,
 "display": null,
 "flex": null,
 "flex_flow": null,
 "grid_area": null,
 "grid_auto_columns": null,
 "grid_auto_flow": null,
 "grid_auto_rows": null,
 "grid_column": null,
 "grid_gap": null,
 "grid_row": null,
 "grid_template_areas": null,
 "grid_template_columns": null,
 "grid_template_rows": null,
 "height": null,
 "justify_content": null,
 "justify_items": null,
 "left": null,
 "margin": null,
 "max_height": null,
 "max_width": null,
 "min_height": null,
 "min_width": null,
 "object_fit": null,
 "object_position": null,
 "order": null,
 "overflow": null,
 "overflow_x": null,
 "overflow_y": null,
 "padding": null,
 "right": null,
 "top": null,
 "visibility": null,
 "width": null
 }
 },
 "898525e3faa1496a84d6235aa5f4d31f": {
 "model_module": "@jupyter-widgets/controls",
 "model_name": "DescriptionStyleModel",
 "model_module_version": "1.5.0",
 "state": {
 "_model_module": "@jupyter-widgets/controls",
 "_model_module_version": "1.5.0",
 "_model_name": "DescriptionStyleModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/base",
 "_view_module_version": "1.2.0",
 "_view_name": "StyleView",
 "description_width": ""
 }
 },
 "b2f4a9019d7e4a3bb38dea50169b7d96": {
 "model_module": "@jupyter-widgets/base",
 "model_name": "LayoutModel",
 "model_module_version": "1.2.0",
 "state": {
 "_model_module": "@jupyter-widgets/base",
 "_model_module_version": "1.2.0",
 "_model_name": "LayoutModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/base",
 "_view_module_version": "1.2.0",
 "_view_name": "LayoutView",
 "align_content": null,
 "align_items": null,
 "align_self": null,
 "border": null,
 "bottom": null,
 "display": null,
 "flex": null,
 "flex_flow": null,
 "grid_area": null,
 "grid_auto_columns": null,
 "grid_auto_flow": null,
 "grid_auto_rows": null,
 "grid_column": null,
 "grid_gap": null,
 "grid_row": null,
 "grid_template_areas": null,
 "grid_template_columns": null,
 "grid_template_rows": null,
 "height": null,
 "justify_content": null,
 "justify_items": null,
 "left": null,
 "margin": null,
 "max_height": null,
 "max_width": null,
 "min_height": null,
 "min_width": null,
 "object_fit": null,
 "object_position": null,
 "order": null,
 "overflow": null,
 "overflow_x": null,
 "overflow_y": null,
 "padding": null,
 "right": null,
 "top": null,
 "visibility": null,
 "width": null
 }
 },
 "4d2c36de32f04a8c9221e5f91d1556b4": {
 "model_module": "@jupyter-widgets/controls",
 "model_name": "ProgressStyleModel",
 "model_module_version": "1.5.0",
 "state": {
 "_model_module": "@jupyter-widgets/controls",
 "_model_module_version": "1.5.0",
 "_model_name": "ProgressStyleModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/base",
 "_view_module_version": "1.2.0",
 "_view_name": "StyleView",
 "bar_color": null,
 "description_width": ""
 }
 },
 "93db32c6ab384c539889db92eb42b9fa": {
 "model_module": "@jupyter-widgets/base",
 "model_name": "LayoutModel",
 "model_module_version": "1.2.0",
 "state": {
 "_model_module": "@jupyter-widgets/base",
 "_model_module_version": "1.2.0",
 "_model_name": "LayoutModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/base",
 "_view_module_version": "1.2.0",
 "_view_name": "LayoutView",
 "align_content": null,
 "align_items": null,
 "align_self": null,
 "border": null,
 "bottom": null,
 "display": null,
 "flex": null,
 "flex_flow": null,
 "grid_area": null,
 "grid_auto_columns": null,
 "grid_auto_flow": null,
 "grid_auto_rows": null,
 "grid_column": null,
 "grid_gap": null,
 "grid_row": null,
 "grid_template_areas": null,
 "grid_template_columns": null,
 "grid_template_rows": null,
 "height": null,
 "justify_content": null,
 "justify_items": null,
 "left": null,
 "margin": null,
 "max_height": null,
 "max_width": null,
 "min_height": null,
 "min_width": null,
 "object_fit": null,
 "object_position": null,
 "order": null,
 "overflow": null,
 "overflow_x": null,
 "overflow_y": null,
 "padding": null,
 "right": null,
 "top": null,
 "visibility": null,
 "width": null
 }
 },
 "85dc5a5d6da04665b338103c6951454a": {
 "model_module": "@jupyter-widgets/controls",
 "model_name": "DescriptionStyleModel",
 "model_module_version": "1.5.0",
 "state": {
 "_model_module": "@jupyter-widgets/controls",
 "_model_module_version": "1.5.0",
 "_model_name": "DescriptionStyleModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/base",
 "_view_module_version": "1.2.0",
 "_view_name": "StyleView",
 "description_width": ""
 }
 },
 "ea383e14b0eb4c6c8f824d2a0dda442a": {
 "model_module": "@jupyter-widgets/controls",
 "model_name": "HBoxModel",
 "model_module_version": "1.5.0",
 "state": {
 "_dom_classes": [],
 "_model_module": "@jupyter-widgets/controls",
 "_model_module_version": "1.5.0",
 "_model_name": "HBoxModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/controls",
 "_view_module_version": "1.5.0",
 "_view_name": "HBoxView",
 "box_style": "",
 "children": [
 "IPY_MODEL_f8825d591ab04586a194cf93a2a1e36e",
 "IPY_MODEL_ba5a38cb9ebe43e6b0c81f8c0c98c074",
 "IPY_MODEL_a86c969dd7d94d68a38ace129013e23b"
 ],
 "layout": "IPY_MODEL_165557436e894cb5a140cd858704d286"
 }
 },
 "f8825d591ab04586a194cf93a2a1e36e": {
 "model_module": "@jupyter-widgets/controls",
 "model_name": "HTMLModel",
 "model_module_version": "1.5.0",
 "state": {
 "_dom_classes": [],
 "_model_module": "@jupyter-widgets/controls",
 "_model_module_version": "1.5.0",
 "_model_name": "HTMLModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/controls",
 "_view_module_version": "1.5.0",
 "_view_name": "HTMLView",
 "description": "",
 "description_tooltip": null,
 "layout": "IPY_MODEL_5f9937c0ac414db4bbb5670668916fce",
 "placeholder": "\u200b",
 "style": "IPY_MODEL_e81b36e85a4946debb783f49f35d9ca0",
 "value": "tokenizer.json:\u2007100%"
 }
 },
 "ba5a38cb9ebe43e6b0c81f8c0c98c074": {
 "model_module": "@jupyter-widgets/controls",
 "model_name": "FloatProgressModel",
 "model_module_version": "1.5.0",
 "state": {
 "_dom_classes": [],
 "_model_module": "@jupyter-widgets/controls",
 "_model_module_version": "1.5.0",
 "_model_name": "FloatProgressModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/controls",
 "_view_module_version": "1.5.0",
 "_view_name": "ProgressView",
 "bar_style": "success",
 "description": "",
 "description_tooltip": null,
 "layout": "IPY_MODEL_841d86e95b38470bb43e7e4c0184a13e",
 "max": 1355863,
 "min": 0,
 "orientation": "horizontal",
 "style": "IPY_MODEL_962e24113fa644698504a6c30f120e0a",
 "value": 1355863
 }
 },
 "a86c969dd7d94d68a38ace129013e23b": {
 "model_module": "@jupyter-widgets/controls",
 "model_name": "HTMLModel",
 "model_module_version": "1.5.0",
 "state": {
 "_dom_classes": [],
 "_model_module": "@jupyter-widgets/controls",
 "_model_module_version": "1.5.0",
 "_model_name": "HTMLModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/controls",
 "_view_module_version": "1.5.0",
 "_view_name": "HTMLView",
 "description": "",
 "description_tooltip": null,
 "layout": "IPY_MODEL_be30a8507de5499cbb1ad968e9421b4f",
 "placeholder": "\u200b",
 "style": "IPY_MODEL_b4de77e7c36b4cf0ab097a4fe83b89da",
 "value": "\u20071.36M/1.36M\u2007[00:00&lt;00:00,\u200717.5MB/s]"
 }
 },
 "165557436e894cb5a140cd858704d286": {
 "model_module": "@jupyter-widgets/base",
 "model_name": "LayoutModel",
 "model_module_version": "1.2.0",
 "state": {
 "_model_module": "@jupyter-widgets/base",
 "_model_module_version": "1.2.0",
 "_model_name": "LayoutModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/base",
 "_view_module_version": "1.2.0",
 "_view_name": "LayoutView",
 "align_content": null,
 "align_items": null,
 "align_self": null,
 "border": null,
 "bottom": null,
 "display": null,
 "flex": null,
 "flex_flow": null,
 "grid_area": null,
 "grid_auto_columns": null,
 "grid_auto_flow": null,
 "grid_auto_rows": null,
 "grid_column": null,
 "grid_gap": null,
 "grid_row": null,
 "grid_template_areas": null,
 "grid_template_columns": null,
 "grid_template_rows": null,
 "height": null,
 "justify_content": null,
 "justify_items": null,
 "left": null,
 "margin": null,
 "max_height": null,
 "max_width": null,
 "min_height": null,
 "min_width": null,
 "object_fit": null,
 "object_position": null,
 "order": null,
 "overflow": null,
 "overflow_x": null,
 "overflow_y": null,
 "padding": null,
 "right": null,
 "top": null,
 "visibility": null,
 "width": null
 }
 },
 "5f9937c0ac414db4bbb5670668916fce": {
 "model_module": "@jupyter-widgets/base",
 "model_name": "LayoutModel",
 "model_module_version": "1.2.0",
 "state": {
 "_model_module": "@jupyter-widgets/base",
 "_model_module_version": "1.2.0",
 "_model_name": "LayoutModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/base",
 "_view_module_version": "1.2.0",
 "_view_name": "LayoutView",
 "align_content": null,
 "align_items": null,
 "align_self": null,
 "border": null,
 "bottom": null,
 "display": null,
 "flex": null,
 "flex_flow": null,
 "grid_area": null,
 "grid_auto_columns": null,
 "grid_auto_flow": null,
 "grid_auto_rows": null,
 "grid_column": null,
 "grid_gap": null,
 "grid_row": null,
 "grid_template_areas": null,
 "grid_template_columns": null,
 "grid_template_rows": null,
 "height": null,
 "justify_content": null,
 "justify_items": null,
 "left": null,
 "margin": null,
 "max_height": null,
 "max_width": null,
 "min_height": null,
 "min_width": null,
 "object_fit": null,
 "object_position": null,
 "order": null,
 "overflow": null,
 "overflow_x": null,
 "overflow_y": null,
 "padding": null,
 "right": null,
 "top": null,
 "visibility": null,
 "width": null
 }
 },
 "e81b36e85a4946debb783f49f35d9ca0": {
 "model_module": "@jupyter-widgets/controls",
 "model_name": "DescriptionStyleModel",
 "model_module_version": "1.5.0",
 "state": {
 "_model_module": "@jupyter-widgets/controls",
 "_model_module_version": "1.5.0",
 "_model_name": "DescriptionStyleModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/base",
 "_view_module_version": "1.2.0",
 "_view_name": "StyleView",
 "description_width": ""
 }
 },
 "841d86e95b38470bb43e7e4c0184a13e": {
 "model_module": "@jupyter-widgets/base",
 "model_name": "LayoutModel",
 "model_module_version": "1.2.0",
 "state": {
 "_model_module": "@jupyter-widgets/base",
 "_model_module_version": "1.2.0",
 "_model_name": "LayoutModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/base",
 "_view_module_version": "1.2.0",
 "_view_name": "LayoutView",
 "align_content": null,
 "align_items": null,
 "align_self": null,
 "border": null,
 "bottom": null,
 "display": null,
 "flex": null,
 "flex_flow": null,
 "grid_area": null,
 "grid_auto_columns": null,
 "grid_auto_flow": null,
 "grid_auto_rows": null,
 "grid_column": null,
 "grid_gap": null,
 "grid_row": null,
 "grid_template_areas": null,
 "grid_template_columns": null,
 "grid_template_rows": null,
 "height": null,
 "justify_content": null,
 "justify_items": null,
 "left": null,
 "margin": null,
 "max_height": null,
 "max_width": null,
 "min_height": null,
 "min_width": null,
 "object_fit": null,
 "object_position": null,
 "order": null,
 "overflow": null,
 "overflow_x": null,
 "overflow_y": null,
 "padding": null,
 "right": null,
 "top": null,
 "visibility": null,
 "width": null
 }
 },
 "962e24113fa644698504a6c30f120e0a": {
 "model_module": "@jupyter-widgets/controls",
 "model_name": "ProgressStyleModel",
 "model_module_version": "1.5.0",
 "state": {
 "_model_module": "@jupyter-widgets/controls",
 "_model_module_version": "1.5.0",
 "_model_name": "ProgressStyleModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/base",
 "_view_module_version": "1.2.0",
 "_view_name": "StyleView",
 "bar_color": null,
 "description_width": ""
 }
 },
 "be30a8507de5499cbb1ad968e9421b4f": {
 "model_module": "@jupyter-widgets/base",
 "model_name": "LayoutModel",
 "model_module_version": "1.2.0",
 "state": {
 "_model_module": "@jupyter-widgets/base",
 "_model_module_version": "1.2.0",
 "_model_name": "LayoutModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/base",
 "_view_module_version": "1.2.0",
 "_view_name": "LayoutView",
 "align_content": null,
 "align_items": null,
 "align_self": null,
 "border": null,
 "bottom": null,
 "display": null,
 "flex": null,
 "flex_flow": null,
 "grid_area": null,
 "grid_auto_columns": null,
 "grid_auto_flow": null,
 "grid_auto_rows": null,
 "grid_column": null,
 "grid_gap": null,
 "grid_row": null,
 "grid_template_areas": null,
 "grid_template_columns": null,
 "grid_template_rows": null,
 "height": null,
 "justify_content": null,
 "justify_items": null,
 "left": null,
 "margin": null,
 "max_height": null,
 "max_width": null,
 "min_height": null,
 "min_width": null,
 "object_fit": null,
 "object_position": null,
 "order": null,
 "overflow": null,
 "overflow_x": null,
 "overflow_y": null,
 "padding": null,
 "right": null,
 "top": null,
 "visibility": null,
 "width": null
 }
 },
 "b4de77e7c36b4cf0ab097a4fe83b89da": {
 "model_module": "@jupyter-widgets/controls",
 "model_name": "DescriptionStyleModel",
 "model_module_version": "1.5.0",
 "state": {
 "_model_module": "@jupyter-widgets/controls",
 "_model_module_version": "1.5.0",
 "_model_name": "DescriptionStyleModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/base",
 "_view_module_version": "1.2.0",
 "_view_name": "StyleView",
 "description_width": ""
 }
 },
 "0027ace2e7614c3ab6eb6db33f3efac0": {
 "model_module": "@jupyter-widgets/controls",
 "model_name": "HBoxModel",
 "model_module_version": "1.5.0",
 "state": {
 "_dom_classes": [],
 "_model_module": "@jupyter-widgets/controls",
 "_model_module_version": "1.5.0",
 "_model_name": "HBoxModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/controls",
 "_view_module_version": "1.5.0",
 "_view_name": "HBoxView",
 "box_style": "",
 "children": [
 "IPY_MODEL_7b65f36ac816481b903b2e53d068761b",
 "IPY_MODEL_e3506725163842068c7dbb1475038712",
 "IPY_MODEL_8fe64231480c459c83d54c4784e75299"
 ],
 "layout": "IPY_MODEL_6f517a1a6b5b4f06b85442bf9beb4f52"
 }
 },
 "7b65f36ac816481b903b2e53d068761b": {
 "model_module": "@jupyter-widgets/controls",
 "model_name": "HTMLModel",
 "model_module_version": "1.5.0",
 "state": {
 "_dom_classes": [],
 "_model_module": "@jupyter-widgets/controls",
 "_model_module_version": "1.5.0",
 "_model_name": "HTMLModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/controls",
 "_view_module_version": "1.5.0",
 "_view_name": "HTMLView",
 "description": "",
 "description_tooltip": null,
 "layout": "IPY_MODEL_3f8819125d00462e9e1503ccfe26ffed",
 "placeholder": "\u200b",
 "style": "IPY_MODEL_e16cb0de1e88409f8c9ef35336bb609e",
 "value": "model.safetensors:\u2007100%"
 }
 },
 "e3506725163842068c7dbb1475038712": {
 "model_module": "@jupyter-widgets/controls",
 "model_name": "FloatProgressModel",
 "model_module_version": "1.5.0",
 "state": {
 "_dom_classes": [],
 "_model_module": "@jupyter-widgets/controls",
 "_model_module_version": "1.5.0",
 "_model_name": "FloatProgressModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/controls",
 "_view_module_version": "1.5.0",
 "_view_name": "ProgressView",
 "bar_style": "success",
 "description": "",
 "description_tooltip": null,
 "layout": "IPY_MODEL_55677e2a7a2c4dad8c0e672895474407",
 "max": 1421700479,
 "min": 0,
 "orientation": "horizontal",
 "style": "IPY_MODEL_3b4638840c2f47279e285d7ea758b1c0",
 "value": 1421700479
 }
 },
 "8fe64231480c459c83d54c4784e75299": {
 "model_module": "@jupyter-widgets/controls",
 "model_name": "HTMLModel",
 "model_module_version": "1.5.0",
 "state": {
 "_dom_classes": [],
 "_model_module": "@jupyter-widgets/controls",
 "_model_module_version": "1.5.0",
 "_model_name": "HTMLModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/controls",
 "_view_module_version": "1.5.0",
 "_view_name": "HTMLView",
 "description": "",
 "description_tooltip": null,
 "layout": "IPY_MODEL_7e093d96dbdc4b709b1d797c41f6b72f",
 "placeholder": "\u200b",
 "style": "IPY_MODEL_a042b02b7d5a4f43959a1cf4dfa7177b",
 "value": "\u20071.42G/1.42G\u2007[00:12&lt;00:00,\u2007254MB/s]"
 }
 },
 "6f517a1a6b5b4f06b85442bf9beb4f52": {
 "model_module": "@jupyter-widgets/base",
 "model_name": "LayoutModel",
 "model_module_version": "1.2.0",
 "state": {
 "_model_module": "@jupyter-widgets/base",
 "_model_module_version": "1.2.0",
 "_model_name": "LayoutModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/base",
 "_view_module_version": "1.2.0",
 "_view_name": "LayoutView",
 "align_content": null,
 "align_items": null,
 "align_self": null,
 "border": null,
 "bottom": null,
 "display": null,
 "flex": null,
 "flex_flow": null,
 "grid_area": null,
 "grid_auto_columns": null,
 "grid_auto_flow": null,
 "grid_auto_rows": null,
 "grid_column": null,
 "grid_gap": null,
 "grid_row": null,
 "grid_template_areas": null,
 "grid_template_columns": null,
 "grid_template_rows": null,
 "height": null,
 "justify_content": null,
 "justify_items": null,
 "left": null,
 "margin": null,
 "max_height": null,
 "max_width": null,
 "min_height": null,
 "min_width": null,
 "object_fit": null,
 "object_position": null,
 "order": null,
 "overflow": null,
 "overflow_x": null,
 "overflow_y": null,
 "padding": null,
 "right": null,
 "top": null,
 "visibility": null,
 "width": null
 }
 },
 "3f8819125d00462e9e1503ccfe26ffed": {
 "model_module": "@jupyter-widgets/base",
 "model_name": "LayoutModel",
 "model_module_version": "1.2.0",
 "state": {
 "_model_module": "@jupyter-widgets/base",
 "_model_module_version": "1.2.0",
 "_model_name": "LayoutModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/base",
 "_view_module_version": "1.2.0",
 "_view_name": "LayoutView",
 "align_content": null,
 "align_items": null,
 "align_self": null,
 "border": null,
 "bottom": null,
 "display": null,
 "flex": null,
 "flex_flow": null,
 "grid_area": null,
 "grid_auto_columns": null,
 "grid_auto_flow": null,
 "grid_auto_rows": null,
 "grid_column": null,
 "grid_gap": null,
 "grid_row": null,
 "grid_template_areas": null,
 "grid_template_columns": null,
 "grid_template_rows": null,
 "height": null,
 "justify_content": null,
 "justify_items": null,
 "left": null,
 "margin": null,
 "max_height": null,
 "max_width": null,
 "min_height": null,
 "min_width": null,
 "object_fit": null,
 "object_position": null,
 "order": null,
 "overflow": null,
 "overflow_x": null,
 "overflow_y": null,
 "padding": null,
 "right": null,
 "top": null,
 "visibility": null,
 "width": null
 }
 },
 "e16cb0de1e88409f8c9ef35336bb609e": {
 "model_module": "@jupyter-widgets/controls",
 "model_name": "DescriptionStyleModel",
 "model_module_version": "1.5.0",
 "state": {
 "_model_module": "@jupyter-widgets/controls",
 "_model_module_version": "1.5.0",
 "_model_name": "DescriptionStyleModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/base",
 "_view_module_version": "1.2.0",
 "_view_name": "StyleView",
 "description_width": ""
 }
 },
 "55677e2a7a2c4dad8c0e672895474407": {
 "model_module": "@jupyter-widgets/base",
 "model_name": "LayoutModel",
 "model_module_version": "1.2.0",
 "state": {
 "_model_module": "@jupyter-widgets/base",
 "_model_module_version": "1.2.0",
 "_model_name": "LayoutModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/base",
 "_view_module_version": "1.2.0",
 "_view_name": "LayoutView",
 "align_content": null,
 "align_items": null,
 "align_self": null,
 "border": null,
 "bottom": null,
 "display": null,
 "flex": null,
 "flex_flow": null,
 "grid_area": null,
 "grid_auto_columns": null,
 "grid_auto_flow": null,
 "grid_auto_rows": null,
 "grid_column": null,
 "grid_gap": null,
 "grid_row": null,
 "grid_template_areas": null,
 "grid_template_columns": null,
 "grid_template_rows": null,
 "height": null,
 "justify_content": null,
 "justify_items": null,
 "left": null,
 "margin": null,
 "max_height": null,
 "max_width": null,
 "min_height": null,
 "min_width": null,
 "object_fit": null,
 "object_position": null,
 "order": null,
 "overflow": null,
 "overflow_x": null,
 "overflow_y": null,
 "padding": null,
 "right": null,
 "top": null,
 "visibility": null,
 "width": null
 }
 },
 "3b4638840c2f47279e285d7ea758b1c0": {
 "model_module": "@jupyter-widgets/controls",
 "model_name": "ProgressStyleModel",
 "model_module_version": "1.5.0",
 "state": {
 "_model_module": "@jupyter-widgets/controls",
 "_model_module_version": "1.5.0",
 "_model_name": "ProgressStyleModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/base",
 "_view_module_version": "1.2.0",
 "_view_name": "StyleView",
 "bar_color": null,
 "description_width": ""
 }
 },
 "7e093d96dbdc4b709b1d797c41f6b72f": {
 "model_module": "@jupyter-widgets/base",
 "model_name": "LayoutModel",
 "model_module_version": "1.2.0",
 "state": {
 "_model_module": "@jupyter-widgets/base",
 "_model_module_version": "1.2.0",
 "_model_name": "LayoutModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/base",
 "_view_module_version": "1.2.0",
 "_view_name": "LayoutView",
 "align_content": null,
 "align_items": null,
 "align_self": null,
 "border": null,
 "bottom": null,
 "display": null,
 "flex": null,
 "flex_flow": null,
 "grid_area": null,
 "grid_auto_columns": null,
 "grid_auto_flow": null,
 "grid_auto_rows": null,
 "grid_column": null,
 "grid_gap": null,
 "grid_row": null,
 "grid_template_areas": null,
 "grid_template_columns": null,
 "grid_template_rows": null,
 "height": null,
 "justify_content": null,
 "justify_items": null,
 "left": null,
 "margin": null,
 "max_height": null,
 "max_width": null,
 "min_height": null,
 "min_width": null,
 "object_fit": null,
 "object_position": null,
 "order": null,
 "overflow": null,
 "overflow_x": null,
 "overflow_y": null,
 "padding": null,
 "right": null,
 "top": null,
 "visibility": null,
 "width": null
 }
 },
 "a042b02b7d5a4f43959a1cf4dfa7177b": {
 "model_module": "@jupyter-widgets/controls",
 "model_name": "DescriptionStyleModel",
 "model_module_version": "1.5.0",
 "state": {
 "_model_module": "@jupyter-widgets/controls",
 "_model_module_version": "1.5.0",
 "_model_name": "DescriptionStyleModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/base",
 "_view_module_version": "1.2.0",
 "_view_name": "StyleView",
 "description_width": ""
 }
 },
 "1ca385be4bc54a00b5e36a7ab793b4b9": {
 "model_module": "@jupyter-widgets/controls",
 "model_name": "HBoxModel",
 "model_module_version": "1.5.0",
 "state": {
 "_dom_classes": [],
 "_model_module": "@jupyter-widgets/controls",
 "_model_module_version": "1.5.0",
 "_model_name": "HBoxModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/controls",
 "_view_module_version": "1.5.0",
 "_view_name": "HBoxView",
 "box_style": "",
 "children": [
 "IPY_MODEL_3706c3f374604361bc38a8b1c50df7e8",
 "IPY_MODEL_3071c4cde66446079d17c4f1d656e9b2",
 "IPY_MODEL_42a692d223eb4f02b9f443ab91c5401a"
 ],
 "layout": "IPY_MODEL_d4a7446d69204b84989611904804d87b"
 }
 },
 "3706c3f374604361bc38a8b1c50df7e8": {
 "model_module": "@jupyter-widgets/controls",
 "model_name": "HTMLModel",
 "model_module_version": "1.5.0",
 "state": {
 "_dom_classes": [],
 "_model_module": "@jupyter-widgets/controls",
 "_model_module_version": "1.5.0",
 "_model_name": "HTMLModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/controls",
 "_view_module_version": "1.5.0",
 "_view_name": "HTMLView",
 "description": "",
 "description_tooltip": null,
 "layout": "IPY_MODEL_6269eaeaaad742ba8ba17b66fd9a2fc3",
 "placeholder": "\u200b",
 "style": "IPY_MODEL_7ff5a13bbeb34e798f689d92a22e830a",
 "value": "Evaluating:\u2007100%"
 }
 },
 "3071c4cde66446079d17c4f1d656e9b2": {
 "model_module": "@jupyter-widgets/controls",
 "model_name": "FloatProgressModel",
 "model_module_version": "1.5.0",
 "state": {
 "_dom_classes": [],
 "_model_module": "@jupyter-widgets/controls",
 "_model_module_version": "1.5.0",
 "_model_name": "FloatProgressModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/controls",
 "_view_module_version": "1.5.0",
 "_view_name": "ProgressView",
 "bar_style": "success",
 "description": "",
 "description_tooltip": null,
 "layout": "IPY_MODEL_441b52492ec84061953d74c35b79ce17",
 "max": 780,
 "min": 0,
 "orientation": "horizontal",
 "style": "IPY_MODEL_aefea3228cb54222aadadf2e5021ec84",
 "value": 780
 }
 },
 "42a692d223eb4f02b9f443ab91c5401a": {
 "model_module": "@jupyter-widgets/controls",
 "model_name": "HTMLModel",
 "model_module_version": "1.5.0",
 "state": {
 "_dom_classes": [],
 "_model_module": "@jupyter-widgets/controls",
 "_model_module_version": "1.5.0",
 "_model_name": "HTMLModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/controls",
 "_view_module_version": "1.5.0",
 "_view_name": "HTMLView",
 "description": "",
 "description_tooltip": null,
 "layout": "IPY_MODEL_83167d53ecc54c61aafc389b12f7ea3a",
 "placeholder": "\u200b",
 "style": "IPY_MODEL_9a290f503fb84433a444e254cdc0558e",
 "value": "\u2007780/780\u2007[16:41&lt;00:00,\u2007\u20072.48s/it]"
 }
 },
 "d4a7446d69204b84989611904804d87b": {
 "model_module": "@jupyter-widgets/base",
 "model_name": "LayoutModel",
 "model_module_version": "1.2.0",
 "state": {
 "_model_module": "@jupyter-widgets/base",
 "_model_module_version": "1.2.0",
 "_model_name": "LayoutModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/base",
 "_view_module_version": "1.2.0",
 "_view_name": "LayoutView",
 "align_content": null,
 "align_items": null,
 "align_self": null,
 "border": null,
 "bottom": null,
 "display": null,
 "flex": null,
 "flex_flow": null,
 "grid_area": null,
 "grid_auto_columns": null,
 "grid_auto_flow": null,
 "grid_auto_rows": null,
 "grid_column": null,
 "grid_gap": null,
 "grid_row": null,
 "grid_template_areas": null,
 "grid_template_columns": null,
 "grid_template_rows": null,
 "height": null,
 "justify_content": null,
 "justify_items": null,
 "left": null,
 "margin": null,
 "max_height": null,
 "max_width": null,
 "min_height": null,
 "min_width": null,
 "object_fit": null,
 "object_position": null,
 "order": null,
 "overflow": null,
 "overflow_x": null,
 "overflow_y": null,
 "padding": null,
 "right": null,
 "top": null,
 "visibility": null,
 "width": null
 }
 },
 "6269eaeaaad742ba8ba17b66fd9a2fc3": {
 "model_module": "@jupyter-widgets/base",
 "model_name": "LayoutModel",
 "model_module_version": "1.2.0",
 "state": {
 "_model_module": "@jupyter-widgets/base",
 "_model_module_version": "1.2.0",
 "_model_name": "LayoutModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/base",
 "_view_module_version": "1.2.0",
 "_view_name": "LayoutView",
 "align_content": null,
 "align_items": null,
 "align_self": null,
 "border": null,
 "bottom": null,
 "display": null,
 "flex": null,
 "flex_flow": null,
 "grid_area": null,
 "grid_auto_columns": null,
 "grid_auto_flow": null,
 "grid_auto_rows": null,
 "grid_column": null,
 "grid_gap": null,
 "grid_row": null,
 "grid_template_areas": null,
 "grid_template_columns": null,
 "grid_template_rows": null,
 "height": null,
 "justify_content": null,
 "justify_items": null,
 "left": null,
 "margin": null,
 "max_height": null,
 "max_width": null,
 "min_height": null,
 "min_width": null,
 "object_fit": null,
 "object_position": null,
 "order": null,
 "overflow": null,
 "overflow_x": null,
 "overflow_y": null,
 "padding": null,
 "right": null,
 "top": null,
 "visibility": null,
 "width": null
 }
 },
 "7ff5a13bbeb34e798f689d92a22e830a": {
 "model_module": "@jupyter-widgets/controls",
 "model_name": "DescriptionStyleModel",
 "model_module_version": "1.5.0",
 "state": {
 "_model_module": "@jupyter-widgets/controls",
 "_model_module_version": "1.5.0",
 "_model_name": "DescriptionStyleModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/base",
 "_view_module_version": "1.2.0",
 "_view_name": "StyleView",
 "description_width": ""
 }
 },
 "441b52492ec84061953d74c35b79ce17": {
 "model_module": "@jupyter-widgets/base",
 "model_name": "LayoutModel",
 "model_module_version": "1.2.0",
 "state": {
 "_model_module": "@jupyter-widgets/base",
 "_model_module_version": "1.2.0",
 "_model_name": "LayoutModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/base",
 "_view_module_version": "1.2.0",
 "_view_name": "LayoutView",
 "align_content": null,
 "align_items": null,
 "align_self": null,
 "border": null,
 "bottom": null,
 "display": null,
 "flex": null,
 "flex_flow": null,
 "grid_area": null,
 "grid_auto_columns": null,
 "grid_auto_flow": null,
 "grid_auto_rows": null,
 "grid_column": null,
 "grid_gap": null,
 "grid_row": null,
 "grid_template_areas": null,
 "grid_template_columns": null,
 "grid_template_rows": null,
 "height": null,
 "justify_content": null,
 "justify_items": null,
 "left": null,
 "margin": null,
 "max_height": null,
 "max_width": null,
 "min_height": null,
 "min_width": null,
 "object_fit": null,
 "object_position": null,
 "order": null,
 "overflow": null,
 "overflow_x": null,
 "overflow_y": null,
 "padding": null,
 "right": null,
 "top": null,
 "visibility": null,
 "width": null
 }
 },
 "aefea3228cb54222aadadf2e5021ec84": {
 "model_module": "@jupyter-widgets/controls",
 "model_name": "ProgressStyleModel",
 "model_module_version": "1.5.0",
 "state": {
 "_model_module": "@jupyter-widgets/controls",
 "_model_module_version": "1.5.0",
 "_model_name": "ProgressStyleModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/base",
 "_view_module_version": "1.2.0",
 "_view_name": "StyleView",
 "bar_color": null,
 "description_width": ""
 }
 },
 "83167d53ecc54c61aafc389b12f7ea3a": {
 "model_module": "@jupyter-widgets/base",
 "model_name": "LayoutModel",
 "model_module_version": "1.2.0",
 "state": {
 "_model_module": "@jupyter-widgets/base",
 "_model_module_version": "1.2.0",
 "_model_name": "LayoutModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/base",
 "_view_module_version": "1.2.0",
 "_view_name": "LayoutView",
 "align_content": null,
 "align_items": null,
 "align_self": null,
 "border": null,
 "bottom": null,
 "display": null,
 "flex": null,
 "flex_flow": null,
 "grid_area": null,
 "grid_auto_columns": null,
 "grid_auto_flow": null,
 "grid_auto_rows": null,
 "grid_column": null,
 "grid_gap": null,
 "grid_row": null,
 "grid_template_areas": null,
 "grid_template_columns": null,
 "grid_template_rows": null,
 "height": null,
 "justify_content": null,
 "justify_items": null,
 "left": null,
 "margin": null,
 "max_height": null,
 "max_width": null,
 "min_height": null,
 "min_width": null,
 "object_fit": null,
 "object_position": null,
 "order": null,
 "overflow": null,
 "overflow_x": null,
 "overflow_y": null,
 "padding": null,
 "right": null,
 "top": null,
 "visibility": null,
 "width": null
 }
 },
 "9a290f503fb84433a444e254cdc0558e": {
 "model_module": "@jupyter-widgets/controls",
 "model_name": "DescriptionStyleModel",
 "model_module_version": "1.5.0",
 "state": {
 "_model_module": "@jupyter-widgets/controls",
 "_model_module_version": "1.5.0",
 "_model_name": "DescriptionStyleModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/base",
 "_view_module_version": "1.2.0",
 "_view_name": "StyleView",
 "description_width": ""
 }
 }
 }
 }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}