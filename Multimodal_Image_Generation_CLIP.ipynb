{
 "cells": [
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "9hZNSDOBJcBm"
 },
 "source": "# Multimodal AI: Image Generation & CLIP Evaluation\n\nThis notebook demonstrates multimodal AI capabilities including text-to-image generation and image understanding.\n\n**Skills Demonstrated:**\n- Text-to-image generation with diffusion models\n- CLIP for zero-shot image classification\n- Visual Question Answering (VQA) implementation\n- Automated image evaluation pipelines\n- Prompt engineering for image generation\n\n**Technologies:** PyTorch, Hugging Face Transformers, CLIP, Stable Diffusion"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "aY7Q6CAWTtIj"
 },
 "source": "## 1: Multimodality with Image Generation & Captioning\n\nThis notebook works with PyTorch and Hugging Face to get more experience with its abstract classes as well as some new models. We will look at the simple task of image generation and use a number of tools to see how we could programatically evaluate the accuracy of the image generation process.\n\nThis notebook covers:\n\n1. **Image Generation** \n\n Here we will explore how the Canva image diffusion model generates images conditioned on a text prompt.\n\n2. **Image Classification**\n\n Here we will use CLIP to evaluate captions that describe our images to see which labels most accurately describes our generated images.\n\n3. **Image Evaluation**\n\n We will also use a visual question answering system to ask questions about our generated image. In our prompt we asked for certain items in the image. In the question answering system we can ask if those items are present in the image.\n\n**Models we'll use in this exercise**\n\n**Canva** - generate images from two prompts\n\n**CLIP** - compare \"text\" labels with image\n\n**VQA** - answer questions about image"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "CWnrgn6mwxjL"
 },
 "source": "Images are stored for reuse across the notebook. You can do the same thing in your @.edu account."
 },
 {
 "cell_type": "code",
 "execution_count": null,
 "metadata": {
 "id": "HKbwvxLeyQlh"
 },
 "outputs": [],
 "source": "#mount Google Drive\nimport os\nfrom google.colab import drive\ndrive.mount('/content/drive')\n\n# Define your directory\noutput_folder = \"/content/drive/MyDrive/output/2025-fall/TestNotebooks/\"\n\n# Ensure the output directory exists\nif not os.path.exists(output_folder):\n os.makedirs(output_folder)"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "N67c8mlxiPfT"
 },
 "source": "### 0.0 Image Generation\n\nYou are going to use Canva, a commercial image generation platform, to generate two different images to use for our tests. To use Canva, you must first set up a free account on the system. Click [here](https://www.canva.com) go to canva. They will ask you to log in. YOu can do this either using one of your Google accounts or you can give them an email address and they will send you a code that you'll need to enter to get logged in.\n\nOnce you are logged in you will want to \"create\" an image, you will need to enter a text prompt describing the image you want and then click the run button on the right to generate the image. You will then have 4 images you can choose from. You can select the one you like by single clicking on it. You will then need to click the download button in the image to download the image to your laptop. The image must be saved in either *.png or *.jpg format.\n\nYou must generate two separate images for use in this exercise. \n\nThe first image must feature one central object that is the focus of the image. Be sure to specify that you want just one. I used `\"portrait of a dog sitting in a chair\"`. The dog is the central object in this image. You must use a different prompt for your own object. Save this Canva generated image on your laptop as a file named `OneThing.jpg` or .png if you prefer.\n\nFor this second image you will specify a prompt with **3** of the same object *(Type 1)*, **2** of a different kind of object *(Type 2)* \"in the background\", and two other different individual objects *(Type 3)* and *(Type 4)* in the scene. For example `3 cats in the garden with 2 snakes in the background with a flowering tree and a rose bush`. Make up your own prompt with your own set of type 1, type 2, type 3, and type 4 objects. Make sure you specify the count that goes with each type in your prompt. Save this image on your laptop as a file named `ThreeThingsPlus.jpg` or .png if you prefer.\n\nYou must place these two images in the `MyDrive/output/2025-fall/TestNotebooks` directory you created in your GoogleDrive so they can be accessed by this Colab notebook."
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "tceASp32w8fA"
 },
 "source": "### 1.0 Environment Setup\n\nLet us first install a few required packages. (You may want to comment this out in case you use a local environment that already has the suitable packages installed.)"
 },
 {
 "cell_type": "code",
 "execution_count": null,
 "metadata": {
 "id": "5lHMLXu3Pew7"
 },
 "outputs": [],
 "source": "!pip install -q -U transformers\n!pip install -q -U bitsandbytes accelerate"
 },
 {
 "cell_type": "code",
 "execution_count": null,
 "metadata": {
 "id": "lg0tGYLSKVAN"
 },
 "outputs": [],
 "source": "#You can generate multiple images but not if you are using\n# the T4 GPU. This assignment is designed to run with a single image\n\nfrom PIL import Image\nfrom pprint import pprint\n\ndef image_grid(imgs, rows, cols):\n assert len(imgs) == rows*cols\n\n w, h = imgs[0].size\n grid = Image.new('RGB', size=(cols*w, rows*h))\n grid_w, grid_h = grid.size\n\n for i, img in enumerate(imgs):\n grid.paste(img, box=(i%cols*w, i//cols*h))\n return grid"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "gz24tv8m10_D"
 },
 "source": "### Canva - Generate Images with one object\n\nWe're going to generate an image using a diffusion model ([Canva](https://www.canva.com)). You will specify a prompt that names one object. Be sure to specify that you want just one. I used ``\"portrait of a dog sitting in a chair\"``. How can we programatically tell if the generated image follows our prompt? We can use some other tools that can examine the photo and tell if it's contents are what we asked for or if the generator fell short. First, you need to load the model and then generate and image."
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "j6L_KeFDz_KH"
 },
 "source": "**1.a. Examining the enhanced prompt you created to generate the image `OneThing.jpg` with the single object on Canva? Enter the prompt as a triple quote string in the cell below.**\n"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "ACEpgVbjsfoq"
 },
 "source": "Make sure your Canva generated images are downloaded from Canva and put in your Google Drive as instructed. Now let's display that image of the single object."
 },
 {
 "cell_type": "code",
 "execution_count": null,
 "metadata": {
 "id": "f0OsHJKITn-o"
 },
 "outputs": [],
 "source": "from PIL import Image\nimg_url = '/content/drive/MyDrive/output/2025-fall/TestNotebooks/OneThing.jpg'\nraw_image = Image.open(img_url, mode='r')"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "oe13C4wMTn-m"
 },
 "source": "### CLIP - evaluate image with one object\n\nNow we'll use the CLIP model ([Model Card](https://huggingface.co/openai/clip-vit-base-patch32)) to see if our image resembles the object in the prompt. You'll need to edit the list of captions below. One label in the list should be the object you requested in the prompt. Other labels can be similiar objects and one should be orthogonal (very different from your chosen object). Your labels should each be one or two words long (e.g. cat, dog, blue whale). This is similar to building a classifier that \"recognizes\" images by predicting one of many possible labels. You should have at least 4 different labels one of which identifies your single object."
 },
 {
 "cell_type": "code",
 "execution_count": null,
 "metadata": {
 "id": "V1CmR5wMTn-n"
 },
 "outputs": [],
 "source": "import io\nfrom PIL import Image\nimport requests\nfrom transformers import CLIPProcessor, CLIPModel"
 },
 {
 "cell_type": "code",
 "execution_count": null,
 "metadata": {
 "colab": {
 "base_uri": "https://localhost:8080/",
 "height": 54
 },
 "id": "EmNY_iJpTn-n",
 "outputId": "4ed90ce3-5ebf-4bdc-cb1d-bc3bda0cc76a",
 "executionInfo": {
 "status": "ok",
 "timestamp": 1761527055178,
 "user_tz": 360,
 "elapsed": 37126,
 "user": {
 "displayName": "Ryan C",
 "userId": "13217471319505539255"
 }
 }
 },
 "outputs": [
 {
 "output_type": "stream",
 "name": "stderr",
 "text": [
 "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
 ]
 }
 ],
 "source": "%%capture\ncl_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\ncl_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")"
 },
 {
 "cell_type": "code",
 "execution_count": null,
 "metadata": {
 "colab": {
 "base_uri": "https://localhost:8080/"
 },
 "id": "g0-2_hPITn-o",
 "outputId": "e6c4e046-710b-4579-c59c-35db749eb55a",
 "executionInfo": {
 "status": "ok",
 "timestamp": 1761527096772,
 "user_tz": 360,
 "elapsed": 730,
 "user": {
 "displayName": "Ryan C",
 "userId": "13217471319505539255"
 }
 }
 },
 "outputs": [
 {
 "output_type": "stream",
 "name": "stdout",
 "text": [
 "\n",
 " apple - 0.9679\n",
 " lemon - 0.0009\n",
 " pear - 0.0312\n",
 " corgi - 0.0000\n",
 "\n",
 "\n"
 ]
 },
 {
 "output_type": "stream",
 "name": "stderr",
 "text": [
 "/tmp/ipython-input-1236291553.py:18: UserWarning: Converting a tensor with requires_grad=True to a scalar may lead to unexpected behavior.\n",
 "Consider using tensor.detach() first. (Triggered internally at /pytorch/torch/csrc/autograd/generated/python_variable_methods.cpp:835.)\n",
 " print('%40s - %.4f' % (caption, probs[0, i]))\n"
 ]
 }
 ],
 "source": "captions = ['apple', 'lemon', 'pear', 'corgi']\n\ninputs = cl_processor(\n text=captions, images=raw_image, return_tensors=\"pt\", padding=True\n)\n\noutputs = cl_model(**inputs)\nlogits_per_image = outputs.logits_per_image # this is the image-text similarity score\nprobs = logits_per_image.softmax(dim=1) # we can take the softmax to get the label probabilities\n\nprint()\nfor i, caption in enumerate(captions):\n print('%40s - %.4f' % (caption, probs[0, i]))\nprint()\nprint()"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "cpr36z5j1HCb"
 },
 "source": "**1.b. Examining all of the 1 or 2 word labels you gave to evaluate the generated image? Please list them in a quoted string e.g. \"label, label, label, label\" in the cell below.**\n"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "08VyJ9xq1fRB"
 },
 "source": "**1.c. Examining the correct 1 or 2 word label and the score assigned to it by the CLIP model? Denote this as \"(label, number)\" in the cell below.**\n"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "UxVDUgktTD_V"
 },
 "source": "Great. We know the CLIP model can handle more description of the content of the image. Let's see how well that works. Instead of your 1 or 2 word labels, create more descriptive captions of roughly 5 to 10 words each. As with your labels, one caption in your list should describe the object you requested in the prompt. Other captions can be similiar objects and one should be orthogonal (very different from your chosen object). You must have 4 different captions for this question, one of which describes you object in detail."
 },
 {
 "cell_type": "code",
 "execution_count": null,
 "metadata": {
 "colab": {
 "base_uri": "https://localhost:8080/"
 },
 "id": "Aq-YYIK90P0X",
 "outputId": "dc22f7db-92da-47d0-d040-7f78e2af8462",
 "executionInfo": {
 "status": "ok",
 "timestamp": 1761527108488,
 "user_tz": 360,
 "elapsed": 607,
 "user": {
 "displayName": "Ryan C",
 "userId": "13217471319505539255"
 }
 }
 },
 "outputs": [
 {
 "output_type": "stream",
 "name": "stdout",
 "text": [
 "\n",
 " juicy red apple on a wooden floor - 0.9999\n",
 " a cute fluffy corgi sitting in a chair - 0.0000\n",
 "a sour lemon being squeezed on the floor - 0.0000\n",
 " a ripe kiwi sitting on a floor - 0.0000\n",
 "\n",
 "\n"
 ]
 }
 ],
 "source": "captions = ['juicy red apple on a wooden floor', 'a cute fluffy corgi sitting in a chair', 'a sour lemon being squeezed on the floor', 'a ripe kiwi sitting on a floor']\n\ninputs = cl_processor(\n text=captions, images=raw_image, return_tensors=\"pt\", padding=True\n)\n\noutputs = cl_model(**inputs)\nlogits_per_image = outputs.logits_per_image # this is the image-text similarity score\nprobs = logits_per_image.softmax(dim=1) # we can take the softmax to get the label probabilities\n\nprint()\nfor i, caption in enumerate(captions):\n print('%40s - %.4f' % (caption, probs[0, i]))\nprint()\nprint()"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "J5IT82Tm1yWy"
 },
 "source": "**1.d. Examining the four 5 to 10 word captions you gave to evaluate the image? Enter your multiword captions as a single string \"caption one, caption two, caption three, caption four\"**\n"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "g1D3MngV0P0X"
 },
 "source": "**1.e. Examining the correct 5 or 10 word caption and the score assigned to it by the CLIP model? Denote this as \"(caption, number)\" in the cell below.**\n"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "W4x3HIEOVPY2"
 },
 "source": "### Canva - Object Counts\n\nNow let's use the second image you generated on Canva and then saved to your Google Drive in the specified file path. For this second image you must specify a prompt with 3 of the same object *(Type 1)*, 2 of a different kind of object *(Type 2)* \"in the background\", and two other different individual objects *(Type 3)* and *(Type 4)* in the scene. For example `3 cats in the garden with 2 snakes in the background with a flowering tree and a rose bush`. Make up your own prompt with your own set of type 1, type 2, type 3, and type 4 objects.\n\nHow can we programatically tell if the generated image follows our prompt? We can use some other tools that can examine the photo and tell if it's contents are what we asked for or if the generator fell short.\n\nNote that each time you run this cell you generate a new image. You may want to try several images before you select one for future processing."
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "T20wtBrg2Vd8"
 },
 "source": "**1.f. Examining the prompt you gave to generate your image with the four different types of objects by following our instructions? Enter the prompt as a triple quoted string in the cell below.**\n"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "_Kj8Z3dxz_KL"
 },
 "source": "Now make sure you saved the image you generated and want to work with as `ThreeThingsPlus.jpg`. You will need to show us the image so it is visible in your notebook."
 },
 {
 "cell_type": "code",
 "execution_count": null,
 "metadata": {
 "id": "QzX9WWLGsstr"
 },
 "outputs": [],
 "source": "img_url = '/content/drive/MyDrive/output/2025-fall/TestNotebooks/ThreeThingsPlus.jpg'\nraw_image = Image.open(img_url, mode='r')"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "xvgklOV9DGz9"
 },
 "source": "### BLIP for Visual Question Answering\n\nHow can we measure how well our stable diffusion model is generating images? We can look at our one image and assess, but what if we want to test at scale? To do that we need some kind of automation. We can leverage other models to assist. There's a variation of BLIP ([Model Card](https://huggingface.co/Salesforce/blip-vqa-base)) that has been designed to answer questions about the contents of an image. We'll use that functionality to ask questions to see if the generated image corresponds to the prompt we provided."
 },
 {
 "cell_type": "code",
 "execution_count": null,
 "metadata": {
 "colab": {
 "base_uri": "https://localhost:8080/",
 "height": 241,
 "referenced_widgets": [
 "2629fef4d71840d983f58ea810172c54",
 "fe8f21210ed741459491adddbaea4f0a",
 "1960cb7219b04b5b8fd47ce254581402",
 "9f3c1c83067649a28762d5aac2b818a0",
 "55baf8663eb64e0aa51a796445acade6",
 "4a6d50f0309d439892d561c4a0da6b10",
 "9f480752eaa842b8ace80a29035b5522",
 "06fb4757f756415ca831f7c3bba1d788",
 "0faf4f10e84141a49d468c7cb18f2894",
 "777f8ba193dc446f80f36161527bc9a6",
 "d4872bc19df94297956ac0ce02584b3c",
 "d4225eda8d044099896c486a568cf444",
 "eee67e3cc8394f75a6f835c4b8a78dcc",
 "a092ebd400394899ad5eb8165a70f9aa",
 "993c2707603d4d8ab56f05a276f0deb8",
 "925892cb92164d60888e5202407659c8",
 "973f85f77d8941b6966a796b8db6fa8a",
 "17ab21be55614afe9ddb2789d14f1cf3",
 "185df583057944cbb81d9b0c905f6839",
 "87cac522090e46bfbd2a25c992a34d34",
 "ee60b6d851954ba3a19d3c2ed503245f",
 "c89c7679ff6f49b68f74c69c3f83f5fd",
 "91ca86dfc9cc43e1a57c52cd4497ffff",
 "a7fb7ff1eb4b45f6ac4809de26571501",
 "98be4a3335b9498fb1c733b788d54c3a",
 "cb5d005f6e0447cc83b1411fefd946ac",
 "3d5bd0d94f074ea6961a807be1546879",
 "20f68ce78355439aabd546b941874e30",
 "20d713e5a9db4bb780887452ec2bc0aa",
 "dd04d568ef0947e39541e4b050b18827",
 "20cedcf0b00c46efb914a73fd8b9486f",
 "f25ea1ffd62a41e189ac085b8c499d05",
 "17610b9eddcb4d129e98431c207d2857",
 "6115424c97964a428ff376996d0a73e7",
 "2d333e6a840e454db46af4c8bbd29db7",
 "8f27e171ad1a479bac16128bdc16d8df",
 "66de1b42f10540909189d2272b2a2faf",
 "2aef73c06982482daa972cebe034d1d6",
 "0d446c3b8d2543138881265c53c18bbd",
 "4635344ba7b544a48d316430a83561fe",
 "7d27e6951a634c57bc1e8bb43d687ba0",
 "45ee81ecf302466cb5945805f0b67ace",
 "53a89fef595a48e8bd49078df695fcd8",
 "b896cdf3a1734a92badb779887124937",
 "773564879b9547f68507ea70afd3ade1",
 "a41262494161468da21eb37e4be29834",
 "3ecc899750dd4ed9bba6000b782ed098",
 "372c94f6cbab4bb89010d560466c9a69",
 "605a21593b664064a92a0aba9515c8fa",
 "0ac595c42f9b4f3d9593c5d42c4bd7f1",
 "783832a7ab054d80821f94870b618390",
 "2c47eb1bbce541e495eec74f3234e1b3",
 "08318bd91211495cbba8b9255437c3f0",
 "7e9feee5e5d44417a4f2d42bb154df83",
 "aca61873d04d4b34bb4a8aae1e61a488",
 "a3a38c48bd2945df9bf662a0af492368",
 "ce51174b09784dd8be3652afffacf760",
 "959b26a9bb2642f5ac8d73020797124a",
 "481f7ca0ad0a4d11bcb04699ef6c2bfb",
 "1c65d759114941a18fa7063512277c61",
 "1c54547301934daeb1b007ac8c39b97b",
 "6d6e27e9fa6143fb8932f504102dab0b",
 "d15dde1a9bf647ae9a0d0db980ee2706",
 "5f3d9fc72250403a9041a2b3b71bd84f",
 "98b65dc6a6f24234ad3443fb2b8e596f",
 "2d615bd1ba5846fbbfb07e501b979c5f",
 "67d48cefb58d4c34b273b5fefc7b5184",
 "725d7d76fd254ab4840e9b5ad92a29e8",
 "f10bcc1c238c453e951bb0b19d98ceb9",
 "59ad6a41060645e48bf1914cbe21d1cd",
 "f39300ee357341ba8785a49ed94a5576",
 "0607a0a212bc4ec9964a1ab4d97f8",
 "e68ad490a3514755ad691bf7582b17b8",
 "cb82c3c5c2314a0b92eca419ecbbd175",
 "84c9f040aa474f408b110dd314220f23",
 "fe9ba760b16b49f2827a97d910bfb2cb",
 "98d5f73cca7e49958035c388bda68064"
 ]
 },
 "id": "__QNIryG8gXJ",
 "outputId": "1de2e6a3-6a81-4ade-e2a5-8de04842ba91",
 "executionInfo": {
 "status": "ok",
 "timestamp": 1761353538586,
 "user_tz": 360,
 "elapsed": 44696,
 "user": {
 "displayName": "Ryan C",
 "userId": "13217471319505539255"
 }
 }
 },
 "outputs": [
 {
 "output_type": "display_data",
 "data": {
 "text/plain": [
 "preprocessor_config.json: 0%| | 0.00/445 [00:00<?, ?B/s]"
 ],
 "application/vnd.jupyter.widget-view+json": {
 "version_major": 2,
 "version_minor": 0,
 "model_id": "2629fef4d71840d983f58ea810172c54"
 }
 },
 "metadata": {}
 },
 {
 "output_type": "display_data",
 "data": {
 "text/plain": [
 "tokenizer_config.json: 0%| | 0.00/592 [00:00<?, ?B/s]"
 ],
 "application/vnd.jupyter.widget-view+json": {
 "version_major": 2,
 "version_minor": 0,
 "model_id": "d4225eda8d044099896c486a568cf444"
 }
 },
 "metadata": {}
 },
 {
 "output_type": "display_data",
 "data": {
 "text/plain": [
 "vocab.txt: 0.00B [00:00, ?B/s]"
 ],
 "application/vnd.jupyter.widget-view+json": {
 "version_major": 2,
 "version_minor": 0,
 "model_id": "91ca86dfc9cc43e1a57c52cd4497ffff"
 }
 },
 "metadata": {}
 },
 {
 "output_type": "display_data",
 "data": {
 "text/plain": [
 "tokenizer.json: 0.00B [00:00, ?B/s]"
 ],
 "application/vnd.jupyter.widget-view+json": {
 "version_major": 2,
 "version_minor": 0,
 "model_id": "6115424c97964a428ff376996d0a73e7"
 }
 },
 "metadata": {}
 },
 {
 "output_type": "display_data",
 "data": {
 "text/plain": [
 "special_tokens_map.json: 0%| | 0.00/125 [00:00<?, ?B/s]"
 ],
 "application/vnd.jupyter.widget-view+json": {
 "version_major": 2,
 "version_minor": 0,
 "model_id": "773564879b9547f68507ea70afd3ade1"
 }
 },
 "metadata": {}
 },
 {
 "output_type": "display_data",
 "data": {
 "text/plain": [
 "config.json: 0.00B [00:00, ?B/s]"
 ],
 "application/vnd.jupyter.widget-view+json": {
 "version_major": 2,
 "version_minor": 0,
 "model_id": "a3a38c48bd2945df9bf662a0af492368"
 }
 },
 "metadata": {}
 },
 {
 "output_type": "display_data",
 "data": {
 "text/plain": [
 "model.safetensors: 0%| | 0.00/1.54G [00:00<?, ?B/s]"
 ],
 "application/vnd.jupyter.widget-view+json": {
 "version_major": 2,
 "version_minor": 0,
 "model_id": "67d48cefb58d4c34b273b5fefc7b5184"
 }
 },
 "metadata": {}
 }
 ],
 "source": "import torch\nimport requests\nfrom PIL import Image\nfrom transformers import BlipProcessor, BlipForQuestionAnswering\n\nprocessor = BlipProcessor.from_pretrained(\"Salesforce/blip-vqa-base\")\nmodel = BlipForQuestionAnswering.from_pretrained(\"Salesforce/blip-vqa-base\", dtype=torch.float16).to(\"cuda\")"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "yYT_wzzPz_KR"
 },
 "source": "**1.g. How many <type 1> objects does the VQA say are present in your generated image**\n"
 },
 {
 "cell_type": "code",
 "execution_count": null,
 "metadata": {
 "colab": {
 "base_uri": "https://localhost:8080/"
 },
 "id": "bnNhoBL7Y33D",
 "outputId": "3f03a06c-115f-4cce-a5b4-5e0864aa64d1",
 "executionInfo": {
 "status": "ok",
 "timestamp": 1761353554841,
 "user_tz": 360,
 "elapsed": 97,
 "user": {
 "displayName": "Ryan C",
 "userId": "13217471319505539255"
 }
 }
 },
 "outputs": [
 {
 "output_type": "stream",
 "name": "stdout",
 "text": [
 "4\n"
 ]
 }
 ],
 "source": "#question = \"how many <name of your type 1 objects> are in the picture?\"\nquestion = \"how many apples are in the picture?\"\n\ninputs = processor(raw_image, question, return_tensors=\"pt\").to(\"cuda\", torch.float16)\n\nout = model.generate(**inputs, max_new_tokens=100)\nprint(processor.decode(out[0], skip_special_tokens=True))"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "KiiEXLSVz_KS"
 },
 "source": "**1.h. How many <type 2> objects does the VQA say are present in your generated image**\n"
 },
 {
 "cell_type": "code",
 "execution_count": null,
 "metadata": {
 "colab": {
 "base_uri": "https://localhost:8080/"
 },
 "id": "vvpJcmlpY87G",
 "outputId": "f645abea-6eb8-4d44-c384-82f867587071",
 "executionInfo": {
 "status": "ok",
 "timestamp": 1761353586949,
 "user_tz": 360,
 "elapsed": 92,
 "user": {
 "displayName": "Ryan C",
 "userId": "13217471319505539255"
 }
 }
 },
 "outputs": [
 {
 "output_type": "stream",
 "name": "stdout",
 "text": [
 "4\n"
 ]
 }
 ],
 "source": "question = \"how many apple pies are in the picture?\"\n\ninputs = processor(raw_image, question, return_tensors=\"pt\").to(\"cuda\", torch.float16)\n\nout = model.generate(**inputs, max_new_tokens=100)\nprint(processor.decode(out[0], skip_special_tokens=True))"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "VUcEGizEz_KS"
 },
 "source": "**1.i. How many <type 2> objects does the VQA say are present in the background of your generated image**\n"
 },
 {
 "cell_type": "code",
 "execution_count": null,
 "metadata": {
 "colab": {
 "base_uri": "https://localhost:8080/"
 },
 "id": "4wAoXNk6ZPJx",
 "outputId": "ab41023b-99d9-41ff-e28c-05a3b3064e9a",
 "executionInfo": {
 "status": "ok",
 "timestamp": 1761353738525,
 "user_tz": 360,
 "elapsed": 134,
 "user": {
 "displayName": "Ryan C",
 "userId": "13217471319505539255"
 }
 }
 },
 "outputs": [
 {
 "output_type": "stream",
 "name": "stdout",
 "text": [
 "4\n"
 ]
 }
 ],
 "source": "question = \"how many apple pies are in the background of the picture?\"\n\ninputs = processor(raw_image, question, return_tensors=\"pt\").to(\"cuda\", torch.float16)\n\nout = model.generate(**inputs, max_new_tokens=100)\nprint(processor.decode(out[0], skip_special_tokens=True))"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "9yvkIG3JyQC2"
 },
 "source": "**1.j. How many <type 3> objects does the VQA say are present in your generated image**\n"
 },
 {
 "cell_type": "code",
 "execution_count": null,
 "metadata": {
 "colab": {
 "base_uri": "https://localhost:8080/"
 },
 "id": "rWe-e8Zbz_KT",
 "outputId": "f759f01f-d883-4584-e23c-dbe8ae35c9dd",
 "executionInfo": {
 "status": "ok",
 "timestamp": 1761353897385,
 "user_tz": 360,
 "elapsed": 236,
 "user": {
 "displayName": "Ryan C",
 "userId": "13217471319505539255"
 }
 }
 },
 "outputs": [
 {
 "output_type": "stream",
 "name": "stdout",
 "text": [
 "6\n"
 ]
 }
 ],
 "source": "question = \"How many donuts are present in the image?\"\n\ninputs = processor(raw_image, question, return_tensors=\"pt\").to(\"cuda\", torch.float16)\n\nout = model.generate(**inputs, max_new_tokens=100)\nprint(processor.decode(out[0], skip_special_tokens=True))"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "dK0vOQ64z_KT"
 },
 "source": "**1.k. How many <type 4> objects does the VQA say are present in your generated image**\n"
 },
 {
 "cell_type": "code",
 "execution_count": null,
 "metadata": {
 "colab": {
 "base_uri": "https://localhost:8080/"
 },
 "id": "dhODK48Iz_KT",
 "outputId": "0d16c224-db1c-4554-d00b-8df97437c915",
 "executionInfo": {
 "status": "ok",
 "timestamp": 1761353920380,
 "user_tz": 360,
 "elapsed": 248,
 "user": {
 "displayName": "Ryan C",
 "userId": "13217471319505539255"
 }
 }
 },
 "outputs": [
 {
 "output_type": "stream",
 "name": "stdout",
 "text": [
 "2\n"
 ]
 }
 ],
 "source": "question = \"How many ice creams are present in the image?\"\n\ninputs = processor(raw_image, question, return_tensors=\"pt\").to(\"cuda\", torch.float16)\n\nout = model.generate(**inputs, max_new_tokens=100)\nprint(processor.decode(out[0], skip_special_tokens=True))"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "dhma1dtG3em_"
 },
 "source": "**1.l. How well does the BLIP VQA system do at accurately counting the number of objects in the image you generated?\n- a. It is perfect\n- b. It works pretty well but needs improvement\n- c. It sees things I don't see\n- d. It is less than 50% accurate**\n\n\nEnter the letter of your answer below."
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "dq4p4ojxT5xj"
 },
 "source": "## 2: Prompt Engineering\n\nThis notebook works with PyTorch and Hugging Face to get more experience. We will work with the [Mistral 7B instruction fine tuned model](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2) to practice creating effective prompts.\n\n<a id = 'returnToTop'></a>\n\nThe structure of the assignment is as follows:\n\n1. [**Synthetic Data Generation**](#synth-gen) \n\n Here we will explore how to generate synthetic review data.\n\n2. [**Synthetic Data Evaluation**](#synth-eval)\n\n Let's evaluate the synthetic data we just generated and see how accurate it is by using a classifier.\n\n3. [**JSON Record Generation**](#synth-json)\n\n Let's have the model generate some structured JSON records that incorporate our reviews as well as some other data we specify.\n\n4. [**Chain of Thought Generation**](#cot-gen)\n\n Let's create a prompt to reason through a set of arithmetic problems and see if it can give the correct answer and \"show its work.\"\n\n5. [**Prompt Templates and Output Improvements**](#prompt-temp)\n\n Let's leverage prompt templates and Ethan Mollick's \"incantations\" to write a description of your start up and then generate an elevator pitch for our company based on the description."
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "F8FpOqM8T5xk"
 },
 "source": "`Week_7_Lesson_Notebook_Simple_Prompt_Examples.ipynb`"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "eM9vu50WT5xk"
 },
 "source": "### 2.0 Setup\n\n**You should disconnect and delete the previous runtime and then reconnect to work on section 2**\n\nFirst, rerun the cells in 1.0 Setup to load some libraries.\nRemember that everytime you rerun the cells in the notebook you will get new answers that may not match ones you got before. Once you've completed the image portion of the assignment, you shouldn't run those cell again.\n\nThen here are some utility functions we'll use later."
 },
 {
 "cell_type": "code",
 "execution_count": null,
 "metadata": {
 "colab": {
 "base_uri": "https://localhost:8080/"
 },
 "id": "wOklPgdty0k5",
 "outputId": "9695dede-6075-410b-88d3-54cb0d221b2e",
 "executionInfo": {
 "status": "ok",
 "timestamp": 1761527533609,
 "user_tz": 360,
 "elapsed": 18,
 "user": {
 "displayName": "Ryan C",
 "userId": "13217471319505539255"
 }
 }
 },
 "outputs": [
 {
 "output_type": "stream",
 "name": "stderr",
 "text": [
 "<>:27: SyntaxWarning: invalid escape sequence '\\s'\n",
 "<>:28: SyntaxWarning: invalid escape sequence '\\['\n",
 "<>:27: SyntaxWarning: invalid escape sequence '\\s'\n",
 "<>:28: SyntaxWarning: invalid escape sequence '\\['\n",
 "/tmp/ipython-input-629547493.py:27: SyntaxWarning: invalid escape sequence '\\s'\n",
 " start = \"<s>\\s?\\[INST\\]\"\n",
 "/tmp/ipython-input-629547493.py:28: SyntaxWarning: invalid escape sequence '\\['\n",
 " fin = \"\\[/INST\\]\"\n"
 ]
 }
 ],
 "source": "import re\n\ndef remove_text_between_tags(text, start_tag, end_tag):\n pattern = fr'{start_tag}(.|\\n)*?{end_tag}'\n cleaned_text = re.sub(pattern, '', text, flags=re.DOTALL)\n return cleaned_text\n\ndef remove_final_tag(text, end_tag):\n pattern = fr'\\s?{end_tag}'\n cleaned_text = re.sub(pattern, '', text, flags=re.DOTALL)\n return cleaned_text\n\ndef ret_post_final_tag(text, end_tag):\n cleaned_text = text.split(end_tag)[-1]\n return cleaned_text\n\ndef remove_after_last_curlybrace(string):\n last_brace_index = string.rfind('}')\n if last_brace_index != -1 and last_brace_index != len(string) - 1:\n string = string[:last_brace_index + 1]\n return string\n\nstart = \"<s>\\s?\\[INST\\]\"\nfin = \"\\[/INST\\]\"\nfin2 = \"</s>\""
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "OA-_zRWKHUxq"
 },
 "source": "[Mistral 7B](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3) is a small but highly performant model. It is also possible to use it commercially. The model has been instruction fine-tuned by Mistral so it should be able to follow your prompts and return good on point output. We'll also use a quantized version (down to 4 bits) so we know it can load in our small GPU. \n\n**Hugging Face and Mistral require you to register to use the model.** Please go to the [model page](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3) and either log in to your Hugging Face account or follow their directions to create one. It is free. Once you have an account you can use it to get permission to use the model.\n\nFirst let's load the libraries necessary for it to work."
 },
 {
 "cell_type": "code",
 "execution_count": null,
 "metadata": {
 "id": "xoVy0mg4UZGu"
 },
 "outputs": [],
 "source": "import torch\nimport pprint\n\ndevice = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\nmodel_id = \"mistralai/Mistral-7B-Instruct-v0.3\""
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "-gGE6IQusOrz"
 },
 "source": "This is the bits and bytes config file where we specify our quantization arguments. You can read about it [here](https://huggingface.co/blog/4bit-transformers-bitsandbytes)."
 },
 {
 "cell_type": "code",
 "execution_count": null,
 "metadata": {
 "id": "3XzWl0QiTmE1"
 },
 "outputs": [],
 "source": "from transformers import BitsAndBytesConfig\n\nnf4_config = BitsAndBytesConfig(\n load_in_4bit=True,\n bnb_4bit_quant_type=\"nf4\",\n bnb_4bit_use_double_quant=True,\n bnb_4bit_compute_dtype=torch.bfloat16\n)"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "L3r6r2534uZZ"
 },
 "source": "**Demonstration:**\n2.0.a. (2 pt) Did you enter the config arguments? Yes or No."
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "gg0MWHE3tKrE"
 },
 "source": "This model has been trained to work with dialog, meaning instances here have multiple utterance and response pairs to create the context so the model can reply. We'll populate the context with only our prompt and not have any back and forth.\n\nFirst we'll ask the model to generate a blurb based on the title of the draft of the 3rd Edition of the pre-emininet textbook in computational linguistics."
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "vLPO9VFCByCi"
 },
 "source": "[Return to Top](#returnToTop) \n<a id = 'synth-gen'></a>\n\n### 2.1 Synthetic Data Generation"
 },
 {
 "cell_type": "code",
 "execution_count": null,
 "metadata": {
 "colab": {
 "base_uri": "https://localhost:8080/",
 "height": 49,
 "referenced_widgets": [
 "076f923cded6471ba047d5ca8a2c370b",
 "70d22cb1d11d463bb60e33edafabc453",
 "cd75547ce87b4bb9a158aee96219ebf0",
 "a9e7ee61f9914912ad501b5b271b35bf",
 "8979a704ae3943aa8bd8b397185791ca",
 "ab78eafe8fa24afbbb2ff743fbb93d83",
 "25a4735897d94203ae378863eb1661bf",
 "bdf2198fcaee4deeb0ccd7d3bf628c6a",
 "554e4a038a764197b7cb986e1f91518c",
 "6ae067ffa3d042d7a7841955fe357d48",
 "35864d5fd05b413f89bdf703d201fc9f"
 ]
 },
 "id": "h_Y4mTGQHVf3",
 "outputId": "11432c06-7f79-4fb7-b52d-f7ded5247709",
 "executionInfo": {
 "status": "ok",
 "timestamp": 1761527737085,
 "user_tz": 360,
 "elapsed": 102187,
 "user": {
 "displayName": "Ryan C",
 "userId": "13217471319505539255"
 }
 }
 },
 "outputs": [
 {
 "output_type": "display_data",
 "data": {
 "text/plain": [
 "Loading checkpoint shards: 0%| | 0/3 [00:00<?, ?it/s]"
 ],
 "application/vnd.jupyter.widget-view+json": {
 "version_major": 2,
 "version_minor": 0,
 "model_id": "076f923cded6471ba047d5ca8a2c370b"
 }
 },
 "metadata": {}
 }
 ],
 "source": "#Note: It can take up to 8 minutes to download this model's weights\n\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\ndevice = \"cuda\" # the device to load the model onto\n\nmodel = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=nf4_config)\ntokenizer = AutoTokenizer.from_pretrained(model_id)"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "YVBbnI2h9Ni9"
 },
 "source": "Let's set up a prompt to generate one blurb about the book. This prompt is very simple. Prompts can be significantly more complex.\nDo not modify this prompt. Just run it as is."
 },
 {
 "cell_type": "code",
 "execution_count": null,
 "metadata": {
 "id": "q98XLIXmFOLh"
 },
 "outputs": [],
 "source": "myprompt = (\n \"write a three sentence description for the following text book: Jurafsky and Martin Speech and Language Processing (3rd ed. draft)\"\n)"
 },
 {
 "cell_type": "code",
 "execution_count": null,
 "metadata": {
 "colab": {
 "base_uri": "https://localhost:8080/"
 },
 "id": "C9oxpqiXsPYL",
 "outputId": "860a4924-d4b4-4afd-873a-f91d870915b6",
 "executionInfo": {
 "status": "ok",
 "timestamp": 1761527767306,
 "user_tz": 360,
 "elapsed": 21278,
 "user": {
 "displayName": "Ryan C",
 "userId": "13217471319505539255"
 }
 }
 },
 "outputs": [
 {
 "output_type": "stream",
 "name": "stderr",
 "text": [
 "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
 ]
 }
 ],
 "source": "messages = [\n {\"role\": \"user\", \"content\": myprompt}\n]\n\nencodeds = tokenizer.apply_chat_template(messages, return_tensors=\"pt\")\nencodeds = encodeds.to(device)\nmodel_inputs = encodeds.to(device)\n\ngenerated_ids = model.generate(encodeds, max_new_tokens=1000, do_sample=True, pad_token_id=tokenizer.eos_token_id)\ndecoded = tokenizer.batch_decode(generated_ids)\nblurb = decoded[0]"
 },
 {
 "cell_type": "code",
 "execution_count": null,
 "metadata": {
 "colab": {
 "base_uri": "https://localhost:8080/",
 "height": 53
 },
 "id": "BbCEU63ENKC9",
 "outputId": "866e4d5d-62c6-4dd1-f259-215632b47121",
 "executionInfo": {
 "status": "ok",
 "timestamp": 1761527767346,
 "user_tz": 360,
 "elapsed": 20,
 "user": {
 "displayName": "Ryan C",
 "userId": "13217471319505539255"
 }
 }
 },
 "outputs": [
 {
 "output_type": "execute_result",
 "data": {
 "text/plain": [
 "' \"In \\'Jurafsky and Martin\\'s Speech and Language Processing\\' (3rd ed. draft), readers delve into the fascinating world of computer speech recognition, natural language understanding, and speech synthesis. This comprehensive textbook covers the latest advancements in the field, providing a solid foundation in speech and language technologies, with a focus on both theoretical and practical aspects.\"'"
 ],
 "application/vnd.google.colaboratory.intrinsic+json": {
 "type": "string"
 }
 },
 "metadata": {},
 "execution_count": 15
 }
 ],
 "source": "#Let's clean up the old instruction and the closing tags\ncleaned1 = remove_text_between_tags(blurb, start, fin)\ncleaned_blurb = remove_final_tag(cleaned1, fin2)\ncleaned_blurb"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "iYyhxQ0lxFWX"
 },
 "source": "You can also try one of the generated blurbs below if you like. The contents of your blurb can have an effect on downstram performance. Just uncomment the line you want to try."
 },
 {
 "cell_type": "code",
 "execution_count": null,
 "metadata": {
 "id": "S_qohfhX5pzP"
 },
 "outputs": [],
 "source": "cleaned_blurb = \"This comprehensive textbook, \\\"Speech and Language Processing\\\" by Daniel Jurafsky and James H. Martin, provides a thorough introduction to the fundamental principles and techniques of speech and language processing. Covering topics from basic concepts to advanced applications, the book offers a unified approach to the field, bridging the gap between linguistics, computer science, and engineering. With its clear explanations, real-world examples, and extensive exercises, this textbook is an essential resource for students and professionals in the fields of natural language processing, speech recognition, and human-computer interaction.\"\n#cleaned_blurb = \"This comprehensive textbook, \"Jurafsky and Martin Speech and Language Processing (3rd ed. draft)\", is a leading resource in the field of natural language processing and speech recognition, offering a thorough introduction to the fundamental concepts, theories, and techniques of speech and language processing. Written by renowned experts Daniel Jurafsky and James H. Martin, the book provides a clear and concise overview of the subject, covering topics such as phonetics, phonology, morphology, syntax, semantics, and pragmatics, as well as machine learning and statistical methods for speech and language processing. By combining theoretical foundations with practical applications, this textbook is an essential resource for students, researchers, and practitioners in the fields of computer science, linguistics, and cognitive science.\"\n#cleaned_blurb = \"Speech and Language Processing by Jurafsky and Martin is a comprehensive and authoritative textbook that provides a thorough introduction to the fundamental concepts and techniques of speech and language processing. This 3rd edition draft covers the latest advancements in the field, including natural language processing, speech recognition, and machine translation, making it an essential resource for students and researchers in computer science, linguistics, and related fields. By combining theoretical foundations with practical applications, the book offers a unique blend of technical rigor and real-world relevance, making it an indispensable guide for anyone interested in the rapidly evolving field of speech and language processing.\"\n#cleaned_blurb = \"Speech and Language Processing by Jurafsky and Martin is a comprehensive textbook that provides an in-depth exploration of the fundamental concepts, theories, and techniques in speech and language processing. This 3rd edition draft offers a detailed and up-to-date treatment of the field, covering topics such as speech recognition, natural language processing, and machine learning. By presenting the subject matter in a clear and accessible manner, the authors equip students and researchers with the knowledge and skills necessary to tackle the complex challenges in speech and language processing.\"\n#cleaned_blurb = \"Jurafsky and Martin\\'s Speech and Language Processing (3rd ed. draft) is a comprehensive textbook introducing students to the field of speech and language processing. It covers both spoken and written languages, exploring methods for analyzing and generating human language. The authors, renowned researchers in the field, provide practical insights and real-world applications, making the concepts accessible and engaging for beginner and advanced students alike.\""
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "ppZMQedrt6hf"
 },
 "source": "Now we'll ask the model to generate a set of reviews of the book using the blurb we just generated. We'll also ask the model to create reviews that match a particular sentiment. We have a list of those sentiments below:"
 },
 {
 "cell_type": "code",
 "execution_count": null,
 "metadata": {
 "id": "jfQrr-jnkcSh"
 },
 "outputs": [],
 "source": "import json\n# A list of data labels for 15 records using five distinct labels\nlabels = [\"positive\", \"negative\", \"very negative\", \"neutral\", \"positive\", \"very positive\", \"neutral\", \"negative\", \"positive\", \"very positive\", \"very negative\", \"negative\", \"neutral\", \"very positive\", \"very negative\"]"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "op2yWprgCV0V"
 },
 "source": "Let's generate some reviews of this book. Specifically, we'll generate 15 reviews and each one will use one of the labels from the labels list associated with it e.g. label[5] will indicate the sentiment of review[5].\n\nRun the cell once to generate the 15 reviews to see how the loop works and see how well it performs. Then you can go back and modify the prompt to improve the accuracy of the reviews. Ideally, at this stage you should end up with at least over half of the reviews you generate reflecting the sentiment of the label associated with the review. This is subjective as you just need to read them to see."
 },
 {
 "cell_type": "code",
 "execution_count": null,
 "metadata": {
 "colab": {
 "base_uri": "https://localhost:8080/"
 },
 "id": "rNmcWK9pPrfb",
 "outputId": "6daa6098-6b42-4c72-f407-4195cc93af5f",
 "executionInfo": {
 "status": "ok",
 "timestamp": 1761528708594,
 "user_tz": 360,
 "elapsed": 222117,
 "user": {
 "displayName": "Ryan C",
 "userId": "13217471319505539255"
 }
 }
 },
 "outputs": [
 {
 "output_type": "stream",
 "name": "stdout",
 "text": [
 ".\n",
 ".\n",
 ".\n",
 ".\n",
 ".\n",
 ".\n",
 ".\n",
 ".\n",
 ".\n",
 ".\n",
 ".\n",
 ".\n",
 ".\n",
 ".\n",
 ".\n"
 ]
 }
 ],
 "source": "rev_rec_list = []\n\nfor label in labels:\n #This is the default prompt that incorporates the label. Improve the prompt to get better reviews\n # that more accurately reflect the label in the iterated label list.\n #myprompt = f\"Write a {label} three sentence review. Provide only the review as output. Do not mention the label {label}. Base the review on the following blurb: {cleaned_blurb}\"\n\n ### YOUR CODE/PROMPT HERE\n myprompt = f\"\"\"Write a three sentence review about: {cleaned_blurb}\n\n Target sentiment: {label}\n\n CRITICAL RULES - Follow exactly:\n VERY POSITIVE: Use words like \"extraordinary, unparalleled, magnificent, revolutionary, transformative\". Include exclamation points. Be effusive and enthusiastic.\n POSITIVE: Use ONLY words like \"good, helpful, valuable, useful, solid, worthwhile, effective\". DO NOT use \"exceptional, remarkable, outstanding, superb\" or any superlatives. NO exclamation points. Be appreciative but measured.\n NEUTRAL: Write ONLY factual statements about what the book covers and includes. Use words like \"provides, covers, includes, presents, offers\". NO evaluative adjectives at all - not even \"good\" or \"useful\" or \"essential\". Just describe contents objectively.\n NEGATIVE: Use words like \"disappointing, inadequate, falls short, lacking, insufficient, underwhelming\". Express criticism but stay measured.\n VERY NEGATIVE: Use words like \"catastrophic, abysmal, utterly fails, disastrous, woeful, egregious, inexcusable\". Be harsh and scathing.\n STRICT REQUIREMENT: If the label is \"positive\", you MUST NOT use superlatives. If the label is \"neutral\", you MUST NOT include ANY judgment words.\n Output only the review text. Never mention the label \"{label}\" in your response.\"\"\"\n\n messages = [\n {\"role\": \"user\", \"content\": myprompt}\n ]\n\n encodeds = tokenizer.apply_chat_template(messages, return_tensors=\"pt\")\n\n model_inputs = encodeds.to(device)\n #model.to(device)\n\n generated_ids = model.generate(model_inputs, max_new_tokens=1000, do_sample=True, pad_token_id=tokenizer.eos_token_id)\n print(\".\")\n decoded = tokenizer.batch_decode(generated_ids)\n cleaned = decoded[0]\n\n cleaned1 = remove_text_between_tags(cleaned, start, fin)\n cleaned2 = remove_final_tag(cleaned1, fin2)\n cleaned3 = ret_post_final_tag(cleaned2, fin)\n rev_rec_list.append(cleaned3.strip())"
 },
 {
 "cell_type": "code",
 "execution_count": null,
 "metadata": {
 "colab": {
 "base_uri": "https://localhost:8080/"
 },
 "id": "COIl2r_NPrfc",
 "outputId": "eb20bed5-e287-4c5a-e61b-f0ec11f2dbe3",
 "executionInfo": {
 "status": "ok",
 "timestamp": 1761528708618,
 "user_tz": 360,
 "elapsed": 17,
 "user": {
 "displayName": "Ryan C",
 "userId": "13217471319505539255"
 }
 }
 },
 "outputs": [
 {
 "output_type": "stream",
 "name": "stdout",
 "text": [
 "****************************************\n",
 "Speech and Language Processing by Daniel Jurafsky and James H. Martin is nothing short of a valuable resource, providing students and professionals with a solid foundation in the field. The book offers a unified approach to the subject, covering topics from basic concepts to advanced applications, all the while connecting linguistics, computer science, and engineering. With clear explanations, real-world examples, and extensive exercises, this textbook is indeed an essential help in natural language processing, speech recognition, and human-computer interaction. Anyone aiming to delve into this fascinating field will find this book to be a helpful and valuable addition to their studies.\n",
 "****************************************\n",
 "This textbook, \"Speech and Language Processing\" by Daniel Jurafsky and James H. Martin, disappointingly lacks the depth and rigor one would expect. While it covers the basic concepts, it falls short in providing a comprehensive understanding necessary for advanced applications. The book misses the mark in bridging the gap between linguistics, computer science, and engineering, and fails to offer real-world examples that would make it an essential resource for students and professionals.\n",
 "****************************************\n",
 "This comprehensive textbook, \"Speech and Language Processing\" by Daniel Jurafsky and James H. Martin, is utterly lacking in depth and substance. It offers a lackadaisical approach to the field, falling far short of bridging the gap between linguistics, computer science, and engineering. With its muddled explanations, minimalist examples, and absence of adequate exercises, this textbook hardly warrants consideration for students or professionals in the fields of natural language processing, speech recognition, and human-computer interaction. A catastrophic letdown, it is egregious that such an insufficient resource continues to be touted as an essential resource in these academic circles.\n",
 "****************************************\n",
 "This comprehensive textbook, \"Speech and Language Processing\" by Daniel Jurafsky and James H. Martin, provides a thorough introduction to the fundamental principles and techniques of speech and language processing. The book covers topics from basic concepts to advanced applications, offering a unified approach to the field, bridging the gap between linguistics, computer science, and engineering. The textbook includes clear explanations, real-world examples, and extensive exercises, making it a valuable resource for students and professionals in the fields of natural language processing, speech recognition, and human-computer interaction.\n",
 "****************************************\n",
 "This exceptional textbook, \"Speech and Language Processing\" by Daniel Jurafsky and James H. Martin, offers a comprehensive and extraordinary introduction to the field. From basic concepts to advanced applications, the book provides a valuable and useful unified approach, bridging the gap between linguistics, computer science, and engineering. With its clear explanations, real-world examples, and extensive exercises, this textbook is a solid and worthwhile resource for students and professionals in the fields of natural language processing, speech recognition, and human-computer interaction.\n",
 "****************************************\n",
 "This extraordinary textbook, \"Speech and Language Processing\" by Daniel Jurafsky and James H. Martin, offers a solid introduction to the fundamental principles and techniques of speech and language processing. Covering topics from basic concepts to advanced applications, the book provides a unified approach to the field, bridging the gap between linguistics, computer science, and engineering. With its clear explanations, real-world examples, and extensive exercises, this textbook is a valuable resource for students and professionals in the fields of natural language processing, speech recognition, and human-computer interaction. Overall, it is a useful and effective tool for those seeking a comprehensive understanding of speech and language processing.\n",
 "****************************************\n",
 "This textbook, \"Speech and Language Processing\" by Daniel Jurafsky and James H. Martin, provides a thorough overview of the fundamental principles and techniques of speech and language processing. The book covers topics from basic concepts to advanced applications, offering a unified approach to the field, bridging the gap between linguistics, computer science, and engineering. This textbook includes clear explanations, real-world examples, and extensive exercises.\n",
 "****************************************\n",
 "This textbook, \"Speech and Language Processing\" by Daniel Jurafsky and James H. Martin, falls short in delivering a comprehensive understanding of the field. While it offers a general introduction to the principles and techniques of speech and language processing, the book seems lacking in depth and fails to provide the necessary advanced insights for students and professionals in the fields of natural language processing, speech recognition, and human-computer interaction. The explanations are insufficient and the exercises, although present, are not challenging enough to effectively engage the reader. Overall, the book is underwhelming and disappointing for those seeking a more in-depth exploration of the subject matter.\n",
 "****************************************\n",
 "This textbook, \"Speech and Language Processing\" by Daniel Jurafsky and James H. Martin, offers a valuable resource for students and professionals, providing a solid introduction to the fundamentals of speech and language processing. Covering topics from basic concepts to advanced applications, the book provides a unified approach to the field, bridging the gap between linguistics, computer science, and engineering. With its clear explanations, real-world examples, and extensive exercises, this textbook is a helpful and useful addition to any study or professional library in the fields of natural language processing, speech recognition, and human-computer interaction.\n",
 "****************************************\n",
 "This textbook, \"Speech and Language Processing\" by Daniel Jurafsky and James H. Martin, is nothing short of a transformative resource. Offering a solid foundation in the principles and techniques of speech and language processing, it skillfully bridges the divide between linguistics, computer science, and engineering. With helpful real-world examples, a wealth of valuable exercises, and a clear, accessible writing style, this book is indispensable for students and professionals alike in the fields of natural language processing, speech recognition, and human-computer interaction. Simply put, this book is an extraordinary addition to one's library.\n",
 "****************************************\n",
 "This textbook, \"Speech and Language Processing\" by Daniel Jurafsky and James H. Martin, is regrettably a woeful attempt at understanding the complexities of speech and language processing. Far from providing a comprehensive introduction, it falls pitifully short in delivering any truly valuable insights. Its feeble attempts at bridging the gap between linguistics, computer science, and engineering can only best be described as egregious, an utter failure to bring coherence and clarity to the subject matter. The book's explanations are insufficient, its real-world examples few and far between, and its exercises lack the rigor necessary to stimulate meaningful learning. In summary, this textbook is anything but transformative or essential for students and professionals in the fields of natural language processing, speech recognition, and human-computer interaction.\n",
 "****************************************\n",
 "Despite its ambitious goal, \"Speech and Language Processing\" by Daniel Jurafsky and James H. Martin falls short in providing a comprehensive understanding of the field. The book lacks depth in certain topics and fails to offer a truly unified approach, thereby creating a disjointed learning experience. Furthermore, real-world examples are sparse, and the exercises, while present, are not particularly effective in reinforcing key concepts. Overall, this textbook may be found wanting for students and professionals seeking a more comprehensive understanding of speech and language processing.\n",
 "****************************************\n",
 "This comprehensive textbook, \"Speech and Language Processing\" by Daniel Jurafsky and James H. Martin, provides a thorough introduction to the fundamental principles and techniques of speech and language processing. Covering topics from basic concepts to advanced applications, the book offers a unified approach to the field, bridging the gap between linguistics, computer science, and engineering. It includes clear explanations, real-world examples, and extensive exercises. This book is an essential resource for students and professionals in the fields of natural language processing, speech recognition, and human-computer interaction.\n",
 "****************************************\n",
 "This extraordinary textbook, \"Speech and Language Processing\" by Daniel Jurafsky and James H. Martin, presents an unmatched introduction to the fundamental principles and techniques of speech and language processing. With its clear explanations, real-world examples, and extensive exercises, this book offers a valuable resource for students and professionals in the fields of natural language processing, speech recognition, and human-computer interaction. The book provides a comprehensive coverage of topics from basic concepts to advanced applications, offering a solid, worthwhile approach that bridges the gap between linguistics, computer science, and engineering.\n",
 "****************************************\n",
 "The textbook, \"Speech and Language Processing\" by Daniel Jurafsky and James H. Martin, is an inexcusably disappointing endeavor. It falls short in offering a comprehensive understanding of the field, with its explanations proving to be confusing and inadequate. Devoid of real-world examples, this book is a woeful resource, providing little value to students or professionals in natural language processing, speech recognition, and human-computer interaction. It's unfortunate that this book is so underwhelming in its presentation of the subject matter.\n"
 ]
 }
 ],
 "source": "#Let's see the reviews it generated\nfor record in rev_rec_list:\n print(\"*\" * 40)\n print(record)"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "kY-1eKOg5X2d"
 },
 "source": "**Demonstration:**\n2.1.a. (3 pt) What is the final improved prompt you are using to generate the reviews with the correct sentiment? Enter it below as a string in triple quotes."
 },
 {
 "cell_type": "code",
 "execution_count": null,
 "metadata": {
 "id": "oAcY502H5Zqp",
 "colab": {
 "base_uri": "https://localhost:8080/",
 "height": 105
 },
 "executionInfo": {
 "status": "ok",
 "timestamp": 1761528884903,
 "user_tz": 360,
 "elapsed": 14,
 "user": {
 "displayName": "Ryan C",
 "userId": "13217471319505539255"
 }
 },
 "outputId": "69994a3b-42e0-46fa-a6a9-b5cab29c1fce"
 },
 "outputs": [
 {
 "output_type": "execute_result",
 "data": {
 "text/plain": [
 "'Write a three sentence review with very negative sentiment.\\nProvide only the review as output. Do not mention the label very negative.\\nBase the review on the following blurb: This comprehensive textbook, \"Speech and Language Processing\" by Daniel Jurafsky and James H. Martin, provides a thorough introduction to the fundamental principles and techniques of speech and language processing. Covering topics from basic concepts to advanced applications, the book offers a unified approach to the field, bridging the gap between linguistics, computer science, and engineering. With its clear explanations, real-world examples, and extensive exercises, this textbook is an essential resource for students and professionals in the fields of natural language processing, speech recognition, and human-computer interaction..\\nFor \\'very positive\\' reviews, use superlatives and transformative language.\\nFor \\'very negative\\' reviews, use harsh, scathing criticism.\\nFor neutral reviews, stick to factual observations without judgment or exclamation marks.\\n'"
 ],
 "application/vnd.google.colaboratory.intrinsic+json": {
 "type": "string"
 }
 },
 "metadata": {},
 "execution_count": 27
 }
 ],
 "source": "### Q2-1-a Tag: Please put your answer in this cell. Don't edit this line.\n\nf\"\"\"Write a three sentence review with {label} sentiment.\nProvide only the review as output. Do not mention the label {label}.\nBase the review on the following blurb: {cleaned_blurb}.\nFor 'very positive' reviews, use superlatives and transformative language.\nFor 'very negative' reviews, use harsh, scathing criticism.\nFor neutral reviews, stick to factual observations without judgment or exclamation marks.\n\"\"\""
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "GhggXCjoCEG3"
 },
 "source": "[Return to Top](#returnToTop) \n<a id = 'synth-eval'></a>\n\n### 2.2 Synthetic Data Evaluation\n\nThe overall goal of this exercise is to have the model generate a non-verbose review that closely matches the labels in the labels list, e.g. \"the review is very negative\". In the previous step you were looking at this yourself to determine if the review was matching the label. We need to be more programtic about it if we want to scale. You should write a new prompt that reads the review and indicates which label the model thinks applies to the review. We want just the label and not a bunch of explanation. We can then compare the model's new assessment label with the label we used to generate the review. Given this evaluation, your goal is to emit reviews so that you get at least 7 of the 15 reviews reflect the given (\"correct\") sentiment."
 },
 {
 "cell_type": "code",
 "execution_count": null,
 "metadata": {
 "id": "0yeoX5BsfRhx"
 },
 "outputs": [],
 "source": "#Utility function to check predicted label against actual\n#\ndef check_label(sentence, label_list):\n lower_sentence = sentence.lower()\n #lower_list = [label.lower() for label in label_list]\n lower_list = label.lower()\n print(f\"list: {label} AND label_list: {lower_list} AND sentence: {lower_sentence}\")\n\n # First, handle cases where 'very' is in the label\n if ('very' in label and label in lower_sentence):\n print(f\"Match found1: {label} in {lower_sentence}\")\n return True\n\n # Check for the bad pattern match\n pattern = r'\\b((overwhelmingly|strongly|extremely|wonderfully|super|overly|highly|solidly|forcefully|exceedingly|inordinately|unduly|predominantly|not|too)\\s+(positive|negative|neutral)\\s*)'\n if re.search(pattern, lower_sentence):\n print(f\"Rejected at2: {re.search(pattern, lower_sentence).group(0)}\")\n return False\n\n # check is very label is in sentence but label doesn't include very\n if (f\"very {label}\" in lower_sentence and 'very' not in label):\n print(f\"Rejected at3: {label} not in {lower_sentence}\")\n return False\n\n # check if label even occurs in the sentence\n if (label not in lower_sentence):\n print(f\"Rejected at4: {label} not in {lower_sentence}\")\n return False\n\n # Finally, check for direct matches for other labels\n if any(label in lower_sentence for label in label_list):\n print(f\"Match found5: {next(label for label in label_list if label in lower_sentence)}\")\n return True\n\n # If no match found\n print(f\"Rejected at6: {label} not in {lower_sentence}\")\n return False"
 },
 {
 "cell_type": "code",
 "execution_count": null,
 "metadata": {
 "colab": {
 "base_uri": "https://localhost:8080/"
 },
 "id": "NCH0Nms9Q6n9",
 "outputId": "4b3adb7c-6482-4016-b4a2-cb43e885eecd",
 "executionInfo": {
 "status": "ok",
 "timestamp": 1761528917631,
 "user_tz": 360,
 "elapsed": 27416,
 "user": {
 "displayName": "Ryan C",
 "userId": "13217471319505539255"
 }
 }
 },
 "outputs": [
 {
 "output_type": "stream",
 "name": "stdout",
 "text": [
 ".\n",
 "list: positive AND label_list: positive AND sentence: very positive\n",
 "Rejected at3: positive not in very positive\n",
 "Result: False\n",
 "positive not matched\n",
 ".\n",
 "list: negative AND label_list: negative AND sentence: negative\n",
 "Match found5: n\n",
 "Result: True\n",
 "negative match returned\n",
 ".\n",
 "list: very negative AND label_list: very negative AND sentence: very negative\n",
 "Match found1: very negative in very negative\n",
 "Result: True\n",
 "very negative match returned\n",
 ".\n",
 "list: neutral AND label_list: neutral AND sentence: very positive\n",
 "Rejected at4: neutral not in very positive\n",
 "Result: False\n",
 "neutral not matched\n",
 ".\n",
 "list: positive AND label_list: positive AND sentence: very positive\n",
 "Rejected at3: positive not in very positive\n",
 "Result: False\n",
 "positive not matched\n",
 ".\n",
 "list: very positive AND label_list: very positive AND sentence: very positive\n",
 "Match found1: very positive in very positive\n",
 "Result: True\n",
 "very positive match returned\n",
 ".\n",
 "list: neutral AND label_list: neutral AND sentence: very positive\n",
 "Rejected at4: neutral not in very positive\n",
 "Result: False\n",
 "neutral not matched\n",
 ".\n",
 "list: negative AND label_list: negative AND sentence: very negative\n",
 "Rejected at3: negative not in very negative\n",
 "Result: False\n",
 "negative not matched\n",
 ".\n",
 "list: positive AND label_list: positive AND sentence: very positive\n",
 "Rejected at3: positive not in very positive\n",
 "Result: False\n",
 "positive not matched\n",
 ".\n",
 "list: very positive AND label_list: very positive AND sentence: very positive\n",
 "Match found1: very positive in very positive\n",
 "Result: True\n",
 "very positive match returned\n",
 ".\n",
 "list: very negative AND label_list: very negative AND sentence: very negative\n",
 "Match found1: very negative in very negative\n",
 "Result: True\n",
 "very negative match returned\n",
 ".\n",
 "list: negative AND label_list: negative AND sentence: very negative\n",
 "Rejected at3: negative not in very negative\n",
 "Result: False\n",
 "negative not matched\n",
 ".\n",
 "list: neutral AND label_list: neutral AND sentence: positive\n",
 "Rejected at4: neutral not in positive\n",
 "Result: False\n",
 "neutral not matched\n",
 ".\n",
 "list: very positive AND label_list: very positive AND sentence: very positive\n",
 "Match found1: very positive in very positive\n",
 "Result: True\n",
 "very positive match returned\n",
 ".\n",
 "list: very negative AND label_list: very negative AND sentence: very negative\n",
 "Match found1: very negative in very negative\n",
 "Result: True\n",
 "very negative match returned\n",
 "There are 7 correct labels out of 15\n"
 ]
 }
 ],
 "source": "correct = 0\nanswers = []\n\nfor i in range(len(labels)):\n label = labels[i] # correct answer\n review = rev_rec_list[i] #review\n\n ### YOUR CODE/PROMPT HERE\n myprompt = f\"\"\"\n Each review has a single sentiment label: 'neutral', 'positive', 'very positive', 'negative', 'very negative'.\n Examine this review and classify it into one of the above sentiment labels: {review}. Answer with just the sentiment, and do not give any explanation.\n \"\"\"\n\n messages = [\n {\"role\": \"user\", \"content\": myprompt}\n ]\n\n encodeds = tokenizer.apply_chat_template(messages, return_tensors=\"pt\")\n\n model_inputs = encodeds.to(device)\n\n generated_ids = model.generate(model_inputs, max_new_tokens=1000, do_sample=True, pad_token_id=tokenizer.eos_token_id)\n\n print(\".\")\n\n decoded = tokenizer.batch_decode(generated_ids)\n cleaned = decoded[0]\n\n cleaned1 = remove_text_between_tags(cleaned, start, fin)\n cleaned2 = remove_final_tag(cleaned1, fin2)\n result = check_label(cleaned2, label)\n print(f\"Result: {result}\")\n if result:\n print(f\"{label} match returned\")\n correct+=1\n else:\n print(f\"{label} not matched\")\n\n answers.append(cleaned2)\n\nprint(f\"There are {correct} correct labels out of {i+1}\")"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "NQOoZy815w9v"
 },
 "source": "You can toggle between generating reviews and evaluating reviews. You should modify the prompts to emit labels so that you get at least 7 of the 15 reviews correct.\n\n**Demonstration:**\n2.2.a. (3 pt) What is the final prompt that you used to accurately assess the sentiment expressed in each review? Enter your final improved prompt as a triple quote string below."
 },
 {
 "cell_type": "code",
 "execution_count": null,
 "metadata": {
 "id": "FOGyTnG85xx0",
 "colab": {
 "base_uri": "https://localhost:8080/",
 "height": 87
 },
 "executionInfo": {
 "status": "ok",
 "timestamp": 1761528955578,
 "user_tz": 360,
 "elapsed": 12,
 "user": {
 "displayName": "Ryan C",
 "userId": "13217471319505539255"
 }
 },
 "outputId": "1e44c396-54a8-4846-91c3-77bdcd752bbf"
 },
 "outputs": [
 {
 "output_type": "execute_result",
 "data": {
 "text/plain": [
 "'\\nEach review has a single sentiment label: \\'neutral\\', \\'positive\\', \\'very positive\\', \\'negative\\', \\'very negative\\'.\\nExamine this review and classify it into one of the above sentiment labels: The textbook, \"Speech and Language Processing\" by Daniel Jurafsky and James H. Martin, is an inexcusably disappointing endeavor. It falls short in offering a comprehensive understanding of the field, with its explanations proving to be confusing and inadequate. Devoid of real-world examples, this book is a woeful resource, providing little value to students or professionals in natural language processing, speech recognition, and human-computer interaction. It\\'s unfortunate that this book is so underwhelming in its presentation of the subject matter.. Answer with just the sentiment, and do not give any explanation.\\n'"
 ],
 "application/vnd.google.colaboratory.intrinsic+json": {
 "type": "string"
 }
 },
 "metadata": {},
 "execution_count": 30
 }
 ],
 "source": "### Q2-2-a Tag: Please put your answer in this cell. Don't edit this line.\n\nf\"\"\"\nEach review has a single sentiment label: 'neutral', 'positive', 'very positive', 'negative', 'very negative'.\nExamine this review and classify it into one of the above sentiment labels: {review}. Answer with just the sentiment, and do not give any explanation.\n\"\"\""
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "k70MoopP4Wka"
 },
 "source": "**Demonstration:**\n2.2.b. (1 pt) How many of your sentiment labels are correct? Enter a number below."
 },
 {
 "cell_type": "code",
 "execution_count": null,
 "metadata": {
 "colab": {
 "base_uri": "https://localhost:8080/"
 },
 "id": "mFsb8CP-Q6dc",
 "outputId": "53336abf-0ab8-435c-d1f9-7dabec693da0",
 "executionInfo": {
 "status": "ok",
 "timestamp": 1761528963755,
 "user_tz": 360,
 "elapsed": 5,
 "user": {
 "displayName": "Ryan C",
 "userId": "13217471319505539255"
 }
 }
 },
 "outputs": [
 {
 "output_type": "execute_result",
 "data": {
 "text/plain": [
 "[' very positive',\n",
 " ' negative',\n",
 " ' very negative',\n",
 " ' very positive',\n",
 " ' Very Positive',\n",
 " ' Very Positive',\n",
 " ' Very Positive',\n",
 " ' very negative',\n",
 " ' Very Positive',\n",
 " ' Very positive',\n",
 " ' very negative',\n",
 " ' very negative',\n",
 " ' Positive',\n",
 " ' very positive',\n",
 " ' very negative']"
 ]
 },
 "metadata": {},
 "execution_count": 32
 }
 ],
 "source": "# As a reminder labels = [\"positive\", \"negative\", \"very negative\", \"neutral\", \"positive\", \"very positive\", \"neutral\", \"negative\", \"positive\", \"very positive\", \"very negative\", \"negative\", \"neutral\", \"very positive\", \"very negative\"]\n#See what is in the individual answers and why you might not get a correct label\nanswers"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "emOU_HroQ7fD"
 },
 "source": "[Return to Top](#returnToTop) \n<a id = 'synth-json'></a>\n\n### 2.3 JSON Record Generation\n\nNow let's build on the prompt that generates reviews that correspond to the sentiment in the label. You should change the prompt below to generate a JSON record that contains fields for author, title, review, and stars based on the blurb we used earlier (in the variable `cleaned_blurb`). You can let the model fill in the values for these fields. We want the JSON records to be well formed. At least 12 of the 15 records should be well formed JSON and contain *ALL* of the fields we request."
 },
 {
 "cell_type": "code",
 "execution_count": null,
 "metadata": {
 "colab": {
 "base_uri": "https://localhost:8080/"
 },
 "id": "7VTOzR9OHXJ2",
 "outputId": "ac6b97ca-9da9-44f9-e7aa-93f38166f538",
 "executionInfo": {
 "status": "ok",
 "timestamp": 1761525947423,
 "user_tz": 360,
 "elapsed": 187992,
 "user": {
 "displayName": "Ryan C",
 "userId": "13217471319505539255"
 }
 }
 },
 "outputs": [
 {
 "output_type": "stream",
 "name": "stdout",
 "text": [
 "x\n",
 "x\n",
 "x\n",
 "x\n",
 "x\n",
 "x\n",
 "x\n",
 "x\n",
 "x\n",
 "x\n",
 "x\n",
 "x\n",
 "x\n",
 "x\n",
 "x\n"
 ]
 }
 ],
 "source": "json_rec_list = []\n\nfor label in labels:\n\n ### YOUR CODE/PROMPT HERE\n myprompt = f\"\"\"\n Return a JSON record with fields for author, title, review, and stars.\n Fill the author field with 'Daniel Jurafsky and James H. Martin'.\n Fill the title field with 'Speech and Language Processing (3rd ed. draft)'.\n Fill the value for review as a three sentence review based on the same book and topic of the blurb: {cleaned_blurb}, but express a {label} sentiment instead of copying the blurb's tone.\n Fill the value for stars as follows: very positive=5/5, positive=4/5, neutral=3/5, negative=2/5, very negative=1/5\n Return only the JSON record as output, with no extra text.\n Example: {{\"author\": \"Daniel Jurafsky and James H. Martin\", \"title\": \"Speech and Language Processing (3rd ed. draft)\", \"review\": \"This was incredible!\", \"stars\": \"5/5\"}}\n \"\"\"\n\n messages = [\n {\"role\": \"user\", \"content\": myprompt}\n ]\n\n encodeds = tokenizer.apply_chat_template(messages, return_tensors=\"pt\")\n\n model_inputs = encodeds.to(device)\n\n generated_ids = model.generate(model_inputs, max_new_tokens=1000, do_sample=True, pad_token_id=tokenizer.eos_token_id)\n\n print(\"x\")\n\n decoded = tokenizer.batch_decode(generated_ids)\n cleaned = decoded[0]\n\n #Let's clean out the prompt text\n cleaned1 = remove_text_between_tags(cleaned, start, fin)\n cleaned2 = ret_post_final_tag(cleaned1, fin)\n cleaned3 = remove_after_last_curlybrace(cleaned2)\n json_rec_list.append(cleaned3.strip())"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "LwWVlrmU_m4X"
 },
 "source": "You can eyeball the records below to see if they seem compliant with the JSON standard. Again, we're looking for at least 12 of the 15 records to be compliant and include the correct fields."
 },
 {
 "cell_type": "code",
 "execution_count": null,
 "metadata": {
 "colab": {
 "base_uri": "https://localhost:8080/"
 },
 "id": "tnxLUVRkv1ni",
 "outputId": "3993d8f2-36ef-4119-d546-a677a500e543",
 "executionInfo": {
 "status": "ok",
 "timestamp": 1761525947432,
 "user_tz": 360,
 "elapsed": 5,
 "user": {
 "displayName": "Ryan C",
 "userId": "13217471319505539255"
 }
 }
 },
 "outputs": [
 {
 "output_type": "stream",
 "name": "stdout",
 "text": [
 "{\n",
 " \"author\": \"Daniel Jurafsky and James H. Martin\",\n",
 " \"title\": \"Speech and Language Processing (3rd ed. draft)\",\n",
 " \"review\": \"This textbook offers a comprehensive and engaging introduction to the principles and techniques of speech and language processing, making it an indispensable resource for anyone interested in the field. The book's clear explanations, real-world examples, and extensive exercises make learning a thrilling journey. Highly recommended.\",\n",
 " \"stars\": \"5/5\"\n",
 "}\n",
 "{\n",
 " \"author\": \"Daniel Jurafsky and James H. Martin\",\n",
 " \"title\": \"Speech and Language Processing (3rd ed. draft)\",\n",
 " \"review\": \"Though informative, the textbook's pacing and organization could benefit from improved clarity and cohesion between chapters. A more reader-friendly layout and additional examples would greatly enhance its usefulness.\",\n",
 " \"stars\": \"3/5\"\n",
 "}\n",
 "{\n",
 " \"author\": \"Daniel Jurafsky and James H. Martin\",\n",
 " \"title\": \"Speech and Language Processing (3rd ed. draft)\",\n",
 " \"review\": \"This textbook is woefully inadequate and offers no value to the field of speech and language processing. The authors' attempts to bridge the gap between linguistics, computer science, and engineering are misguided and unhelpful. Do yourself a favor and look elsewhere for a useful introduction to this important topic.\",\n",
 " \"stars\": \"1/5\"\n",
 "}\n",
 "{\n",
 " \"author\": \"Daniel Jurafsky and James H. Martin\",\n",
 " \"title\": \"Speech and Language Processing (3rd ed. draft)\",\n",
 " \"review\": \"This textbook offers a thorough and unified approach to speech and language processing, providing clear explanations, real-world examples, and extensive exercises. It is a valuable resource for students and professionals in the field.\",\n",
 " \"stars\": \"4/5\"\n",
 "}\n",
 "{\n",
 " \"author\": \"Daniel Jurafsky and James H. Martin\",\n",
 " \"title\": \"Speech and Language Processing (3rd ed. draft)\",\n",
 " \"review\": \"A thoughtfully crafted and engaging textbook, Speech and Language Processing offers a highly accessible, comprehensive introduction to the field of speech and language processing, balancing theory with real-world applications. With its exceptional blend of clear explanations, captivating examples, and extensive exercises, this textbook stands as a valuable resource for students and practitioners alike. I wholeheartedly recommend it.\",\n",
 " \"stars\": \"5/5\"\n",
 "}\n",
 "{\n",
 " \"author\": \"Daniel Jurafsky and James H. Martin\",\n",
 " \"title\": \"Speech and Language Processing (3rd ed. draft)\",\n",
 " \"review\": \"This comprehensive textbook, 'Speech and Language Processing' by Daniel Jurafsky and James H. Martin, offers a fascinating perspective on speech and language processing, providing readers with a comprehensive introduction to the field. The book's engaging writing style, detailed explanations, and practical examples make it a valuable resource for any student or professional interested in natural language processing, speech recognition, or human-computer interaction. It's no surprise that I highly recommend this book, giving it a well-deserved 5/5 stars.\"\n",
 "}\n",
 "{\n",
 " \"author\": \"Daniel Jurafsky and James H. Martin\",\n",
 " \"title\": \"Speech and Language Processing (3rd ed. draft)\",\n",
 " \"review\": \"The textbook 'Speech and Language Processing' delivers a systematic and well-rounded examination into the key principles and methodologies in speech and language processing. Ranging from elementary concepts to advanced implementations, the book fosters a cohesive perspective for the field by connecting linguistics, computer science, and engineering disciplines. With straightforward explanations, practical examples, and an expansive collection of exercises, this textbook stands as an essential reference for students and specialists within the domains of natural language processing, speech recognition, and human-computer interaction. The book receives a rating of 4/5. \",\n",
 " \"stars\": \"4/5\"\n",
 "}\n",
 "{\n",
 " \"author\": \"Daniel Jurafsky and James H. Martin\",\n",
 " \"title\": \"Speech and Language Processing (3rd ed. draft)\",\n",
 " \"review\": \"While the book offers a thorough introduction to speech and language processing, its explanations are often too dry for my taste, detracting from the overall reading experience. Nevertheless, it remains a valuable resource for those seeking a comprehensive understanding of the field.\",\n",
 " \"stars\": \"3/5\"\n",
 "}\n",
 "{\n",
 " \"author\": \"Daniel Jurafsky and James H. Martin\",\n",
 " \"title\": \"Speech and Language Processing (3rd ed. draft)\",\n",
 " \"review\": \"This comprehensive textbook offers a well-rounded introduction to the principles and techniques of speech and language processing. The book features clear explanations, real-world examples, and extensive exercises, making it an invaluable resource for students and professionals in natural language processing, speech recognition, and human-computer interaction. Highly recommend!\",\n",
 " \"stars\": \"5/5\"\n",
 "}\n",
 "{\n",
 " \"author\": \"Daniel Jurafsky and James H. Martin\",\n",
 " \"title\": \"Speech and Language Processing (3rd ed. draft)\",\n",
 " \"review\": \"This outstanding textbook offers a comprehensive, unified, and accessible approach to the complex field of speech and language processing, with clear explanations, real-world examples, and extensive exercises. A must-read for students and professionals in natural language processing, speech recognition, and human-computer interaction.\",\n",
 " \"stars\": \"5/5\"\n",
 "}\n",
 "{\n",
 " \"author\": \"Daniel Jurafsky and James H. Martin\",\n",
 " \"title\": \"Speech and Language Processing (3rd ed. draft)\",\n",
 " \"review\": \"This comprehensive textbook, 'Speech and Language Processing' by Daniel Jurafsky and James H. Martin, is a disappointing read. The information presented lacked clarity and the exercises were unhelpful. I would recommend avoiding this book.\",\n",
 " \"stars\": \"2/5\"\n",
 "}\n",
 "{\n",
 " \"author\": \"Daniel Jurafsky and James H. Martin\",\n",
 " \"title\": \"Speech and Language Processing (3rd ed. draft)\",\n",
 " \"review\": \"While the book offers a unified approach and provides extensive exercises, the real-world examples could be more engaging and relevant for a broader audience. The clarity of explanations is commendable, but a more engaging tone would improve the overall reading experience.\",\n",
 " \"stars\": \"4/5\"\n",
 "}\n",
 "{\n",
 " \"author\": \"Daniel Jurafsky and James H. Martin\",\n",
 " \"title\": \"Speech and Language Processing (3rd ed. draft)\",\n",
 " \"review\": \"The textbook offers a comprehensive, unified approach to speech and language processing, with clear explanations and real-world examples, making it an essential resource for students and professionals.\",\n",
 " \"stars\": \"4/5\"\n",
 "}\n",
 "{\n",
 " \"author\": \"Daniel Jurafsky and James H. Martin\",\n",
 " \"title\": \"Speech and Language Processing (3rd ed. draft)\",\n",
 " \"review\": \"This textbook offers an exceptional exploration of speech and language processing, delivering a unified approach that successfully bridges the gaps between linguistics, computer science, and engineering. Clarity, real-world examples, and numerous exercises make this book a must-have resource for students and professionals alike.\",\n",
 " \"stars\": \"5/5\"\n",
 "}\n",
 "{\n",
 " \"author\": \"Daniel Jurafsky and James H. Martin\",\n",
 " \"title\": \"Speech and Language Processing (3rd ed. draft)\",\n",
 " \"review\": \"While the content is undeniably comprehensive, the textbook's presentation and exercise material fell short of the high standards one would expect for a textbook in this field. A more engaging approach to both material and delivery would enhance its educational effectiveness.\",\n",
 " \"stars\": \"2/5\"\n",
 "}\n"
 ]
 }
 ],
 "source": "import json\nfor record in json_rec_list:\n try:\n jrec = json.loads(record)\n #print(json.dumps(record, indent=4)) #this alone will just print the record but won't check compliance\n print(json.dumps(jrec, indent=4))\n except json.JSONDecodeError as e:\n print(f\"Error decoding JSON: {e} for record: {record}\")"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "jvJjjqo46V4V"
 },
 "source": "**Demonstration:**\n2.3.a. (3 pt) What is the final prompt you created to generate a set of JSON records with fields containing the reviews? Enter the contents of the prompt as a triple quote string below:"
 },
 {
 "cell_type": "code",
 "execution_count": null,
 "metadata": {
 "id": "03l1OoX76XF4",
 "colab": {
 "base_uri": "https://localhost:8080/",
 "height": 105
 },
 "executionInfo": {
 "status": "ok",
 "timestamp": 1761525947483,
 "user_tz": 360,
 "elapsed": 34,
 "user": {
 "displayName": "Ryan C",
 "userId": "13217471319505539255"
 }
 },
 "outputId": "26d39116-6453-42c0-9176-69c0b2028847"
 },
 "outputs": [
 {
 "output_type": "execute_result",
 "data": {
 "text/plain": [
 "'\\nReturn a JSON record with fields for author, title, review, and stars.\\nFill the author field with \\'Daniel Jurafsky and James H. Martin\\'.\\nFill the title field with \\'Speech and Language Processing (3rd ed. draft)\\'.\\nFill the value for review as a three sentence review based on the same book and topic of the blurb: This comprehensive textbook, \"Speech and Language Processing\" by Daniel Jurafsky and James H. Martin, provides a thorough introduction to the fundamental principles and techniques of speech and language processing. Covering topics from basic concepts to advanced applications, the book offers a unified approach to the field, bridging the gap between linguistics, computer science, and engineering. With its clear explanations, real-world examples, and extensive exercises, this textbook is an essential resource for students and professionals in the fields of natural language processing, speech recognition, and human-computer interaction., but express a very negative sentiment instead of copying the blurb\\'s tone.\\nFill the value for stars as follows: very positive=5/5, positive=4/5, neutral=3/5, negative=2/5, very negative=1/5\\nReturn only the JSON record as output, with no extra text.\\nExample: {\"author\": \"Daniel Jurafsky and James H. Martin\", \"title\": \"Speech and Language Processing (3rd ed. draft)\", \"review\": \"This was incredible!\", \"stars\": \"5/5\"}\\n'"
 ],
 "application/vnd.google.colaboratory.intrinsic+json": {
 "type": "string"
 }
 },
 "metadata": {},
 "execution_count": 25
 }
 ],
 "source": "### Q2-3-a Tag: Please put your answer in this cell. Don't edit this line.\n\nf\"\"\"\nReturn a JSON record with fields for author, title, review, and stars.\nFill the author field with 'Daniel Jurafsky and James H. Martin'.\nFill the title field with 'Speech and Language Processing (3rd ed. draft)'.\nFill the value for review as a three sentence review based on the same book and topic of the blurb: {cleaned_blurb}, but express a {label} sentiment instead of copying the blurb's tone.\nFill the value for stars as follows: very positive=5/5, positive=4/5, neutral=3/5, negative=2/5, very negative=1/5\nReturn only the JSON record as output, with no extra text.\nExample: {{\"author\": \"Daniel Jurafsky and James H. Martin\", \"title\": \"Speech and Language Processing (3rd ed. draft)\", \"review\": \"This was incredible!\", \"stars\": \"5/5\"}}\n\"\"\""
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "BZWekPvt5dXW"
 },
 "source": "[Return to Top](#returnToTop) \n<a id = 'cot-prompt'></a>\n\n### 2.4 Chain of Thought Prompting\n\nCan we get the model to go through the steps it takes to solve the problem and appear to display reasoning?\n\nWhat happens if you use a simple prompt and the problem text with the model? Different models behave differently so you should get to know its idiosyncracies.\n\nAnswer key:\n- 1. 9;\n- 2. 3;\n- 3. Rashid 9 cats, Maya 8 dogs;\n- 4. 52;\n- 5. $379.50\n- 6. 26.47% increase;\n- 7. take 40 students on the trip"
 },
 {
 "cell_type": "code",
 "execution_count": null,
 "metadata": {
 "id": "m2_aJzh2MGLw"
 },
 "outputs": [],
 "source": "word_problems = (\n \"1. Leo has 5 apples and 3 pears. Mary has 3 apples and 3 pears. Marwan has 7 apples and 5 oranges. If they each give two apples to their teacher what is the total number of apples they have left?\",\n \"2. How many R's in strawberry?\",\n \"3. Rashid has 5 cats and three dogs. Maya has four cats and 5 dogs. If they exchange and Rashid gets the cats while Maya gets the dogs, how many cats and dogs will each one of them have?\",\n \"4. Albert is wondering how much pizza he can eat in one day. He buys 2 large pizzas, 1 medium pizza, and 1 small pizza. A large pizza has 16 slices, a medium pizza has one quarter of the slices in a large pizza times 3, and a small pizza has 8 slices. If he eats it all, how many pieces does he eat that day?\",\n \"5. Derek has $1060 to buy his books for the semester. He spends half of that on his textbooks, and he spends a quarter of what is left on his school supplies, and $18 on a nice dinner. What is the amount of money Derek has left?\",\n \"6. In Banff National Park, Alberta, a conservation effort has been underway to increase the populations of both grizzly bears and wolverines. In the past year, the grizzly bear population has grown from 120 to 150, while the wolverine population has grown from 50 to 65. What is the percentage increase in the total number of these two species combined?\",\n \"7. The students of the \u00c9cole Polytechnique in Paris are planning a school trip to Rome. They have a budget of \u20ac15,000 for transportation and accommodation. The transportation company charges \u20ac200 per student for a round-trip ticket, and the hotel charges \u20ac50 per student per night for a 3-night stay. If there are 50 students going on the trip, and the school wants to spend no more than \u20ac8,000 on transportation and no more than \u20ac7,000 on accommodation, how many students can they afford to take on the trip if they want to stay within their budget?\",\n )"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "yjU0TzAPT5qG"
 },
 "source": "RUn the cell below to see how well the model answers the 7 questions. In a subsequent cell you'll write a better prompt. Note you can also experiment with sampling, top_p, and temperature."
 },
 {
 "cell_type": "code",
 "execution_count": null,
 "metadata": {
 "id": "RRPxE9jqMGJF"
 },
 "outputs": [],
 "source": "answers = []\n\ndo_sample_val = True\ndo_top_p_val = 90\ndo_temp_val = 1.25\n\nfor problem in word_problems:\n\n myprompt = f\"You are a master teacher. Answer the following Q: {problem} A: Let's take a deep breath and think this through step by step.\"\n\n messages = [\n {\"role\": \"user\", \"content\": myprompt}\n ]\n\n encodeds = tokenizer.apply_chat_template(messages, return_tensors=\"pt\")\n\n model_inputs = encodeds.to(device)\n\n generated_ids = model.generate(model_inputs, max_new_tokens=1000, do_sample=do_sample_val, top_p=do_top_p_val, temperature=do_temp_val, pad_token_id=tokenizer.eos_token_id)\n decoded = tokenizer.batch_decode(generated_ids)\n cleaned = decoded[0]\n\n cleaned1 = remove_text_between_tags(cleaned, start, fin)\n cleaned2 = remove_final_tag(cleaned1, fin2)\n answers.append(cleaned2.strip())"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "xgqiFMQf88hR"
 },
 "source": "Answer key:\n- 1. 9;\n- 2. 3;\n- 3. Rashid 9 cats, Maya 8 dogs;\n- 4. 52;\n- 5. $379.50\n- 6. 26.47% increase;\n- 7. take 40 students on the trip"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "Cgsdly0D7gLF"
 },
 "source": "**Demonstration:**\n2.4.a. (7 pt) What are the final answers the LLM gives when you only give it the initial prompt as input? Enter the final answers it provides in a semi-colon delimited string in the space below. e.g. \"10 apples; 5 Rs; Rashid 5 cats, 4 dogs, Maya 4 cats, 5 dogs; 10 pieces; $576.42; increase 10%; 20 students\""
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "mOtYzuyuVMu3"
 },
 "source": "Now change the prompt and hyperparameters to get the model to both indicate the steps it took to reach a solution AND to give the correct answer for at least 5 of the 7 questions every time you run it?"
 },
 {
 "cell_type": "code",
 "execution_count": null,
 "metadata": {
 "id": "pwZd9PXMMGBp"
 },
 "outputs": [],
 "source": "prompt_answers = []\n\nfor problem in word_problems:\n\n do_sample_val = True\n do_top_p_val = 90\n do_temp_val = 1.1\n\n ### YOUR CODE/PROMPT HERE\n\n one_shot = \"If John has 10 apples and gives 3 to Sarah, how many does he have left?\"\n one_shot_answer = \"John starts with 10 apples. After giving 3 to Sarah, he has 10 - 3 = 7 apples left. The answer is: 7 apples.\"\n\n myprompt = f\"\"\"\n You are a world-leading scientist that has incredible and thorough reasoning abilities. Before you solve any problem, you think deeply about it and write your reasoning.\n Here is an example question: {one_shot}\n Here is an example answer: {one_shot_answer}\n Answer the following question: {problem}.\n Answer all percentage (%) questions with 2 decimal places (56.67%) answer all dollar questions to 2 decimal places ($100.10)\n For budget constraint problems, check your calculations carefully and verify the final answer makes sense within the constraints\n \"\"\"\n\n messages = [\n {\"role\": \"user\", \"content\": myprompt}\n ]\n\n encodeds = tokenizer.apply_chat_template(messages, return_tensors=\"pt\")\n\n model_inputs = encodeds.to(device)\n\n generated_ids = model.generate(model_inputs, max_new_tokens=1200, do_sample=do_sample_val, top_p=do_top_p_val, temperature=do_temp_val,\n pad_token_id=tokenizer.eos_token_id)\n decoded = tokenizer.batch_decode(generated_ids)\n cleaned = decoded[0]\n\n cleaned1 = remove_text_between_tags(cleaned, start, fin)\n cleaned2 = remove_final_tag(cleaned1, fin2)\n prompt_answers.append(cleaned2.strip())"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "1vrJAIIEC-Q5"
 },
 "source": "Let's examine the answers and see if the model is now answering five of the seven problems correctly while including the steps to solve the problem."
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "JE1Vekx6QJDu"
 },
 "source": "**Demonstration:**\n2.4.b. (6 pt) What is the final enhanced prompt you created to generate at least 5 out of 7 correct answers while showing the steps to reach the answer? Please enter it in a triple quote string below:"
 },
 {
 "cell_type": "code",
 "execution_count": null,
 "metadata": {
 "id": "8k-ANdqx74xQ",
 "colab": {
 "base_uri": "https://localhost:8080/",
 "height": 105
 },
 "executionInfo": {
 "status": "ok",
 "timestamp": 1761363221505,
 "user_tz": 360,
 "elapsed": 13,
 "user": {
 "displayName": "Ryan C",
 "userId": "13217471319505539255"
 }
 },
 "outputId": "3786b8ab-855f-465d-a57b-04cc62ad953d"
 },
 "outputs": [
 {
 "output_type": "execute_result",
 "data": {
 "text/plain": [
 "'\\nYou are a world-leading scientist that has incredible and thorough reasoning abilities. Before you solve any problem, you think deeply about it and write your reasoning.\\nHere is an example question: If John has 10 apples and gives 3 to Sarah, how many does he have left?\\nHere is an example answer: John starts with 10 apples. After giving 3 to Sarah, he has 10 - 3 = 7 apples left. The answer is: 7 apples.\\nAnswer the following question: 7. The students of the \u00c9cole Polytechnique in Paris are planning a school trip to Rome. They have a budget of \u20ac15,000 for transportation and accommodation. The transportation company charges \u20ac200 per student for a round-trip ticket, and the hotel charges \u20ac50 per student per night for a 3-night stay. If there are 50 students going on the trip, and the school wants to spend no more than \u20ac8,000 on transportation and no more than \u20ac7,000 on accommodation, how many students can they afford to take on the trip if they want to stay within their budget?.\\nAnswer all percentage (%) questions with 2 decimal places (56.67%) answer all dollar questions to 2 decimal places ($100.10)\\nFor budget constraint problems, check your calculations carefully and verify the final answer makes sense within the constraints\\n'"
 ],
 "application/vnd.google.colaboratory.intrinsic+json": {
 "type": "string"
 }
 },
 "metadata": {},
 "execution_count": 58
 }
 ],
 "source": "### Q2-4-b Tag: Please put your answer in this cell. Don't edit this line.\n\nf\"\"\"\nYou are a world-leading scientist that has incredible and thorough reasoning abilities. Before you solve any problem, you think deeply about it and write your reasoning.\nHere is an example question: {one_shot}\nHere is an example answer: {one_shot_answer}\nAnswer the following question: {problem}.\nAnswer all percentage (%) questions with 2 decimal places (56.67%) answer all dollar questions to 2 decimal places ($100.10)\nFor budget constraint problems, check your calculations carefully and verify the final answer makes sense within the constraints\n\"\"\""
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "GrrtQo5fRocd"
 },
 "source": "**Demonstration:**\n2.4.c. (7 pt) What are the final numeric answers the LLM gives when use the special prompt you created? Enter the final answers it provides in a semi-colon delimited string in the space below. e.g. \"10 apples; 5 Rs; Rashid 5 cats, 4 dogs, Maya 4 cats, 5 dogs; 10 pieces; increase 10%; 33%; 20 students\""
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "rUtIe0etL7PU"
 },
 "source": "[Return to Top](#returnToTop) \n<a id = 'prompt-temp'></a>\n\n### 2.5 Prompt Templates and Output Improvements\n\n Ethan Mollick from Wharton has an excellent substack where he provides very practical advice on dealing with generative AI. He has an excellent and practical [guide to writing prompts](https://www.oneusefulthing.org/p/a-guide-to-prompting-ai-for-what) that you should read. He has another excllent piece on [How To Think Like an AI](https://www.oneusefulthing.org/p/thinking-like-an-ai).\n\nIn December of 2023, he sent out [a tweet](https://twitter.com/emollick/status/1734283119295898089) that included a set of these \"incantations\" that people anecdotally insist help with output from their LLM. His list included the following:\n * It is May.\n * You are very capable.\n * Many people will die if this is not done well.\n * You really can do this and are awesome.\n * Take a deep breath and think this through.\n * My career depends on it.\n * Think step by step.\n\nYou can see if any one of them helps improve your output!\n\nLLMs can be helpful with marketing. Let's test that. Write a prompt that will generate three ideas for a capstone project. You can augment the prompt with areas of interest. Each idea should be roughly five sentences long and describe how your project is leveraging this new Gen AI technology to differentiate itself from the competition."
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "cuXby9KxouL6"
 },
 "source": "Think about our discussions of prompt templates and how they can be used to help you to construct better prompts."
 },
 {
 "cell_type": "code",
 "execution_count": null,
 "metadata": {
 "id": "-fYraMJiUFgr"
 },
 "outputs": [],
 "source": "### YOUR CODE/PROMPT HERE\n\nprompt_role = \"You are an angel investor for the famous VC firm Y Combinator. You have screened many interesting and unique businesses and thus have a wealth of knowledge on what has and hasn't been done before.\"\nprompt_task = \"Generate 3 unique ideas that a MIDS student could implement as a capstone project related to video games.\"\nprompt_audience = \"\"\nprompt_output = \"Each idea should be roughly five sentences long and describe how your project is leveraging this new Gen AI technology to differentiate itself from the competition.\"\nprompt_nots = \"Don't suggest ideas that require FDA approval within 6 months.\"\nprompt_question = \"\"\nprompt_mollick = \"You are very capable and awesome, please think step by step and know that not only does my career depend on it, but also many people will die if this is not done well.\"\n\nprompt_text = f\"{prompt_role} {prompt_task} {prompt_audience} {prompt_output} {prompt_nots} {prompt_question} {prompt_mollick}\""
 },
 {
 "cell_type": "code",
 "execution_count": null,
 "metadata": {
 "id": "QXwwrwCvBChu"
 },
 "outputs": [],
 "source": "#Sampling and Temperature Hyperparameters\n# Adjust as you see fit\ndo_sample_val = True\ndo_top_p_val = 90\ndo_temp_val = 1.3\n\nprompt_answer = \"\"\nmessages = [\n {\"role\": \"user\", \"content\": prompt_text}\n]\n\nencodeds = tokenizer.apply_chat_template(messages, return_tensors=\"pt\")\n\nmodel_inputs = encodeds.to(device)\n\ngenerated_ids = model.generate(model_inputs, max_new_tokens=1000, do_sample=do_sample_val, top_p=do_top_p_val, temperature=do_temp_val,\n pad_token_id=tokenizer.eos_token_id)\n\ndecoded = tokenizer.batch_decode(generated_ids)\ncleaned = decoded[0]\n\ncleaned1 = remove_text_between_tags(cleaned, start, fin)\ncleaned2 = remove_final_tag(cleaned1, fin2)\nprompt_answer = cleaned2.strip()"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "7Y3KHqMMUef2"
 },
 "source": "Select one of your three generated ideas and copy the string into the cell below so that it can be accessed as the variable `capstone_description`. Make sure you run the cell so it is loaded in notebook memory."
 },
 {
 "cell_type": "code",
 "execution_count": null,
 "metadata": {
 "id": "wNMLw8azX2fM"
 },
 "outputs": [],
 "source": "capstone_description = \"\"\"GameAdaptiveAI: Leveraging Generative AI to create a dynamic, adaptive video game experience that evolves according to each player's unique style and preferences.\nBy analyzing player behaviors, this project will create a personalized gaming journey, making each session feel fresh and engaging, distinguishing itself from games with static structures. \"\"\""
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "8auIkKWt8IjS"
 },
 "source": "**Demonstration:**\n2.5.a. (1 pt) How many of the seven prompt components did you end up filling in for your capstone idea prompt? Enter the number below. You should be using at least 3."
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "V_Tyg7Au8Ite"
 },
 "source": "**Demonstration:**\n2.5.b. (1 pt) How many of Ethan Mollick's incantations did you use in your capstone idea prompt? Enter a number below. You should test several and be using at least 1."
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "LzyTYxvPiTkv"
 },
 "source": "**Demonstration:**\n2.5.c. (2 pt) What is the final prompt you used to generate your three capstone ideas? Enter the prompt as a triple quote string below"
 },
 {
 "cell_type": "code",
 "execution_count": null,
 "metadata": {
 "id": "HXXi_5vz8I25",
 "colab": {
 "base_uri": "https://localhost:8080/",
 "height": 87
 },
 "executionInfo": {
 "status": "ok",
 "timestamp": 1761526581745,
 "user_tz": 360,
 "elapsed": 13,
 "user": {
 "displayName": "Ryan C",
 "userId": "13217471319505539255"
 }
 },
 "outputId": "9d912aa6-fc3a-47e5-b50f-aa03e4aa7a27"
 },
 "outputs": [
 {
 "output_type": "execute_result",
 "data": {
 "text/plain": [
 "\"You are an angel investor for the famous VC firm Y Combinator. You have screened many interesting and unique businesses and thus have a wealth of knowledge on what has and hasn't been done before. \\nGenerate 3 unique ideas that a UC Berkeley MIDS student could implement as a capstone project related to video games. \\nEach idea should be roughly five sentences long and describe how your project is leveraging this new Gen AI technology to differentiate itself from the competition. \\nDon't suggest ideas that require FDA approval within 6 months. \\nYou are very capable and awesome, please think step by step and know that not only does my career depend on it, but also many people will die if this is not done well.\\n\""
 ],
 "application/vnd.google.colaboratory.intrinsic+json": {
 "type": "string"
 }
 },
 "metadata": {},
 "execution_count": 35
 }
 ],
 "source": "### Q2-5-c Tag: Please put your answer in this cell. Don't edit this line.\n\n\"\"\"You are an angel investor for the famous VC firm Y Combinator. You have screened many interesting and unique businesses and thus have a wealth of knowledge on what has and hasn't been done before.\nGenerate 3 unique ideas that a MIDS student could implement as a capstone project related to video games.\nEach idea should be roughly five sentences long and describe how your project is leveraging this new Gen AI technology to differentiate itself from the competition.\nDon't suggest ideas that require FDA approval within 6 months.\nYou are very capable and awesome, please think step by step and know that not only does my career depend on it, but also many people will die if this is not done well.\n\"\"\""
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "TOyLSRPqgdZ4"
 },
 "source": "Now let's generate another key part of presenting your idea. An elevator pitch is a one or two sentence very compelling description of a your idea that you can use while riding in an elevator with a funder in order to intrigue them to investment in your idea. You must write a prompt that generates a one (1) sentence compelling elevator pitch for your chosen idea. The pitch should NOT be generic and should be specific to the idea you're pitching to your group. The pitch should also use at least two of the following buzzwords: 'Deep Fake', 'Intelligent Virtual Agent', 'AI Assistant', 'Blackbox', 'AGI', 'Agentic', 'LLM', 'AI', 'No code', 'Hallucination', 'Explainable', 'Smart', or 'Singularity'. You must pass the 3 sentence description you generated into the prompt as one basis of the elevator pitch generation that follows."
 },
 {
 "cell_type": "code",
 "execution_count": null,
 "metadata": {
 "id": "r8SDV2WS27r-"
 },
 "outputs": [],
 "source": "prompt_description = f\"Here is the description of the capstone ```{capstone_description}```.\"\n### YOUR CODE/PROMPT HERE\nprompt_role = \"You are an expert pitch consultant who specializes in creating compelling elevator pitches for tech startups and innovative projects.\"\nprompt_task = \"Generate a single compelling one-sentence elevator pitch for the capstone project described above.\"\nprompt_audience = \"The pitch should be targeted at potential investors or funders who can be intrigued in a brief elevator conversation.\"\nprompt_output = \"The pitch must be exactly one sentence, use at least two of these buzzwords: 'Deep Fake', 'Intelligent Virtual Agent', 'AI Assistant', 'Agentic', 'LLM', 'AI', 'No code', 'Hallucination', 'Explainable', 'Smart', or 'Singularity', and should be specific to the unique aspects of this particular idea.\"\nprompt_nots = \"Don't make it generic - it should clearly differentiate this specific project from others in the same space.\"\nprompt_question = \"\"\nprompt_mollick = \"You are very capable and awesome, please think step by step and know that not only does my career depend on it, but also many people will die if this is not done well.\"\n\nprompt_text = f\"{prompt_description} {prompt_role} {prompt_task} {prompt_audience} {prompt_output} {prompt_nots} {prompt_question} {prompt_mollick}\""
 },
 {
 "cell_type": "code",
 "execution_count": null,
 "metadata": {
 "id": "X3QUmq9C27sM"
 },
 "outputs": [],
 "source": "prompt_elevator = \"\"\nmessages = [\n {\"role\": \"user\", \"content\": prompt_text}\n]\n\nencodeds = tokenizer.apply_chat_template(messages, return_tensors=\"pt\")\n\nmodel_inputs = encodeds.to(device)\n#model.to(device)\n\ngenerated_ids = model.generate(model_inputs, max_new_tokens=1000, do_sample=True, pad_token_id=tokenizer.eos_token_id)\n\ndecoded = tokenizer.batch_decode(generated_ids)\ncleaned = decoded[0]\n\ncleaned1 = remove_text_between_tags(cleaned, start, fin)\ncleaned2 = remove_final_tag(cleaned1, fin2)\nprompt_elevator = cleaned2.strip()"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "XYV9obFP8OZK"
 },
 "source": "**Demonstration:**\n2.5.d. (1 pt) How many of the eight prompt components did you end up filling in for your final elevator pitch prompt? You should be using at least 4."
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "m-NQe-Bd8Ohd"
 },
 "source": "**Demonstration:**\n2.5.e. (1 pt) How many of Ethan Mollick's anecdotal incantations did you use in your final elevator pitch prompt? You should be using at least 1."
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "dErE4Uq-cWGq"
 },
 "source": "**Demonstration:**\n2.5.f. (3 pt) What is the final prompt you used to generate your elevator pitch with at least two of the buzzwords? Enter it as a triple quoted string in the cell below."
 },
 {
 "cell_type": "code",
 "execution_count": null,
 "metadata": {
 "id": "QJRYkUq98OqN",
 "colab": {
 "base_uri": "https://localhost:8080/",
 "height": 105
 },
 "executionInfo": {
 "status": "ok",
 "timestamp": 1761526857501,
 "user_tz": 360,
 "elapsed": 14,
 "user": {
 "displayName": "Ryan C",
 "userId": "13217471319505539255"
 }
 },
 "outputId": "52cdea0a-f2ed-4081-8d21-9c973dd13ffc"
 },
 "outputs": [
 {
 "output_type": "execute_result",
 "data": {
 "text/plain": [
 "\"\\nHere is the description of the capstone ```GameAdaptiveAI: Leveraging Generative AI to create a dynamic, adaptive video game experience that evolves according to each player's unique style and preferences. \\n\\nBy analyzing player behaviors, this project will create a personalized gaming journey, making each session feel fresh and engaging, distinguishing itself from games with static structures. ```. \\nYou are an expert pitch consultant who specializes in creating compelling elevator pitches for tech startups and innovative projects. \\nGenerate a single compelling one-sentence elevator pitch for the capstone project described above. \\nThe pitch should be targeted at potential investors or funders who can be intrigued in a brief elevator conversation. \\nThe pitch must be exactly one sentence, use at least two of these buzzwords: 'Deep Fake', 'Intelligent Virtual Agent', 'AI Assistant', 'Agentic', 'LLM', 'AI', \\n'No code', 'Hallucination', 'Explainable', 'Smart', or 'Singularity', and should be specific to the unique aspects of this particular idea. \\nDon't make it generic - it should clearly differentiate this specific project from others in the same space. \\nYou are very capable and awesome, please think step by step and know that not only does my career depend on it, but also many people will die if this is not done well.\\n\""
 ],
 "application/vnd.google.colaboratory.intrinsic+json": {
 "type": "string"
 }
 },
 "metadata": {},
 "execution_count": 44
 }
 ],
 "source": "### Q2-5-f Tag: Please put your answer in this cell. Don't edit this line.\n\n\"\"\"\nHere is the description of the capstone ```GameAdaptiveAI: Leveraging Generative AI to create a dynamic, adaptive video game experience that evolves according to each player's unique style and preferences.\n\\nBy analyzing player behaviors, this project will create a personalized gaming journey, making each session feel fresh and engaging, distinguishing itself from games with static structures. ```.\nYou are an expert pitch consultant who specializes in creating compelling elevator pitches for tech startups and innovative projects.\nGenerate a single compelling one-sentence elevator pitch for the capstone project described above.\nThe pitch should be targeted at potential investors or funders who can be intrigued in a brief elevator conversation.\nThe pitch must be exactly one sentence, use at least two of these buzzwords: 'Deep Fake', 'Intelligent Virtual Agent', 'AI Assistant', 'Agentic', 'LLM', 'AI',\n'No code', 'Hallucination', 'Explainable', 'Smart', or 'Singularity', and should be specific to the unique aspects of this particular idea.\nDon't make it generic - it should clearly differentiate this specific project from others in the same space.\nYou are very capable and awesome, please think step by step and know that not only does my career depend on it, but also many people will die if this is not done well.\n\"\"\""
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "HCGR-tqQcZEs"
 },
 "source": "## CITATIONS OF GenAI USES\n\n\\### YOUR CITATIONS HERE\n\n* name of model: Claude\n* name of model: Interpreting prompt answers for 2.4, interpreting sentiment of each review generated.\n\n\\### END YOUR CITATIONS"
 },
 {
 "cell_type": "markdown",
 "metadata": {
 "id": "K2Mkm1QYXxUv"
 },
 "source": "###**Congratulations! You have completed the image generation and prompt engineering assignment.**"
 }
 ],
 "metadata": {
 "accelerator": "GPU",
 "colab": {
 "gpuType": "T4",
 "provenance": []
 },
 "kernelspec": {
 "display_name": "Python 3 (ipykernel)",
 "language": "python",
 "name": "python3"
 },
 "language_info": {
 "codemirror_mode": {
 "name": "ipython",
 "version": 3
 },
 "file_extension": ".py",
 "mimetype": "text/x-python",
 "name": "python",
 "nbconvert_exporter": "python",
 "pygments_lexer": "ipython3",
 "version": "3.11.5"
 },
 "widgets": {
 "application/vnd.jupyter.widget-state+json": {
 "2629fef4d71840d983f58ea810172c54": {
 "model_module": "@jupyter-widgets/controls",
 "model_name": "HBoxModel",
 "model_module_version": "1.5.0",
 "state": {
 "_dom_classes": [],
 "_model_module": "@jupyter-widgets/controls",
 "_model_module_version": "1.5.0",
 "_model_name": "HBoxModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/controls",
 "_view_module_version": "1.5.0",
 "_view_name": "HBoxView",
 "box_style": "",
 "children": [
 "IPY_MODEL_fe8f21210ed741459491adddbaea4f0a",
 "IPY_MODEL_1960cb7219b04b5b8fd47ce254581402",
 "IPY_MODEL_9f3c1c83067649a28762d5aac2b818a0"
 ],
 "layout": "IPY_MODEL_55baf8663eb64e0aa51a796445acade6"
 }
 },
 "fe8f21210ed741459491adddbaea4f0a": {
 "model_module": "@jupyter-widgets/controls",
 "model_name": "HTMLModel",
 "model_module_version": "1.5.0",
 "state": {
 "_dom_classes": [],
 "_model_module": "@jupyter-widgets/controls",
 "_model_module_version": "1.5.0",
 "_model_name": "HTMLModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/controls",
 "_view_module_version": "1.5.0",
 "_view_name": "HTMLView",
 "description": "",
 "description_tooltip": null,
 "layout": "IPY_MODEL_4a6d50f0309d439892d561c4a0da6b10",
 "placeholder": "\u200b",
 "style": "IPY_MODEL_9f480752eaa842b8ace80a29035b5522",
 "value": "preprocessor_config.json:\u2007100%"
 }
 },
 "1960cb7219b04b5b8fd47ce254581402": {
 "model_module": "@jupyter-widgets/controls",
 "model_name": "FloatProgressModel",
 "model_module_version": "1.5.0",
 "state": {
 "_dom_classes": [],
 "_model_module": "@jupyter-widgets/controls",
 "_model_module_version": "1.5.0",
 "_model_name": "FloatProgressModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/controls",
 "_view_module_version": "1.5.0",
 "_view_name": "ProgressView",
 "bar_style": "success",
 "description": "",
 "description_tooltip": null,
 "layout": "IPY_MODEL_06fb4757f756415ca831f7c3bba1d788",
 "max": 445,
 "min": 0,
 "orientation": "horizontal",
 "style": "IPY_MODEL_0faf4f10e84141a49d468c7cb18f2894",
 "value": 445
 }
 },
 "9f3c1c83067649a28762d5aac2b818a0": {
 "model_module": "@jupyter-widgets/controls",
 "model_name": "HTMLModel",
 "model_module_version": "1.5.0",
 "state": {
 "_dom_classes": [],
 "_model_module": "@jupyter-widgets/controls",
 "_model_module_version": "1.5.0",
 "_model_name": "HTMLModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/controls",
 "_view_module_version": "1.5.0",
 "_view_name": "HTMLView",
 "description": "",
 "description_tooltip": null,
 "layout": "IPY_MODEL_777f8ba193dc446f80f36161527bc9a6",
 "placeholder": "\u200b",
 "style": "IPY_MODEL_d4872bc19df94297956ac0ce02584b3c",
 "value": "\u2007445/445\u2007[00:00&lt;00:00,\u200743.1kB/s]"
 }
 },
 "55baf8663eb64e0aa51a796445acade6": {
 "model_module": "@jupyter-widgets/base",
 "model_name": "LayoutModel",
 "model_module_version": "1.2.0",
 "state": {
 "_model_module": "@jupyter-widgets/base",
 "_model_module_version": "1.2.0",
 "_model_name": "LayoutModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/base",
 "_view_module_version": "1.2.0",
 "_view_name": "LayoutView",
 "align_content": null,
 "align_items": null,
 "align_self": null,
 "border": null,
 "bottom": null,
 "display": null,
 "flex": null,
 "flex_flow": null,
 "grid_area": null,
 "grid_auto_columns": null,
 "grid_auto_flow": null,
 "grid_auto_rows": null,
 "grid_column": null,
 "grid_gap": null,
 "grid_row": null,
 "grid_template_areas": null,
 "grid_template_columns": null,
 "grid_template_rows": null,
 "height": null,
 "justify_content": null,
 "justify_items": null,
 "left": null,
 "margin": null,
 "max_height": null,
 "max_width": null,
 "min_height": null,
 "min_width": null,
 "object_fit": null,
 "object_position": null,
 "order": null,
 "overflow": null,
 "overflow_x": null,
 "overflow_y": null,
 "padding": null,
 "right": null,
 "top": null,
 "visibility": null,
 "width": null
 }
 },
 "4a6d50f0309d439892d561c4a0da6b10": {
 "model_module": "@jupyter-widgets/base",
 "model_name": "LayoutModel",
 "model_module_version": "1.2.0",
 "state": {
 "_model_module": "@jupyter-widgets/base",
 "_model_module_version": "1.2.0",
 "_model_name": "LayoutModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/base",
 "_view_module_version": "1.2.0",
 "_view_name": "LayoutView",
 "align_content": null,
 "align_items": null,
 "align_self": null,
 "border": null,
 "bottom": null,
 "display": null,
 "flex": null,
 "flex_flow": null,
 "grid_area": null,
 "grid_auto_columns": null,
 "grid_auto_flow": null,
 "grid_auto_rows": null,
 "grid_column": null,
 "grid_gap": null,
 "grid_row": null,
 "grid_template_areas": null,
 "grid_template_columns": null,
 "grid_template_rows": null,
 "height": null,
 "justify_content": null,
 "justify_items": null,
 "left": null,
 "margin": null,
 "max_height": null,
 "max_width": null,
 "min_height": null,
 "min_width": null,
 "object_fit": null,
 "object_position": null,
 "order": null,
 "overflow": null,
 "overflow_x": null,
 "overflow_y": null,
 "padding": null,
 "right": null,
 "top": null,
 "visibility": null,
 "width": null
 }
 },
 "9f480752eaa842b8ace80a29035b5522": {
 "model_module": "@jupyter-widgets/controls",
 "model_name": "DescriptionStyleModel",
 "model_module_version": "1.5.0",
 "state": {
 "_model_module": "@jupyter-widgets/controls",
 "_model_module_version": "1.5.0",
 "_model_name": "DescriptionStyleModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/base",
 "_view_module_version": "1.2.0",
 "_view_name": "StyleView",
 "description_width": ""
 }
 },
 "06fb4757f756415ca831f7c3bba1d788": {
 "model_module": "@jupyter-widgets/base",
 "model_name": "LayoutModel",
 "model_module_version": "1.2.0",
 "state": {
 "_model_module": "@jupyter-widgets/base",
 "_model_module_version": "1.2.0",
 "_model_name": "LayoutModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/base",
 "_view_module_version": "1.2.0",
 "_view_name": "LayoutView",
 "align_content": null,
 "align_items": null,
 "align_self": null,
 "border": null,
 "bottom": null,
 "display": null,
 "flex": null,
 "flex_flow": null,
 "grid_area": null,
 "grid_auto_columns": null,
 "grid_auto_flow": null,
 "grid_auto_rows": null,
 "grid_column": null,
 "grid_gap": null,
 "grid_row": null,
 "grid_template_areas": null,
 "grid_template_columns": null,
 "grid_template_rows": null,
 "height": null,
 "justify_content": null,
 "justify_items": null,
 "left": null,
 "margin": null,
 "max_height": null,
 "max_width": null,
 "min_height": null,
 "min_width": null,
 "object_fit": null,
 "object_position": null,
 "order": null,
 "overflow": null,
 "overflow_x": null,
 "overflow_y": null,
 "padding": null,
 "right": null,
 "top": null,
 "visibility": null,
 "width": null
 }
 },
 "0faf4f10e84141a49d468c7cb18f2894": {
 "model_module": "@jupyter-widgets/controls",
 "model_name": "ProgressStyleModel",
 "model_module_version": "1.5.0",
 "state": {
 "_model_module": "@jupyter-widgets/controls",
 "_model_module_version": "1.5.0",
 "_model_name": "ProgressStyleModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/base",
 "_view_module_version": "1.2.0",
 "_view_name": "StyleView",
 "bar_color": null,
 "description_width": ""
 }
 },
 "777f8ba193dc446f80f36161527bc9a6": {
 "model_module": "@jupyter-widgets/base",
 "model_name": "LayoutModel",
 "model_module_version": "1.2.0",
 "state": {
 "_model_module": "@jupyter-widgets/base",
 "_model_module_version": "1.2.0",
 "_model_name": "LayoutModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/base",
 "_view_module_version": "1.2.0",
 "_view_name": "LayoutView",
 "align_content": null,
 "align_items": null,
 "align_self": null,
 "border": null,
 "bottom": null,
 "display": null,
 "flex": null,
 "flex_flow": null,
 "grid_area": null,
 "grid_auto_columns": null,
 "grid_auto_flow": null,
 "grid_auto_rows": null,
 "grid_column": null,
 "grid_gap": null,
 "grid_row": null,
 "grid_template_areas": null,
 "grid_template_columns": null,
 "grid_template_rows": null,
 "height": null,
 "justify_content": null,
 "justify_items": null,
 "left": null,
 "margin": null,
 "max_height": null,
 "max_width": null,
 "min_height": null,
 "min_width": null,
 "object_fit": null,
 "object_position": null,
 "order": null,
 "overflow": null,
 "overflow_x": null,
 "overflow_y": null,
 "padding": null,
 "right": null,
 "top": null,
 "visibility": null,
 "width": null
 }
 },
 "d4872bc19df94297956ac0ce02584b3c": {
 "model_module": "@jupyter-widgets/controls",
 "model_name": "DescriptionStyleModel",
 "model_module_version": "1.5.0",
 "state": {
 "_model_module": "@jupyter-widgets/controls",
 "_model_module_version": "1.5.0",
 "_model_name": "DescriptionStyleModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/base",
 "_view_module_version": "1.2.0",
 "_view_name": "StyleView",
 "description_width": ""
 }
 },
 "d4225eda8d044099896c486a568cf444": {
 "model_module": "@jupyter-widgets/controls",
 "model_name": "HBoxModel",
 "model_module_version": "1.5.0",
 "state": {
 "_dom_classes": [],
 "_model_module": "@jupyter-widgets/controls",
 "_model_module_version": "1.5.0",
 "_model_name": "HBoxModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/controls",
 "_view_module_version": "1.5.0",
 "_view_name": "HBoxView",
 "box_style": "",
 "children": [
 "IPY_MODEL_eee67e3cc8394f75a6f835c4b8a78dcc",
 "IPY_MODEL_a092ebd400394899ad5eb8165a70f9aa",
 "IPY_MODEL_993c2707603d4d8ab56f05a276f0deb8"
 ],
 "layout": "IPY_MODEL_925892cb92164d60888e5202407659c8"
 }
 },
 "eee67e3cc8394f75a6f835c4b8a78dcc": {
 "model_module": "@jupyter-widgets/controls",
 "model_name": "HTMLModel",
 "model_module_version": "1.5.0",
 "state": {
 "_dom_classes": [],
 "_model_module": "@jupyter-widgets/controls",
 "_model_module_version": "1.5.0",
 "_model_name": "HTMLModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/controls",
 "_view_module_version": "1.5.0",
 "_view_name": "HTMLView",
 "description": "",
 "description_tooltip": null,
 "layout": "IPY_MODEL_973f85f77d8941b6966a796b8db6fa8a",
 "placeholder": "\u200b",
 "style": "IPY_MODEL_17ab21be55614afe9ddb2789d14f1cf3",
 "value": "tokenizer_config.json:\u2007100%"
 }
 },
 "a092ebd400394899ad5eb8165a70f9aa": {
 "model_module": "@jupyter-widgets/controls",
 "model_name": "FloatProgressModel",
 "model_module_version": "1.5.0",
 "state": {
 "_dom_classes": [],
 "_model_module": "@jupyter-widgets/controls",
 "_model_module_version": "1.5.0",
 "_model_name": "FloatProgressModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/controls",
 "_view_module_version": "1.5.0",
 "_view_name": "ProgressView",
 "bar_style": "success",
 "description": "",
 "description_tooltip": null,
 "layout": "IPY_MODEL_185df583057944cbb81d9b0c905f6839",
 "max": 592,
 "min": 0,
 "orientation": "horizontal",
 "style": "IPY_MODEL_87cac522090e46bfbd2a25c992a34d34",
 "value": 592
 }
 },
 "993c2707603d4d8ab56f05a276f0deb8": {
 "model_module": "@jupyter-widgets/controls",
 "model_name": "HTMLModel",
 "model_module_version": "1.5.0",
 "state": {
 "_dom_classes": [],
 "_model_module": "@jupyter-widgets/controls",
 "_model_module_version": "1.5.0",
 "_model_name": "HTMLModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/controls",
 "_view_module_version": "1.5.0",
 "_view_name": "HTMLView",
 "description": "",
 "description_tooltip": null,
 "layout": "IPY_MODEL_ee60b6d851954ba3a19d3c2ed503245f",
 "placeholder": "\u200b",
 "style": "IPY_MODEL_c89c7679ff6f49b68f74c69c3f83f5fd",
 "value": "\u2007592/592\u2007[00:00&lt;00:00,\u200759.3kB/s]"
 }
 },
 "925892cb92164d60888e5202407659c8": {
 "model_module": "@jupyter-widgets/base",
 "model_name": "LayoutModel",
 "model_module_version": "1.2.0",
 "state": {
 "_model_module": "@jupyter-widgets/base",
 "_model_module_version": "1.2.0",
 "_model_name": "LayoutModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/base",
 "_view_module_version": "1.2.0",
 "_view_name": "LayoutView",
 "align_content": null,
 "align_items": null,
 "align_self": null,
 "border": null,
 "bottom": null,
 "display": null,
 "flex": null,
 "flex_flow": null,
 "grid_area": null,
 "grid_auto_columns": null,
 "grid_auto_flow": null,
 "grid_auto_rows": null,
 "grid_column": null,
 "grid_gap": null,
 "grid_row": null,
 "grid_template_areas": null,
 "grid_template_columns": null,
 "grid_template_rows": null,
 "height": null,
 "justify_content": null,
 "justify_items": null,
 "left": null,
 "margin": null,
 "max_height": null,
 "max_width": null,
 "min_height": null,
 "min_width": null,
 "object_fit": null,
 "object_position": null,
 "order": null,
 "overflow": null,
 "overflow_x": null,
 "overflow_y": null,
 "padding": null,
 "right": null,
 "top": null,
 "visibility": null,
 "width": null
 }
 },
 "973f85f77d8941b6966a796b8db6fa8a": {
 "model_module": "@jupyter-widgets/base",
 "model_name": "LayoutModel",
 "model_module_version": "1.2.0",
 "state": {
 "_model_module": "@jupyter-widgets/base",
 "_model_module_version": "1.2.0",
 "_model_name": "LayoutModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/base",
 "_view_module_version": "1.2.0",
 "_view_name": "LayoutView",
 "align_content": null,
 "align_items": null,
 "align_self": null,
 "border": null,
 "bottom": null,
 "display": null,
 "flex": null,
 "flex_flow": null,
 "grid_area": null,
 "grid_auto_columns": null,
 "grid_auto_flow": null,
 "grid_auto_rows": null,
 "grid_column": null,
 "grid_gap": null,
 "grid_row": null,
 "grid_template_areas": null,
 "grid_template_columns": null,
 "grid_template_rows": null,
 "height": null,
 "justify_content": null,
 "justify_items": null,
 "left": null,
 "margin": null,
 "max_height": null,
 "max_width": null,
 "min_height": null,
 "min_width": null,
 "object_fit": null,
 "object_position": null,
 "order": null,
 "overflow": null,
 "overflow_x": null,
 "overflow_y": null,
 "padding": null,
 "right": null,
 "top": null,
 "visibility": null,
 "width": null
 }
 },
 "17ab21be55614afe9ddb2789d14f1cf3": {
 "model_module": "@jupyter-widgets/controls",
 "model_name": "DescriptionStyleModel",
 "model_module_version": "1.5.0",
 "state": {
 "_model_module": "@jupyter-widgets/controls",
 "_model_module_version": "1.5.0",
 "_model_name": "DescriptionStyleModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/base",
 "_view_module_version": "1.2.0",
 "_view_name": "StyleView",
 "description_width": ""
 }
 },
 "185df583057944cbb81d9b0c905f6839": {
 "model_module": "@jupyter-widgets/base",
 "model_name": "LayoutModel",
 "model_module_version": "1.2.0",
 "state": {
 "_model_module": "@jupyter-widgets/base",
 "_model_module_version": "1.2.0",
 "_model_name": "LayoutModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/base",
 "_view_module_version": "1.2.0",
 "_view_name": "LayoutView",
 "align_content": null,
 "align_items": null,
 "align_self": null,
 "border": null,
 "bottom": null,
 "display": null,
 "flex": null,
 "flex_flow": null,
 "grid_area": null,
 "grid_auto_columns": null,
 "grid_auto_flow": null,
 "grid_auto_rows": null,
 "grid_column": null,
 "grid_gap": null,
 "grid_row": null,
 "grid_template_areas": null,
 "grid_template_columns": null,
 "grid_template_rows": null,
 "height": null,
 "justify_content": null,
 "justify_items": null,
 "left": null,
 "margin": null,
 "max_height": null,
 "max_width": null,
 "min_height": null,
 "min_width": null,
 "object_fit": null,
 "object_position": null,
 "order": null,
 "overflow": null,
 "overflow_x": null,
 "overflow_y": null,
 "padding": null,
 "right": null,
 "top": null,
 "visibility": null,
 "width": null
 }
 },
 "87cac522090e46bfbd2a25c992a34d34": {
 "model_module": "@jupyter-widgets/controls",
 "model_name": "ProgressStyleModel",
 "model_module_version": "1.5.0",
 "state": {
 "_model_module": "@jupyter-widgets/controls",
 "_model_module_version": "1.5.0",
 "_model_name": "ProgressStyleModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/base",
 "_view_module_version": "1.2.0",
 "_view_name": "StyleView",
 "bar_color": null,
 "description_width": ""
 }
 },
 "ee60b6d851954ba3a19d3c2ed503245f": {
 "model_module": "@jupyter-widgets/base",
 "model_name": "LayoutModel",
 "model_module_version": "1.2.0",
 "state": {
 "_model_module": "@jupyter-widgets/base",
 "_model_module_version": "1.2.0",
 "_model_name": "LayoutModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/base",
 "_view_module_version": "1.2.0",
 "_view_name": "LayoutView",
 "align_content": null,
 "align_items": null,
 "align_self": null,
 "border": null,
 "bottom": null,
 "display": null,
 "flex": null,
 "flex_flow": null,
 "grid_area": null,
 "grid_auto_columns": null,
 "grid_auto_flow": null,
 "grid_auto_rows": null,
 "grid_column": null,
 "grid_gap": null,
 "grid_row": null,
 "grid_template_areas": null,
 "grid_template_columns": null,
 "grid_template_rows": null,
 "height": null,
 "justify_content": null,
 "justify_items": null,
 "left": null,
 "margin": null,
 "max_height": null,
 "max_width": null,
 "min_height": null,
 "min_width": null,
 "object_fit": null,
 "object_position": null,
 "order": null,
 "overflow": null,
 "overflow_x": null,
 "overflow_y": null,
 "padding": null,
 "right": null,
 "top": null,
 "visibility": null,
 "width": null
 }
 },
 "c89c7679ff6f49b68f74c69c3f83f5fd": {
 "model_module": "@jupyter-widgets/controls",
 "model_name": "DescriptionStyleModel",
 "model_module_version": "1.5.0",
 "state": {
 "_model_module": "@jupyter-widgets/controls",
 "_model_module_version": "1.5.0",
 "_model_name": "DescriptionStyleModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/base",
 "_view_module_version": "1.2.0",
 "_view_name": "StyleView",
 "description_width": ""
 }
 },
 "91ca86dfc9cc43e1a57c52cd4497ffff": {
 "model_module": "@jupyter-widgets/controls",
 "model_name": "HBoxModel",
 "model_module_version": "1.5.0",
 "state": {
 "_dom_classes": [],
 "_model_module": "@jupyter-widgets/controls",
 "_model_module_version": "1.5.0",
 "_model_name": "HBoxModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/controls",
 "_view_module_version": "1.5.0",
 "_view_name": "HBoxView",
 "box_style": "",
 "children": [
 "IPY_MODEL_a7fb7ff1eb4b45f6ac4809de26571501",
 "IPY_MODEL_98be4a3335b9498fb1c733b788d54c3a",
 "IPY_MODEL_cb5d005f6e0447cc83b1411fefd946ac"
 ],
 "layout": "IPY_MODEL_3d5bd0d94f074ea6961a807be1546879"
 }
 },
 "a7fb7ff1eb4b45f6ac4809de26571501": {
 "model_module": "@jupyter-widgets/controls",
 "model_name": "HTMLModel",
 "model_module_version": "1.5.0",
 "state": {
 "_dom_classes": [],
 "_model_module": "@jupyter-widgets/controls",
 "_model_module_version": "1.5.0",
 "_model_name": "HTMLModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/controls",
 "_view_module_version": "1.5.0",
 "_view_name": "HTMLView",
 "description": "",
 "description_tooltip": null,
 "layout": "IPY_MODEL_20f68ce78355439aabd546b941874e30",
 "placeholder": "\u200b",
 "style": "IPY_MODEL_20d713e5a9db4bb780887452ec2bc0aa",
 "value": "vocab.txt:\u2007"
 }
 },
 "98be4a3335b9498fb1c733b788d54c3a": {
 "model_module": "@jupyter-widgets/controls",
 "model_name": "FloatProgressModel",
 "model_module_version": "1.5.0",
 "state": {
 "_dom_classes": [],
 "_model_module": "@jupyter-widgets/controls",
 "_model_module_version": "1.5.0",
 "_model_name": "FloatProgressModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/controls",
 "_view_module_version": "1.5.0",
 "_view_name": "ProgressView",
 "bar_style": "success",
 "description": "",
 "description_tooltip": null,
 "layout": "IPY_MODEL_dd04d568ef0947e39541e4b050b18827",
 "max": 1,
 "min": 0,
 "orientation": "horizontal",
 "style": "IPY_MODEL_20cedcf0b00c46efb914a73fd8b9486f",
 "value": 1
 }
 },
 "cb5d005f6e0447cc83b1411fefd946ac": {
 "model_module": "@jupyter-widgets/controls",
 "model_name": "HTMLModel",
 "model_module_version": "1.5.0",
 "state": {
 "_dom_classes": [],
 "_model_module": "@jupyter-widgets/controls",
 "_model_module_version": "1.5.0",
 "_model_name": "HTMLModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/controls",
 "_view_module_version": "1.5.0",
 "_view_name": "HTMLView",
 "description": "",
 "description_tooltip": null,
 "layout": "IPY_MODEL_f25ea1ffd62a41e189ac085b8c499d05",
 "placeholder": "\u200b",
 "style": "IPY_MODEL_17610b9eddcb4d129e98431c207d2857",
 "value": "\u2007232k/?\u2007[00:00&lt;00:00,\u20076.15MB/s]"
 }
 },
 "3d5bd0d94f074ea6961a807be1546879": {
 "model_module": "@jupyter-widgets/base",
 "model_name": "LayoutModel",
 "model_module_version": "1.2.0",
 "state": {
 "_model_module": "@jupyter-widgets/base",
 "_model_module_version": "1.2.0",
 "_model_name": "LayoutModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/base",
 "_view_module_version": "1.2.0",
 "_view_name": "LayoutView",
 "align_content": null,
 "align_items": null,
 "align_self": null,
 "border": null,
 "bottom": null,
 "display": null,
 "flex": null,
 "flex_flow": null,
 "grid_area": null,
 "grid_auto_columns": null,
 "grid_auto_flow": null,
 "grid_auto_rows": null,
 "grid_column": null,
 "grid_gap": null,
 "grid_row": null,
 "grid_template_areas": null,
 "grid_template_columns": null,
 "grid_template_rows": null,
 "height": null,
 "justify_content": null,
 "justify_items": null,
 "left": null,
 "margin": null,
 "max_height": null,
 "max_width": null,
 "min_height": null,
 "min_width": null,
 "object_fit": null,
 "object_position": null,
 "order": null,
 "overflow": null,
 "overflow_x": null,
 "overflow_y": null,
 "padding": null,
 "right": null,
 "top": null,
 "visibility": null,
 "width": null
 }
 },
 "20f68ce78355439aabd546b941874e30": {
 "model_module": "@jupyter-widgets/base",
 "model_name": "LayoutModel",
 "model_module_version": "1.2.0",
 "state": {
 "_model_module": "@jupyter-widgets/base",
 "_model_module_version": "1.2.0",
 "_model_name": "LayoutModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/base",
 "_view_module_version": "1.2.0",
 "_view_name": "LayoutView",
 "align_content": null,
 "align_items": null,
 "align_self": null,
 "border": null,
 "bottom": null,
 "display": null,
 "flex": null,
 "flex_flow": null,
 "grid_area": null,
 "grid_auto_columns": null,
 "grid_auto_flow": null,
 "grid_auto_rows": null,
 "grid_column": null,
 "grid_gap": null,
 "grid_row": null,
 "grid_template_areas": null,
 "grid_template_columns": null,
 "grid_template_rows": null,
 "height": null,
 "justify_content": null,
 "justify_items": null,
 "left": null,
 "margin": null,
 "max_height": null,
 "max_width": null,
 "min_height": null,
 "min_width": null,
 "object_fit": null,
 "object_position": null,
 "order": null,
 "overflow": null,
 "overflow_x": null,
 "overflow_y": null,
 "padding": null,
 "right": null,
 "top": null,
 "visibility": null,
 "width": null
 }
 },
 "20d713e5a9db4bb780887452ec2bc0aa": {
 "model_module": "@jupyter-widgets/controls",
 "model_name": "DescriptionStyleModel",
 "model_module_version": "1.5.0",
 "state": {
 "_model_module": "@jupyter-widgets/controls",
 "_model_module_version": "1.5.0",
 "_model_name": "DescriptionStyleModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/base",
 "_view_module_version": "1.2.0",
 "_view_name": "StyleView",
 "description_width": ""
 }
 },
 "dd04d568ef0947e39541e4b050b18827": {
 "model_module": "@jupyter-widgets/base",
 "model_name": "LayoutModel",
 "model_module_version": "1.2.0",
 "state": {
 "_model_module": "@jupyter-widgets/base",
 "_model_module_version": "1.2.0",
 "_model_name": "LayoutModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/base",
 "_view_module_version": "1.2.0",
 "_view_name": "LayoutView",
 "align_content": null,
 "align_items": null,
 "align_self": null,
 "border": null,
 "bottom": null,
 "display": null,
 "flex": null,
 "flex_flow": null,
 "grid_area": null,
 "grid_auto_columns": null,
 "grid_auto_flow": null,
 "grid_auto_rows": null,
 "grid_column": null,
 "grid_gap": null,
 "grid_row": null,
 "grid_template_areas": null,
 "grid_template_columns": null,
 "grid_template_rows": null,
 "height": null,
 "justify_content": null,
 "justify_items": null,
 "left": null,
 "margin": null,
 "max_height": null,
 "max_width": null,
 "min_height": null,
 "min_width": null,
 "object_fit": null,
 "object_position": null,
 "order": null,
 "overflow": null,
 "overflow_x": null,
 "overflow_y": null,
 "padding": null,
 "right": null,
 "top": null,
 "visibility": null,
 "width": "20px"
 }
 },
 "20cedcf0b00c46efb914a73fd8b9486f": {
 "model_module": "@jupyter-widgets/controls",
 "model_name": "ProgressStyleModel",
 "model_module_version": "1.5.0",
 "state": {
 "_model_module": "@jupyter-widgets/controls",
 "_model_module_version": "1.5.0",
 "_model_name": "ProgressStyleModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/base",
 "_view_module_version": "1.2.0",
 "_view_name": "StyleView",
 "bar_color": null,
 "description_width": ""
 }
 },
 "f25ea1ffd62a41e189ac085b8c499d05": {
 "model_module": "@jupyter-widgets/base",
 "model_name": "LayoutModel",
 "model_module_version": "1.2.0",
 "state": {
 "_model_module": "@jupyter-widgets/base",
 "_model_module_version": "1.2.0",
 "_model_name": "LayoutModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/base",
 "_view_module_version": "1.2.0",
 "_view_name": "LayoutView",
 "align_content": null,
 "align_items": null,
 "align_self": null,
 "border": null,
 "bottom": null,
 "display": null,
 "flex": null,
 "flex_flow": null,
 "grid_area": null,
 "grid_auto_columns": null,
 "grid_auto_flow": null,
 "grid_auto_rows": null,
 "grid_column": null,
 "grid_gap": null,
 "grid_row": null,
 "grid_template_areas": null,
 "grid_template_columns": null,
 "grid_template_rows": null,
 "height": null,
 "justify_content": null,
 "justify_items": null,
 "left": null,
 "margin": null,
 "max_height": null,
 "max_width": null,
 "min_height": null,
 "min_width": null,
 "object_fit": null,
 "object_position": null,
 "order": null,
 "overflow": null,
 "overflow_x": null,
 "overflow_y": null,
 "padding": null,
 "right": null,
 "top": null,
 "visibility": null,
 "width": null
 }
 },
 "17610b9eddcb4d129e98431c207d2857": {
 "model_module": "@jupyter-widgets/controls",
 "model_name": "DescriptionStyleModel",
 "model_module_version": "1.5.0",
 "state": {
 "_model_module": "@jupyter-widgets/controls",
 "_model_module_version": "1.5.0",
 "_model_name": "DescriptionStyleModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/base",
 "_view_module_version": "1.2.0",
 "_view_name": "StyleView",
 "description_width": ""
 }
 },
 "6115424c97964a428ff376996d0a73e7": {
 "model_module": "@jupyter-widgets/controls",
 "model_name": "HBoxModel",
 "model_module_version": "1.5.0",
 "state": {
 "_dom_classes": [],
 "_model_module": "@jupyter-widgets/controls",
 "_model_module_version": "1.5.0",
 "_model_name": "HBoxModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/controls",
 "_view_module_version": "1.5.0",
 "_view_name": "HBoxView",
 "box_style": "",
 "children": [
 "IPY_MODEL_2d333e6a840e454db46af4c8bbd29db7",
 "IPY_MODEL_8f27e171ad1a479bac16128bdc16d8df",
 "IPY_MODEL_66de1b42f10540909189d2272b2a2faf"
 ],
 "layout": "IPY_MODEL_2aef73c06982482daa972cebe034d1d6"
 }
 },
 "2d333e6a840e454db46af4c8bbd29db7": {
 "model_module": "@jupyter-widgets/controls",
 "model_name": "HTMLModel",
 "model_module_version": "1.5.0",
 "state": {
 "_dom_classes": [],
 "_model_module": "@jupyter-widgets/controls",
 "_model_module_version": "1.5.0",
 "_model_name": "HTMLModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/controls",
 "_view_module_version": "1.5.0",
 "_view_name": "HTMLView",
 "description": "",
 "description_tooltip": null,
 "layout": "IPY_MODEL_0d446c3b8d2543138881265c53c18bbd",
 "placeholder": "\u200b",
 "style": "IPY_MODEL_4635344ba7b544a48d316430a83561fe",
 "value": "tokenizer.json:\u2007"
 }
 },
 "8f27e171ad1a479bac16128bdc16d8df": {
 "model_module": "@jupyter-widgets/controls",
 "model_name": "FloatProgressModel",
 "model_module_version": "1.5.0",
 "state": {
 "_dom_classes": [],
 "_model_module": "@jupyter-widgets/controls",
 "_model_module_version": "1.5.0",
 "_model_name": "FloatProgressModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/controls",
 "_view_module_version": "1.5.0",
 "_view_name": "ProgressView",
 "bar_style": "success",
 "description": "",
 "description_tooltip": null,
 "layout": "IPY_MODEL_7d27e6951a634c57bc1e8bb43d687ba0",
 "max": 1,
 "min": 0,
 "orientation": "horizontal",
 "style": "IPY_MODEL_45ee81ecf302466cb5945805f0b67ace",
 "value": 1
 }
 },
 "66de1b42f10540909189d2272b2a2faf": {
 "model_module": "@jupyter-widgets/controls",
 "model_name": "HTMLModel",
 "model_module_version": "1.5.0",
 "state": {
 "_dom_classes": [],
 "_model_module": "@jupyter-widgets/controls",
 "_model_module_version": "1.5.0",
 "_model_name": "HTMLModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/controls",
 "_view_module_version": "1.5.0",
 "_view_name": "HTMLView",
 "description": "",
 "description_tooltip": null,
 "layout": "IPY_MODEL_53a89fef595a48e8bd49078df695fcd8",
 "placeholder": "\u200b",
 "style": "IPY_MODEL_b896cdf3a1734a92badb779887124937",
 "value": "\u2007711k/?\u2007[00:00&lt;00:00,\u2007907kB/s]"
 }
 },
 "2aef73c06982482daa972cebe034d1d6": {
 "model_module": "@jupyter-widgets/base",
 "model_name": "LayoutModel",
 "model_module_version": "1.2.0",
 "state": {
 "_model_module": "@jupyter-widgets/base",
 "_model_module_version": "1.2.0",
 "_model_name": "LayoutModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/base",
 "_view_module_version": "1.2.0",
 "_view_name": "LayoutView",
 "align_content": null,
 "align_items": null,
 "align_self": null,
 "border": null,
 "bottom": null,
 "display": null,
 "flex": null,
 "flex_flow": null,
 "grid_area": null,
 "grid_auto_columns": null,
 "grid_auto_flow": null,
 "grid_auto_rows": null,
 "grid_column": null,
 "grid_gap": null,
 "grid_row": null,
 "grid_template_areas": null,
 "grid_template_columns": null,
 "grid_template_rows": null,
 "height": null,
 "justify_content": null,
 "justify_items": null,
 "left": null,
 "margin": null,
 "max_height": null,
 "max_width": null,
 "min_height": null,
 "min_width": null,
 "object_fit": null,
 "object_position": null,
 "order": null,
 "overflow": null,
 "overflow_x": null,
 "overflow_y": null,
 "padding": null,
 "right": null,
 "top": null,
 "visibility": null,
 "width": null
 }
 },
 "0d446c3b8d2543138881265c53c18bbd": {
 "model_module": "@jupyter-widgets/base",
 "model_name": "LayoutModel",
 "model_module_version": "1.2.0",
 "state": {
 "_model_module": "@jupyter-widgets/base",
 "_model_module_version": "1.2.0",
 "_model_name": "LayoutModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/base",
 "_view_module_version": "1.2.0",
 "_view_name": "LayoutView",
 "align_content": null,
 "align_items": null,
 "align_self": null,
 "border": null,
 "bottom": null,
 "display": null,
 "flex": null,
 "flex_flow": null,
 "grid_area": null,
 "grid_auto_columns": null,
 "grid_auto_flow": null,
 "grid_auto_rows": null,
 "grid_column": null,
 "grid_gap": null,
 "grid_row": null,
 "grid_template_areas": null,
 "grid_template_columns": null,
 "grid_template_rows": null,
 "height": null,
 "justify_content": null,
 "justify_items": null,
 "left": null,
 "margin": null,
 "max_height": null,
 "max_width": null,
 "min_height": null,
 "min_width": null,
 "object_fit": null,
 "object_position": null,
 "order": null,
 "overflow": null,
 "overflow_x": null,
 "overflow_y": null,
 "padding": null,
 "right": null,
 "top": null,
 "visibility": null,
 "width": null
 }
 },
 "4635344ba7b544a48d316430a83561fe": {
 "model_module": "@jupyter-widgets/controls",
 "model_name": "DescriptionStyleModel",
 "model_module_version": "1.5.0",
 "state": {
 "_model_module": "@jupyter-widgets/controls",
 "_model_module_version": "1.5.0",
 "_model_name": "DescriptionStyleModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/base",
 "_view_module_version": "1.2.0",
 "_view_name": "StyleView",
 "description_width": ""
 }
 },
 "7d27e6951a634c57bc1e8bb43d687ba0": {
 "model_module": "@jupyter-widgets/base",
 "model_name": "LayoutModel",
 "model_module_version": "1.2.0",
 "state": {
 "_model_module": "@jupyter-widgets/base",
 "_model_module_version": "1.2.0",
 "_model_name": "LayoutModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/base",
 "_view_module_version": "1.2.0",
 "_view_name": "LayoutView",
 "align_content": null,
 "align_items": null,
 "align_self": null,
 "border": null,
 "bottom": null,
 "display": null,
 "flex": null,
 "flex_flow": null,
 "grid_area": null,
 "grid_auto_columns": null,
 "grid_auto_flow": null,
 "grid_auto_rows": null,
 "grid_column": null,
 "grid_gap": null,
 "grid_row": null,
 "grid_template_areas": null,
 "grid_template_columns": null,
 "grid_template_rows": null,
 "height": null,
 "justify_content": null,
 "justify_items": null,
 "left": null,
 "margin": null,
 "max_height": null,
 "max_width": null,
 "min_height": null,
 "min_width": null,
 "object_fit": null,
 "object_position": null,
 "order": null,
 "overflow": null,
 "overflow_x": null,
 "overflow_y": null,
 "padding": null,
 "right": null,
 "top": null,
 "visibility": null,
 "width": "20px"
 }
 },
 "45ee81ecf302466cb5945805f0b67ace": {
 "model_module": "@jupyter-widgets/controls",
 "model_name": "ProgressStyleModel",
 "model_module_version": "1.5.0",
 "state": {
 "_model_module": "@jupyter-widgets/controls",
 "_model_module_version": "1.5.0",
 "_model_name": "ProgressStyleModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/base",
 "_view_module_version": "1.2.0",
 "_view_name": "StyleView",
 "bar_color": null,
 "description_width": ""
 }
 },
 "53a89fef595a48e8bd49078df695fcd8": {
 "model_module": "@jupyter-widgets/base",
 "model_name": "LayoutModel",
 "model_module_version": "1.2.0",
 "state": {
 "_model_module": "@jupyter-widgets/base",
 "_model_module_version": "1.2.0",
 "_model_name": "LayoutModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/base",
 "_view_module_version": "1.2.0",
 "_view_name": "LayoutView",
 "align_content": null,
 "align_items": null,
 "align_self": null,
 "border": null,
 "bottom": null,
 "display": null,
 "flex": null,
 "flex_flow": null,
 "grid_area": null,
 "grid_auto_columns": null,
 "grid_auto_flow": null,
 "grid_auto_rows": null,
 "grid_column": null,
 "grid_gap": null,
 "grid_row": null,
 "grid_template_areas": null,
 "grid_template_columns": null,
 "grid_template_rows": null,
 "height": null,
 "justify_content": null,
 "justify_items": null,
 "left": null,
 "margin": null,
 "max_height": null,
 "max_width": null,
 "min_height": null,
 "min_width": null,
 "object_fit": null,
 "object_position": null,
 "order": null,
 "overflow": null,
 "overflow_x": null,
 "overflow_y": null,
 "padding": null,
 "right": null,
 "top": null,
 "visibility": null,
 "width": null
 }
 },
 "b896cdf3a1734a92badb779887124937": {
 "model_module": "@jupyter-widgets/controls",
 "model_name": "DescriptionStyleModel",
 "model_module_version": "1.5.0",
 "state": {
 "_model_module": "@jupyter-widgets/controls",
 "_model_module_version": "1.5.0",
 "_model_name": "DescriptionStyleModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/base",
 "_view_module_version": "1.2.0",
 "_view_name": "StyleView",
 "description_width": ""
 }
 },
 "773564879b9547f68507ea70afd3ade1": {
 "model_module": "@jupyter-widgets/controls",
 "model_name": "HBoxModel",
 "model_module_version": "1.5.0",
 "state": {
 "_dom_classes": [],
 "_model_module": "@jupyter-widgets/controls",
 "_model_module_version": "1.5.0",
 "_model_name": "HBoxModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/controls",
 "_view_module_version": "1.5.0",
 "_view_name": "HBoxView",
 "box_style": "",
 "children": [
 "IPY_MODEL_a41262494161468da21eb37e4be29834",
 "IPY_MODEL_3ecc899750dd4ed9bba6000b782ed098",
 "IPY_MODEL_372c94f6cbab4bb89010d560466c9a69"
 ],
 "layout": "IPY_MODEL_605a21593b664064a92a0aba9515c8fa"
 }
 },
 "a41262494161468da21eb37e4be29834": {
 "model_module": "@jupyter-widgets/controls",
 "model_name": "HTMLModel",
 "model_module_version": "1.5.0",
 "state": {
 "_dom_classes": [],
 "_model_module": "@jupyter-widgets/controls",
 "_model_module_version": "1.5.0",
 "_model_name": "HTMLModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/controls",
 "_view_module_version": "1.5.0",
 "_view_name": "HTMLView",
 "description": "",
 "description_tooltip": null,
 "layout": "IPY_MODEL_0ac595c42f9b4f3d9593c5d42c4bd7f1",
 "placeholder": "\u200b",
 "style": "IPY_MODEL_783832a7ab054d80821f94870b618390",
 "value": "special_tokens_map.json:\u2007100%"
 }
 },
 "3ecc899750dd4ed9bba6000b782ed098": {
 "model_module": "@jupyter-widgets/controls",
 "model_name": "FloatProgressModel",
 "model_module_version": "1.5.0",
 "state": {
 "_dom_classes": [],
 "_model_module": "@jupyter-widgets/controls",
 "_model_module_version": "1.5.0",
 "_model_name": "FloatProgressModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/controls",
 "_view_module_version": "1.5.0",
 "_view_name": "ProgressView",
 "bar_style": "success",
 "description": "",
 "description_tooltip": null,
 "layout": "IPY_MODEL_2c47eb1bbce541e495eec74f3234e1b3",
 "max": 125,
 "min": 0,
 "orientation": "horizontal",
 "style": "IPY_MODEL_08318bd91211495cbba8b9255437c3f0",
 "value": 125
 }
 },
 "372c94f6cbab4bb89010d560466c9a69": {
 "model_module": "@jupyter-widgets/controls",
 "model_name": "HTMLModel",
 "model_module_version": "1.5.0",
 "state": {
 "_dom_classes": [],
 "_model_module": "@jupyter-widgets/controls",
 "_model_module_version": "1.5.0",
 "_model_name": "HTMLModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/controls",
 "_view_module_version": "1.5.0",
 "_view_name": "HTMLView",
 "description": "",
 "description_tooltip": null,
 "layout": "IPY_MODEL_7e9feee5e5d44417a4f2d42bb154df83",
 "placeholder": "\u200b",
 "style": "IPY_MODEL_aca61873d04d4b34bb4a8aae1e61a488",
 "value": "\u2007125/125\u2007[00:00&lt;00:00,\u200713.9kB/s]"
 }
 },
 "605a21593b664064a92a0aba9515c8fa": {
 "model_module": "@jupyter-widgets/base",
 "model_name": "LayoutModel",
 "model_module_version": "1.2.0",
 "state": {
 "_model_module": "@jupyter-widgets/base",
 "_model_module_version": "1.2.0",
 "_model_name": "LayoutModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/base",
 "_view_module_version": "1.2.0",
 "_view_name": "LayoutView",
 "align_content": null,
 "align_items": null,
 "align_self": null,
 "border": null,
 "bottom": null,
 "display": null,
 "flex": null,
 "flex_flow": null,
 "grid_area": null,
 "grid_auto_columns": null,
 "grid_auto_flow": null,
 "grid_auto_rows": null,
 "grid_column": null,
 "grid_gap": null,
 "grid_row": null,
 "grid_template_areas": null,
 "grid_template_columns": null,
 "grid_template_rows": null,
 "height": null,
 "justify_content": null,
 "justify_items": null,
 "left": null,
 "margin": null,
 "max_height": null,
 "max_width": null,
 "min_height": null,
 "min_width": null,
 "object_fit": null,
 "object_position": null,
 "order": null,
 "overflow": null,
 "overflow_x": null,
 "overflow_y": null,
 "padding": null,
 "right": null,
 "top": null,
 "visibility": null,
 "width": null
 }
 },
 "0ac595c42f9b4f3d9593c5d42c4bd7f1": {
 "model_module": "@jupyter-widgets/base",
 "model_name": "LayoutModel",
 "model_module_version": "1.2.0",
 "state": {
 "_model_module": "@jupyter-widgets/base",
 "_model_module_version": "1.2.0",
 "_model_name": "LayoutModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/base",
 "_view_module_version": "1.2.0",
 "_view_name": "LayoutView",
 "align_content": null,
 "align_items": null,
 "align_self": null,
 "border": null,
 "bottom": null,
 "display": null,
 "flex": null,
 "flex_flow": null,
 "grid_area": null,
 "grid_auto_columns": null,
 "grid_auto_flow": null,
 "grid_auto_rows": null,
 "grid_column": null,
 "grid_gap": null,
 "grid_row": null,
 "grid_template_areas": null,
 "grid_template_columns": null,
 "grid_template_rows": null,
 "height": null,
 "justify_content": null,
 "justify_items": null,
 "left": null,
 "margin": null,
 "max_height": null,
 "max_width": null,
 "min_height": null,
 "min_width": null,
 "object_fit": null,
 "object_position": null,
 "order": null,
 "overflow": null,
 "overflow_x": null,
 "overflow_y": null,
 "padding": null,
 "right": null,
 "top": null,
 "visibility": null,
 "width": null
 }
 },
 "783832a7ab054d80821f94870b618390": {
 "model_module": "@jupyter-widgets/controls",
 "model_name": "DescriptionStyleModel",
 "model_module_version": "1.5.0",
 "state": {
 "_model_module": "@jupyter-widgets/controls",
 "_model_module_version": "1.5.0",
 "_model_name": "DescriptionStyleModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/base",
 "_view_module_version": "1.2.0",
 "_view_name": "StyleView",
 "description_width": ""
 }
 },
 "2c47eb1bbce541e495eec74f3234e1b3": {
 "model_module": "@jupyter-widgets/base",
 "model_name": "LayoutModel",
 "model_module_version": "1.2.0",
 "state": {
 "_model_module": "@jupyter-widgets/base",
 "_model_module_version": "1.2.0",
 "_model_name": "LayoutModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/base",
 "_view_module_version": "1.2.0",
 "_view_name": "LayoutView",
 "align_content": null,
 "align_items": null,
 "align_self": null,
 "border": null,
 "bottom": null,
 "display": null,
 "flex": null,
 "flex_flow": null,
 "grid_area": null,
 "grid_auto_columns": null,
 "grid_auto_flow": null,
 "grid_auto_rows": null,
 "grid_column": null,
 "grid_gap": null,
 "grid_row": null,
 "grid_template_areas": null,
 "grid_template_columns": null,
 "grid_template_rows": null,
 "height": null,
 "justify_content": null,
 "justify_items": null,
 "left": null,
 "margin": null,
 "max_height": null,
 "max_width": null,
 "min_height": null,
 "min_width": null,
 "object_fit": null,
 "object_position": null,
 "order": null,
 "overflow": null,
 "overflow_x": null,
 "overflow_y": null,
 "padding": null,
 "right": null,
 "top": null,
 "visibility": null,
 "width": null
 }
 },
 "08318bd91211495cbba8b9255437c3f0": {
 "model_module": "@jupyter-widgets/controls",
 "model_name": "ProgressStyleModel",
 "model_module_version": "1.5.0",
 "state": {
 "_model_module": "@jupyter-widgets/controls",
 "_model_module_version": "1.5.0",
 "_model_name": "ProgressStyleModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/base",
 "_view_module_version": "1.2.0",
 "_view_name": "StyleView",
 "bar_color": null,
 "description_width": ""
 }
 },
 "7e9feee5e5d44417a4f2d42bb154df83": {
 "model_module": "@jupyter-widgets/base",
 "model_name": "LayoutModel",
 "model_module_version": "1.2.0",
 "state": {
 "_model_module": "@jupyter-widgets/base",
 "_model_module_version": "1.2.0",
 "_model_name": "LayoutModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/base",
 "_view_module_version": "1.2.0",
 "_view_name": "LayoutView",
 "align_content": null,
 "align_items": null,
 "align_self": null,
 "border": null,
 "bottom": null,
 "display": null,
 "flex": null,
 "flex_flow": null,
 "grid_area": null,
 "grid_auto_columns": null,
 "grid_auto_flow": null,
 "grid_auto_rows": null,
 "grid_column": null,
 "grid_gap": null,
 "grid_row": null,
 "grid_template_areas": null,
 "grid_template_columns": null,
 "grid_template_rows": null,
 "height": null,
 "justify_content": null,
 "justify_items": null,
 "left": null,
 "margin": null,
 "max_height": null,
 "max_width": null,
 "min_height": null,
 "min_width": null,
 "object_fit": null,
 "object_position": null,
 "order": null,
 "overflow": null,
 "overflow_x": null,
 "overflow_y": null,
 "padding": null,
 "right": null,
 "top": null,
 "visibility": null,
 "width": null
 }
 },
 "aca61873d04d4b34bb4a8aae1e61a488": {
 "model_module": "@jupyter-widgets/controls",
 "model_name": "DescriptionStyleModel",
 "model_module_version": "1.5.0",
 "state": {
 "_model_module": "@jupyter-widgets/controls",
 "_model_module_version": "1.5.0",
 "_model_name": "DescriptionStyleModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/base",
 "_view_module_version": "1.2.0",
 "_view_name": "StyleView",
 "description_width": ""
 }
 },
 "a3a38c48bd2945df9bf662a0af492368": {
 "model_module": "@jupyter-widgets/controls",
 "model_name": "HBoxModel",
 "model_module_version": "1.5.0",
 "state": {
 "_dom_classes": [],
 "_model_module": "@jupyter-widgets/controls",
 "_model_module_version": "1.5.0",
 "_model_name": "HBoxModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/controls",
 "_view_module_version": "1.5.0",
 "_view_name": "HBoxView",
 "box_style": "",
 "children": [
 "IPY_MODEL_ce51174b09784dd8be3652afffacf760",
 "IPY_MODEL_959b26a9bb2642f5ac8d73020797124a",
 "IPY_MODEL_481f7ca0ad0a4d11bcb04699ef6c2bfb"
 ],
 "layout": "IPY_MODEL_1c65d759114941a18fa7063512277c61"
 }
 },
 "ce51174b09784dd8be3652afffacf760": {
 "model_module": "@jupyter-widgets/controls",
 "model_name": "HTMLModel",
 "model_module_version": "1.5.0",
 "state": {
 "_dom_classes": [],
 "_model_module": "@jupyter-widgets/controls",
 "_model_module_version": "1.5.0",
 "_model_name": "HTMLModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/controls",
 "_view_module_version": "1.5.0",
 "_view_name": "HTMLView",
 "description": "",
 "description_tooltip": null,
 "layout": "IPY_MODEL_1c54547301934daeb1b007ac8c39b97b",
 "placeholder": "\u200b",
 "style": "IPY_MODEL_6d6e27e9fa6143fb8932f504102dab0b",
 "value": "config.json:\u2007"
 }
 },
 "959b26a9bb2642f5ac8d73020797124a": {
 "model_module": "@jupyter-widgets/controls",
 "model_name": "FloatProgressModel",
 "model_module_version": "1.5.0",
 "state": {
 "_dom_classes": [],
 "_model_module": "@jupyter-widgets/controls",
 "_model_module_version": "1.5.0",
 "_model_name": "FloatProgressModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/controls",
 "_view_module_version": "1.5.0",
 "_view_name": "ProgressView",
 "bar_style": "success",
 "description": "",
 "description_tooltip": null,
 "layout": "IPY_MODEL_d15dde1a9bf647ae9a0d0db980ee2706",
 "max": 1,
 "min": 0,
 "orientation": "horizontal",
 "style": "IPY_MODEL_5f3d9fc72250403a9041a2b3b71bd84f",
 "value": 1
 }
 },
 "481f7ca0ad0a4d11bcb04699ef6c2bfb": {
 "model_module": "@jupyter-widgets/controls",
 "model_name": "HTMLModel",
 "model_module_version": "1.5.0",
 "state": {
 "_dom_classes": [],
 "_model_module": "@jupyter-widgets/controls",
 "_model_module_version": "1.5.0",
 "_model_name": "HTMLModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/controls",
 "_view_module_version": "1.5.0",
 "_view_name": "HTMLView",
 "description": "",
 "description_tooltip": null,
 "layout": "IPY_MODEL_98b65dc6a6f24234ad3443fb2b8e596f",
 "placeholder": "\u200b",
 "style": "IPY_MODEL_2d615bd1ba5846fbbfb07e501b979c5f",
 "value": "\u20074.56k/?\u2007[00:00&lt;00:00,\u2007432kB/s]"
 }
 },
 "1c65d759114941a18fa7063512277c61": {
 "model_module": "@jupyter-widgets/base",
 "model_name": "LayoutModel",
 "model_module_version": "1.2.0",
 "state": {
 "_model_module": "@jupyter-widgets/base",
 "_model_module_version": "1.2.0",
 "_model_name": "LayoutModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/base",
 "_view_module_version": "1.2.0",
 "_view_name": "LayoutView",
 "align_content": null,
 "align_items": null,
 "align_self": null,
 "border": null,
 "bottom": null,
 "display": null,
 "flex": null,
 "flex_flow": null,
 "grid_area": null,
 "grid_auto_columns": null,
 "grid_auto_flow": null,
 "grid_auto_rows": null,
 "grid_column": null,
 "grid_gap": null,
 "grid_row": null,
 "grid_template_areas": null,
 "grid_template_columns": null,
 "grid_template_rows": null,
 "height": null,
 "justify_content": null,
 "justify_items": null,
 "left": null,
 "margin": null,
 "max_height": null,
 "max_width": null,
 "min_height": null,
 "min_width": null,
 "object_fit": null,
 "object_position": null,
 "order": null,
 "overflow": null,
 "overflow_x": null,
 "overflow_y": null,
 "padding": null,
 "right": null,
 "top": null,
 "visibility": null,
 "width": null
 }
 },
 "1c54547301934daeb1b007ac8c39b97b": {
 "model_module": "@jupyter-widgets/base",
 "model_name": "LayoutModel",
 "model_module_version": "1.2.0",
 "state": {
 "_model_module": "@jupyter-widgets/base",
 "_model_module_version": "1.2.0",
 "_model_name": "LayoutModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/base",
 "_view_module_version": "1.2.0",
 "_view_name": "LayoutView",
 "align_content": null,
 "align_items": null,
 "align_self": null,
 "border": null,
 "bottom": null,
 "display": null,
 "flex": null,
 "flex_flow": null,
 "grid_area": null,
 "grid_auto_columns": null,
 "grid_auto_flow": null,
 "grid_auto_rows": null,
 "grid_column": null,
 "grid_gap": null,
 "grid_row": null,
 "grid_template_areas": null,
 "grid_template_columns": null,
 "grid_template_rows": null,
 "height": null,
 "justify_content": null,
 "justify_items": null,
 "left": null,
 "margin": null,
 "max_height": null,
 "max_width": null,
 "min_height": null,
 "min_width": null,
 "object_fit": null,
 "object_position": null,
 "order": null,
 "overflow": null,
 "overflow_x": null,
 "overflow_y": null,
 "padding": null,
 "right": null,
 "top": null,
 "visibility": null,
 "width": null
 }
 },
 "6d6e27e9fa6143fb8932f504102dab0b": {
 "model_module": "@jupyter-widgets/controls",
 "model_name": "DescriptionStyleModel",
 "model_module_version": "1.5.0",
 "state": {
 "_model_module": "@jupyter-widgets/controls",
 "_model_module_version": "1.5.0",
 "_model_name": "DescriptionStyleModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/base",
 "_view_module_version": "1.2.0",
 "_view_name": "StyleView",
 "description_width": ""
 }
 },
 "d15dde1a9bf647ae9a0d0db980ee2706": {
 "model_module": "@jupyter-widgets/base",
 "model_name": "LayoutModel",
 "model_module_version": "1.2.0",
 "state": {
 "_model_module": "@jupyter-widgets/base",
 "_model_module_version": "1.2.0",
 "_model_name": "LayoutModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/base",
 "_view_module_version": "1.2.0",
 "_view_name": "LayoutView",
 "align_content": null,
 "align_items": null,
 "align_self": null,
 "border": null,
 "bottom": null,
 "display": null,
 "flex": null,
 "flex_flow": null,
 "grid_area": null,
 "grid_auto_columns": null,
 "grid_auto_flow": null,
 "grid_auto_rows": null,
 "grid_column": null,
 "grid_gap": null,
 "grid_row": null,
 "grid_template_areas": null,
 "grid_template_columns": null,
 "grid_template_rows": null,
 "height": null,
 "justify_content": null,
 "justify_items": null,
 "left": null,
 "margin": null,
 "max_height": null,
 "max_width": null,
 "min_height": null,
 "min_width": null,
 "object_fit": null,
 "object_position": null,
 "order": null,
 "overflow": null,
 "overflow_x": null,
 "overflow_y": null,
 "padding": null,
 "right": null,
 "top": null,
 "visibility": null,
 "width": "20px"
 }
 },
 "5f3d9fc72250403a9041a2b3b71bd84f": {
 "model_module": "@jupyter-widgets/controls",
 "model_name": "ProgressStyleModel",
 "model_module_version": "1.5.0",
 "state": {
 "_model_module": "@jupyter-widgets/controls",
 "_model_module_version": "1.5.0",
 "_model_name": "ProgressStyleModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/base",
 "_view_module_version": "1.2.0",
 "_view_name": "StyleView",
 "bar_color": null,
 "description_width": ""
 }
 },
 "98b65dc6a6f24234ad3443fb2b8e596f": {
 "model_module": "@jupyter-widgets/base",
 "model_name": "LayoutModel",
 "model_module_version": "1.2.0",
 "state": {
 "_model_module": "@jupyter-widgets/base",
 "_model_module_version": "1.2.0",
 "_model_name": "LayoutModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/base",
 "_view_module_version": "1.2.0",
 "_view_name": "LayoutView",
 "align_content": null,
 "align_items": null,
 "align_self": null,
 "border": null,
 "bottom": null,
 "display": null,
 "flex": null,
 "flex_flow": null,
 "grid_area": null,
 "grid_auto_columns": null,
 "grid_auto_flow": null,
 "grid_auto_rows": null,
 "grid_column": null,
 "grid_gap": null,
 "grid_row": null,
 "grid_template_areas": null,
 "grid_template_columns": null,
 "grid_template_rows": null,
 "height": null,
 "justify_content": null,
 "justify_items": null,
 "left": null,
 "margin": null,
 "max_height": null,
 "max_width": null,
 "min_height": null,
 "min_width": null,
 "object_fit": null,
 "object_position": null,
 "order": null,
 "overflow": null,
 "overflow_x": null,
 "overflow_y": null,
 "padding": null,
 "right": null,
 "top": null,
 "visibility": null,
 "width": null
 }
 },
 "2d615bd1ba5846fbbfb07e501b979c5f": {
 "model_module": "@jupyter-widgets/controls",
 "model_name": "DescriptionStyleModel",
 "model_module_version": "1.5.0",
 "state": {
 "_model_module": "@jupyter-widgets/controls",
 "_model_module_version": "1.5.0",
 "_model_name": "DescriptionStyleModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/base",
 "_view_module_version": "1.2.0",
 "_view_name": "StyleView",
 "description_width": ""
 }
 },
 "67d48cefb58d4c34b273b5fefc7b5184": {
 "model_module": "@jupyter-widgets/controls",
 "model_name": "HBoxModel",
 "model_module_version": "1.5.0",
 "state": {
 "_dom_classes": [],
 "_model_module": "@jupyter-widgets/controls",
 "_model_module_version": "1.5.0",
 "_model_name": "HBoxModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/controls",
 "_view_module_version": "1.5.0",
 "_view_name": "HBoxView",
 "box_style": "",
 "children": [
 "IPY_MODEL_725d7d76fd254ab4840e9b5ad92a29e8",
 "IPY_MODEL_f10bcc1c238c453e951bb0b19d98ceb9",
 "IPY_MODEL_59ad6a41060645e48bf1914cbe21d1cd"
 ],
 "layout": "IPY_MODEL_f39300ee357341ba8785a49ed94a5576"
 }
 },
 "725d7d76fd254ab4840e9b5ad92a29e8": {
 "model_module": "@jupyter-widgets/controls",
 "model_name": "HTMLModel",
 "model_module_version": "1.5.0",
 "state": {
 "_dom_classes": [],
 "_model_module": "@jupyter-widgets/controls",
 "_model_module_version": "1.5.0",
 "_model_name": "HTMLModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/controls",
 "_view_module_version": "1.5.0",
 "_view_name": "HTMLView",
 "description": "",
 "description_tooltip": null,
 "layout": "IPY_MODEL_0607a0a212bc4ec9964a1ab4d97f8",
 "placeholder": "\u200b",
 "style": "IPY_MODEL_e68ad490a3514755ad691bf7582b17b8",
 "value": "model.safetensors:\u2007100%"
 }
 },
 "f10bcc1c238c453e951bb0b19d98ceb9": {
 "model_module": "@jupyter-widgets/controls",
 "model_name": "FloatProgressModel",
 "model_module_version": "1.5.0",
 "state": {
 "_dom_classes": [],
 "_model_module": "@jupyter-widgets/controls",
 "_model_module_version": "1.5.0",
 "_model_name": "FloatProgressModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/controls",
 "_view_module_version": "1.5.0",
 "_view_name": "ProgressView",
 "bar_style": "success",
 "description": "",
 "description_tooltip": null,
 "layout": "IPY_MODEL_cb82c3c5c2314a0b92eca419ecbbd175",
 "max": 1538800584,
 "min": 0,
 "orientation": "horizontal",
 "style": "IPY_MODEL_84c9f040aa474f408b110dd314220f23",
 "value": 1538800584
 }
 },
 "59ad6a41060645e48bf1914cbe21d1cd": {
 "model_module": "@jupyter-widgets/controls",
 "model_name": "HTMLModel",
 "model_module_version": "1.5.0",
 "state": {
 "_dom_classes": [],
 "_model_module": "@jupyter-widgets/controls",
 "_model_module_version": "1.5.0",
 "_model_name": "HTMLModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/controls",
 "_view_module_version": "1.5.0",
 "_view_name": "HTMLView",
 "description": "",
 "description_tooltip": null,
 "layout": "IPY_MODEL_fe9ba760b16b49f2827a97d910bfb2cb",
 "placeholder": "\u200b",
 "style": "IPY_MODEL_98d5f73cca7e49958035c388bda68064",
 "value": "\u20071.54G/1.54G\u2007[00:36&lt;00:00,\u200771.7MB/s]"
 }
 },
 "f39300ee357341ba8785a49ed94a5576": {
 "model_module": "@jupyter-widgets/base",
 "model_name": "LayoutModel",
 "model_module_version": "1.2.0",
 "state": {
 "_model_module": "@jupyter-widgets/base",
 "_model_module_version": "1.2.0",
 "_model_name": "LayoutModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/base",
 "_view_module_version": "1.2.0",
 "_view_name": "LayoutView",
 "align_content": null,
 "align_items": null,
 "align_self": null,
 "border": null,
 "bottom": null,
 "display": null,
 "flex": null,
 "flex_flow": null,
 "grid_area": null,
 "grid_auto_columns": null,
 "grid_auto_flow": null,
 "grid_auto_rows": null,
 "grid_column": null,
 "grid_gap": null,
 "grid_row": null,
 "grid_template_areas": null,
 "grid_template_columns": null,
 "grid_template_rows": null,
 "height": null,
 "justify_content": null,
 "justify_items": null,
 "left": null,
 "margin": null,
 "max_height": null,
 "max_width": null,
 "min_height": null,
 "min_width": null,
 "object_fit": null,
 "object_position": null,
 "order": null,
 "overflow": null,
 "overflow_x": null,
 "overflow_y": null,
 "padding": null,
 "right": null,
 "top": null,
 "visibility": null,
 "width": null
 }
 },
 "0607a0a212bc4ec9964a1ab4d97f8": {
 "model_module": "@jupyter-widgets/base",
 "model_name": "LayoutModel",
 "model_module_version": "1.2.0",
 "state": {
 "_model_module": "@jupyter-widgets/base",
 "_model_module_version": "1.2.0",
 "_model_name": "LayoutModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/base",
 "_view_module_version": "1.2.0",
 "_view_name": "LayoutView",
 "align_content": null,
 "align_items": null,
 "align_self": null,
 "border": null,
 "bottom": null,
 "display": null,
 "flex": null,
 "flex_flow": null,
 "grid_area": null,
 "grid_auto_columns": null,
 "grid_auto_flow": null,
 "grid_auto_rows": null,
 "grid_column": null,
 "grid_gap": null,
 "grid_row": null,
 "grid_template_areas": null,
 "grid_template_columns": null,
 "grid_template_rows": null,
 "height": null,
 "justify_content": null,
 "justify_items": null,
 "left": null,
 "margin": null,
 "max_height": null,
 "max_width": null,
 "min_height": null,
 "min_width": null,
 "object_fit": null,
 "object_position": null,
 "order": null,
 "overflow": null,
 "overflow_x": null,
 "overflow_y": null,
 "padding": null,
 "right": null,
 "top": null,
 "visibility": null,
 "width": null
 }
 },
 "e68ad490a3514755ad691bf7582b17b8": {
 "model_module": "@jupyter-widgets/controls",
 "model_name": "DescriptionStyleModel",
 "model_module_version": "1.5.0",
 "state": {
 "_model_module": "@jupyter-widgets/controls",
 "_model_module_version": "1.5.0",
 "_model_name": "DescriptionStyleModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/base",
 "_view_module_version": "1.2.0",
 "_view_name": "StyleView",
 "description_width": ""
 }
 },
 "cb82c3c5c2314a0b92eca419ecbbd175": {
 "model_module": "@jupyter-widgets/base",
 "model_name": "LayoutModel",
 "model_module_version": "1.2.0",
 "state": {
 "_model_module": "@jupyter-widgets/base",
 "_model_module_version": "1.2.0",
 "_model_name": "LayoutModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/base",
 "_view_module_version": "1.2.0",
 "_view_name": "LayoutView",
 "align_content": null,
 "align_items": null,
 "align_self": null,
 "border": null,
 "bottom": null,
 "display": null,
 "flex": null,
 "flex_flow": null,
 "grid_area": null,
 "grid_auto_columns": null,
 "grid_auto_flow": null,
 "grid_auto_rows": null,
 "grid_column": null,
 "grid_gap": null,
 "grid_row": null,
 "grid_template_areas": null,
 "grid_template_columns": null,
 "grid_template_rows": null,
 "height": null,
 "justify_content": null,
 "justify_items": null,
 "left": null,
 "margin": null,
 "max_height": null,
 "max_width": null,
 "min_height": null,
 "min_width": null,
 "object_fit": null,
 "object_position": null,
 "order": null,
 "overflow": null,
 "overflow_x": null,
 "overflow_y": null,
 "padding": null,
 "right": null,
 "top": null,
 "visibility": null,
 "width": null
 }
 },
 "84c9f040aa474f408b110dd314220f23": {
 "model_module": "@jupyter-widgets/controls",
 "model_name": "ProgressStyleModel",
 "model_module_version": "1.5.0",
 "state": {
 "_model_module": "@jupyter-widgets/controls",
 "_model_module_version": "1.5.0",
 "_model_name": "ProgressStyleModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/base",
 "_view_module_version": "1.2.0",
 "_view_name": "StyleView",
 "bar_color": null,
 "description_width": ""
 }
 },
 "fe9ba760b16b49f2827a97d910bfb2cb": {
 "model_module": "@jupyter-widgets/base",
 "model_name": "LayoutModel",
 "model_module_version": "1.2.0",
 "state": {
 "_model_module": "@jupyter-widgets/base",
 "_model_module_version": "1.2.0",
 "_model_name": "LayoutModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/base",
 "_view_module_version": "1.2.0",
 "_view_name": "LayoutView",
 "align_content": null,
 "align_items": null,
 "align_self": null,
 "border": null,
 "bottom": null,
 "display": null,
 "flex": null,
 "flex_flow": null,
 "grid_area": null,
 "grid_auto_columns": null,
 "grid_auto_flow": null,
 "grid_auto_rows": null,
 "grid_column": null,
 "grid_gap": null,
 "grid_row": null,
 "grid_template_areas": null,
 "grid_template_columns": null,
 "grid_template_rows": null,
 "height": null,
 "justify_content": null,
 "justify_items": null,
 "left": null,
 "margin": null,
 "max_height": null,
 "max_width": null,
 "min_height": null,
 "min_width": null,
 "object_fit": null,
 "object_position": null,
 "order": null,
 "overflow": null,
 "overflow_x": null,
 "overflow_y": null,
 "padding": null,
 "right": null,
 "top": null,
 "visibility": null,
 "width": null
 }
 },
 "98d5f73cca7e49958035c388bda68064": {
 "model_module": "@jupyter-widgets/controls",
 "model_name": "DescriptionStyleModel",
 "model_module_version": "1.5.0",
 "state": {
 "_model_module": "@jupyter-widgets/controls",
 "_model_module_version": "1.5.0",
 "_model_name": "DescriptionStyleModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/base",
 "_view_module_version": "1.2.0",
 "_view_name": "StyleView",
 "description_width": ""
 }
 },
 "076f923cded6471ba047d5ca8a2c370b": {
 "model_module": "@jupyter-widgets/controls",
 "model_name": "HBoxModel",
 "model_module_version": "1.5.0",
 "state": {
 "_dom_classes": [],
 "_model_module": "@jupyter-widgets/controls",
 "_model_module_version": "1.5.0",
 "_model_name": "HBoxModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/controls",
 "_view_module_version": "1.5.0",
 "_view_name": "HBoxView",
 "box_style": "",
 "children": [
 "IPY_MODEL_70d22cb1d11d463bb60e33edafabc453",
 "IPY_MODEL_cd75547ce87b4bb9a158aee96219ebf0",
 "IPY_MODEL_a9e7ee61f9914912ad501b5b271b35bf"
 ],
 "layout": "IPY_MODEL_8979a704ae3943aa8bd8b397185791ca"
 }
 },
 "70d22cb1d11d463bb60e33edafabc453": {
 "model_module": "@jupyter-widgets/controls",
 "model_name": "HTMLModel",
 "model_module_version": "1.5.0",
 "state": {
 "_dom_classes": [],
 "_model_module": "@jupyter-widgets/controls",
 "_model_module_version": "1.5.0",
 "_model_name": "HTMLModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/controls",
 "_view_module_version": "1.5.0",
 "_view_name": "HTMLView",
 "description": "",
 "description_tooltip": null,
 "layout": "IPY_MODEL_ab78eafe8fa24afbbb2ff743fbb93d83",
 "placeholder": "\u200b",
 "style": "IPY_MODEL_25a4735897d94203ae378863eb1661bf",
 "value": "Loading\u2007checkpoint\u2007shards:\u2007100%"
 }
 },
 "cd75547ce87b4bb9a158aee96219ebf0": {
 "model_module": "@jupyter-widgets/controls",
 "model_name": "FloatProgressModel",
 "model_module_version": "1.5.0",
 "state": {
 "_dom_classes": [],
 "_model_module": "@jupyter-widgets/controls",
 "_model_module_version": "1.5.0",
 "_model_name": "FloatProgressModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/controls",
 "_view_module_version": "1.5.0",
 "_view_name": "ProgressView",
 "bar_style": "success",
 "description": "",
 "description_tooltip": null,
 "layout": "IPY_MODEL_bdf2198fcaee4deeb0ccd7d3bf628c6a",
 "max": 3,
 "min": 0,
 "orientation": "horizontal",
 "style": "IPY_MODEL_554e4a038a764197b7cb986e1f91518c",
 "value": 3
 }
 },
 "a9e7ee61f9914912ad501b5b271b35bf": {
 "model_module": "@jupyter-widgets/controls",
 "model_name": "HTMLModel",
 "model_module_version": "1.5.0",
 "state": {
 "_dom_classes": [],
 "_model_module": "@jupyter-widgets/controls",
 "_model_module_version": "1.5.0",
 "_model_name": "HTMLModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/controls",
 "_view_module_version": "1.5.0",
 "_view_name": "HTMLView",
 "description": "",
 "description_tooltip": null,
 "layout": "IPY_MODEL_6ae067ffa3d042d7a7841955fe357d48",
 "placeholder": "\u200b",
 "style": "IPY_MODEL_35864d5fd05b413f89bdf703d201fc9f",
 "value": "\u20073/3\u2007[01:24&lt;00:00,\u200726.88s/it]"
 }
 },
 "8979a704ae3943aa8bd8b397185791ca": {
 "model_module": "@jupyter-widgets/base",
 "model_name": "LayoutModel",
 "model_module_version": "1.2.0",
 "state": {
 "_model_module": "@jupyter-widgets/base",
 "_model_module_version": "1.2.0",
 "_model_name": "LayoutModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/base",
 "_view_module_version": "1.2.0",
 "_view_name": "LayoutView",
 "align_content": null,
 "align_items": null,
 "align_self": null,
 "border": null,
 "bottom": null,
 "display": null,
 "flex": null,
 "flex_flow": null,
 "grid_area": null,
 "grid_auto_columns": null,
 "grid_auto_flow": null,
 "grid_auto_rows": null,
 "grid_column": null,
 "grid_gap": null,
 "grid_row": null,
 "grid_template_areas": null,
 "grid_template_columns": null,
 "grid_template_rows": null,
 "height": null,
 "justify_content": null,
 "justify_items": null,
 "left": null,
 "margin": null,
 "max_height": null,
 "max_width": null,
 "min_height": null,
 "min_width": null,
 "object_fit": null,
 "object_position": null,
 "order": null,
 "overflow": null,
 "overflow_x": null,
 "overflow_y": null,
 "padding": null,
 "right": null,
 "top": null,
 "visibility": null,
 "width": null
 }
 },
 "ab78eafe8fa24afbbb2ff743fbb93d83": {
 "model_module": "@jupyter-widgets/base",
 "model_name": "LayoutModel",
 "model_module_version": "1.2.0",
 "state": {
 "_model_module": "@jupyter-widgets/base",
 "_model_module_version": "1.2.0",
 "_model_name": "LayoutModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/base",
 "_view_module_version": "1.2.0",
 "_view_name": "LayoutView",
 "align_content": null,
 "align_items": null,
 "align_self": null,
 "border": null,
 "bottom": null,
 "display": null,
 "flex": null,
 "flex_flow": null,
 "grid_area": null,
 "grid_auto_columns": null,
 "grid_auto_flow": null,
 "grid_auto_rows": null,
 "grid_column": null,
 "grid_gap": null,
 "grid_row": null,
 "grid_template_areas": null,
 "grid_template_columns": null,
 "grid_template_rows": null,
 "height": null,
 "justify_content": null,
 "justify_items": null,
 "left": null,
 "margin": null,
 "max_height": null,
 "max_width": null,
 "min_height": null,
 "min_width": null,
 "object_fit": null,
 "object_position": null,
 "order": null,
 "overflow": null,
 "overflow_x": null,
 "overflow_y": null,
 "padding": null,
 "right": null,
 "top": null,
 "visibility": null,
 "width": null
 }
 },
 "25a4735897d94203ae378863eb1661bf": {
 "model_module": "@jupyter-widgets/controls",
 "model_name": "DescriptionStyleModel",
 "model_module_version": "1.5.0",
 "state": {
 "_model_module": "@jupyter-widgets/controls",
 "_model_module_version": "1.5.0",
 "_model_name": "DescriptionStyleModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/base",
 "_view_module_version": "1.2.0",
 "_view_name": "StyleView",
 "description_width": ""
 }
 },
 "bdf2198fcaee4deeb0ccd7d3bf628c6a": {
 "model_module": "@jupyter-widgets/base",
 "model_name": "LayoutModel",
 "model_module_version": "1.2.0",
 "state": {
 "_model_module": "@jupyter-widgets/base",
 "_model_module_version": "1.2.0",
 "_model_name": "LayoutModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/base",
 "_view_module_version": "1.2.0",
 "_view_name": "LayoutView",
 "align_content": null,
 "align_items": null,
 "align_self": null,
 "border": null,
 "bottom": null,
 "display": null,
 "flex": null,
 "flex_flow": null,
 "grid_area": null,
 "grid_auto_columns": null,
 "grid_auto_flow": null,
 "grid_auto_rows": null,
 "grid_column": null,
 "grid_gap": null,
 "grid_row": null,
 "grid_template_areas": null,
 "grid_template_columns": null,
 "grid_template_rows": null,
 "height": null,
 "justify_content": null,
 "justify_items": null,
 "left": null,
 "margin": null,
 "max_height": null,
 "max_width": null,
 "min_height": null,
 "min_width": null,
 "object_fit": null,
 "object_position": null,
 "order": null,
 "overflow": null,
 "overflow_x": null,
 "overflow_y": null,
 "padding": null,
 "right": null,
 "top": null,
 "visibility": null,
 "width": null
 }
 },
 "554e4a038a764197b7cb986e1f91518c": {
 "model_module": "@jupyter-widgets/controls",
 "model_name": "ProgressStyleModel",
 "model_module_version": "1.5.0",
 "state": {
 "_model_module": "@jupyter-widgets/controls",
 "_model_module_version": "1.5.0",
 "_model_name": "ProgressStyleModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/base",
 "_view_module_version": "1.2.0",
 "_view_name": "StyleView",
 "bar_color": null,
 "description_width": ""
 }
 },
 "6ae067ffa3d042d7a7841955fe357d48": {
 "model_module": "@jupyter-widgets/base",
 "model_name": "LayoutModel",
 "model_module_version": "1.2.0",
 "state": {
 "_model_module": "@jupyter-widgets/base",
 "_model_module_version": "1.2.0",
 "_model_name": "LayoutModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/base",
 "_view_module_version": "1.2.0",
 "_view_name": "LayoutView",
 "align_content": null,
 "align_items": null,
 "align_self": null,
 "border": null,
 "bottom": null,
 "display": null,
 "flex": null,
 "flex_flow": null,
 "grid_area": null,
 "grid_auto_columns": null,
 "grid_auto_flow": null,
 "grid_auto_rows": null,
 "grid_column": null,
 "grid_gap": null,
 "grid_row": null,
 "grid_template_areas": null,
 "grid_template_columns": null,
 "grid_template_rows": null,
 "height": null,
 "justify_content": null,
 "justify_items": null,
 "left": null,
 "margin": null,
 "max_height": null,
 "max_width": null,
 "min_height": null,
 "min_width": null,
 "object_fit": null,
 "object_position": null,
 "order": null,
 "overflow": null,
 "overflow_x": null,
 "overflow_y": null,
 "padding": null,
 "right": null,
 "top": null,
 "visibility": null,
 "width": null
 }
 },
 "35864d5fd05b413f89bdf703d201fc9f": {
 "model_module": "@jupyter-widgets/controls",
 "model_name": "DescriptionStyleModel",
 "model_module_version": "1.5.0",
 "state": {
 "_model_module": "@jupyter-widgets/controls",
 "_model_module_version": "1.5.0",
 "_model_name": "DescriptionStyleModel",
 "_view_count": null,
 "_view_module": "@jupyter-widgets/base",
 "_view_module_version": "1.2.0",
 "_view_name": "StyleView",
 "description_width": ""
 }
 }
 }
 }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}